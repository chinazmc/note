
# OpenResty + Lua 实现灰度发布

[郭子龙](https://tech.qimao.com/author/guozilong/) 发表于 2023/11/06

# **背景说明**

七猫作为一家成熟的互联网公司，业务有超百万QPS的高并发。我们的产品开发逃不开的特色就是不停的升级升级再升级。随着敏捷小组的建立，发版频率也逐渐提升至每两周一次或者每周一次。然而系统升级总是会伴随着各种风险，一些系统风险比如：宕机风险，服务不可用的风险；还有一些用户体验风险：业务改动使得用户体验改变导致用户流失等风险。为了规避或者提前预知这些风险，灰度发布的概念应运而生。

灰度发布是指在黑与白之间，能够平滑过渡的一种发布方式。其主要思想就是把影响集中到一个点，然后慢慢发散到整个面，期间出现意外情况可以很快回退，整体影响可控。

我们基于 OpenResty + Lua 实现了一套简单易用的灰度发布的方案。

# 系统方案设计

本方案是基于 OpenResty + Lua 的方式实现。OpenResty 是一个基于 Nginx 和 Lua 的高性能 Web 平台，内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项，可以用于方便地搭建高并发且强拓展性的动态 Web 应用、Web 服务和动态网关。

本方案主要是实现了对 HTTP 协议的灰度功能，主要基于 ngx_http_lua_module 模块实现。我们将主要通过以下三个方面来介绍:

1. Nginx + Lua 实现灰度的方式和用法；
2. 通过 Lua 接入 Prometheus，实现 Host 级别的 QPS，P99 延迟、状态码等详细监控；
3. 针对高并发的关键性能优化；  


本方案有以下优点：

- 运维成本低：几人小团队在几天时间可快速落地；
- 容易学习：Lua 脚本学习成本低，即使从未接触过，也可在短时间内迅速上手；
- 复杂度低：架构清晰，易于理解。Nginx 简单易用；
- 技术成熟： OpenResty 稳定性、性能和可靠性已经经过多年的实践和验证；

# 一、灰度发布的实现

先给大家上一张整体的流程图，如下：

![](https://tech.qimao.com/content/images/2023/11/img_v2_a86eccb1-f460-47b6-b1af-fe635a01bcag.jpg)

## 架构概览

可以看到，我们的整个流程分为 3 个部分：

1. 管理后台：管理员在后台页面设置灰度用户百分比；
2. 灰度服务端：定时定期拉取管理后台的灰度配置，处理请求 Cookie 中的 UID 和 UUID，返回版本号；
3. Nginx 层：此处使用 Nginx + Lua 实现灰度发布的版本切换逻辑。整体流程来看，就是管理员在后台提前配置好灰度规则，灰度服务端拉取并同步该规则。同时提供接口，接收请求；

## 整体流程

1. 用户请求服务端的时候 Cookie 中会带有 UID 和 UUID 字段；
2. 使用 Lua 读取 Cookie 中 UID 和 UUID，如果没有 UUID 则生成；
3. 发送 UID 和 UUID 到灰度服务端，灰度服务端根据 UUID 按预先设置好的版本比例随机（通过同步管理后台）返回版本号（如果 UID 有指定固定版本号，则优先返回固定版本）；
4. Lua 收到版本号后，修改目标 root 路径。（也可以修改 IP 或者端口号，以便应用到更复杂的场景）；
5. 返回内容和 UUID 写入用户本地 Cookie；

## 后台管理页

面管理后台的页面如下图，管理员通过此后台去配置用户的灰度流量比例和那些用户走灰度版本。

![](https://tech.qimao.com/content/images/2023/11/image-21.png)

## Cookie 操作说明

当浏览器收到包含 "Set-Cookie" 头部的响应时，它会解析该头部并将其中的 Cookie 信息存储在浏览器的 Cookie 存储中。这样，在之后的请求中，浏览器会自动在请求头中包含相应的 Cookie 信息，将其发送给服务器。

### 如何获取Cookie

```yaml
获取单个cookie： ngx.var.cookie_name, 获取单个cookie，_后面的cookie的name，如果不存在则返回nil 。
```

### 设置Cookie

```yaml
ngx.header['Set-Cookie'] = {'a=32; path=/', 'b=4; path=/'}-- 批量设置cookie
ngx.header['Set-Cookie'] = 'a=32; path=/' -- 设置单个cookie,通过多次调用来设置多个值
ngx.header['Set-Cookie'] = 'c=5; path=/; Expires=' .. ngx.cookie_time(ngx.time() + 60 * 30) -- 设置Cookie过期时间为30分钟
```

_注：设置 Cookie 时要一定要加 Path 和 Expires（过期时间），不然无法生效，因为默认过期时间为立即。_

### Nginx + Lua 代码流程

起初，我们的代码流图如下图所示

![](https://tech.qimao.com/content/images/2023/11/image-22.png)

- 用户请求到 Nginx 层后，我们通过Lua 脚本判断 Cookie 中是否存在 UUID，如果不存在，则会为其生成一个，随后将新的 Cookie 写入客户端，然后返回 302 并重定向会当前 URL；
- 如果存在，则再判断 Cookie 中是否存在 UID，如果不存在，则使用默认版本号；存在则使用 UUID + UID 请求服务端接口，使用接口返回的版本号；
- 最后根据使用的版本号，返回客户端 302 并重定向。这个版本的流程其实存在一个比较明显的问题：逻辑上存在大量冗余。 用户第一次请求会客户端自动刷新一次，对用户体验有轻微影响。 为了减少冗余以及减轻对用户体验的影响，我们顺势优化了第二版的流程；

### Nginx + Lua 代码流程 v2

优化后的流程图如下：

![](https://tech.qimao.com/content/images/2023/11/image-23.png)

这个版本中，如果 Cookie 中的 UUID 不存在，则生成 UUID；如果存在，则直接使用 Cookie 中的 UUID ，通过 Lua 发送 Cookie 中的 UID 和 UUID 到服务端接口，获取版本号；最后根据版本号设置 nginx 的 root 目录。这一版的代码，做到了一次判断，一次请求，整体更加简洁高效。

### 代码部分实现示例

```nginx
    location  /  {
        set $docroot ""; #设置默认root目录变量
        index index.html;

        rewrite_by_lua_block{

            -- 请求ab-agnet程序，返回版本号,请求失败不返回
            local function version(uid,my_uuid)
                local res = ngx.location.capture('/api', { args = { account_id = uid , uuid = my_uuid } })

                if res.body and string.match(res.body,"%d.%d.%d") then
                    ngx.var.docroot = string.match(res.body,"%d.%d.%d")
                end
            end

            -- 新设备生成uuid写入cookie
            local function writeUuid()
                if ngx.var.cookie_uuid == nli then
                    local uuid = io.open("/proc/sys/kernel/random/uuid", "r"):read()
                    ngx.header["Set-Cookie"] = string.format("uuid=%s; Expires=%s;path=%s", uuid, ngx.cookie_time(ngx.time() + 86400 * 1000),"/")
                    return uuid
                end
            end


            -- main
            if ngx.var.cookie_uuid == nil then
                if pcall(version,ngx.var.cookie_uid,writeUuid()) then
                else
                    ngx.var.docroot = ""
                end

            else
                if pcall(version,ngx.var.cookie_uid,ngx.var.cookie_uuid) then
                else
                    ngx.var.docroot = ""
                end
            end

        }
    }
```

_注：ngx.location.capture不支持 http2.0，可以用 lua-resty-http 代替_

# 二、监控方案

> 运维行业有句话：“无监控、不运维”，是的，这句话一点也不夸张，没了监控，什么基础运维，业务运维都是“瞎子”。所以说监控是运维这个职业的根本。监控系统不完善，运维人365天如何睡得踏实？所以作为一个运维工程师，重要场景的监控必不可少。

其实 Nginx 原生通过 stub_status 页面暴露了部分监控指标。Nginx Prometheus Exporter 会采集单个 Nginx 实例指标，并将其转化为 Prometheus 可用的监控数据， 最终通过 HTTP 协议暴露给 Prometheus 服务进行采集。我们可以通过 Exporter 上报重点关注的监控指标，用于异常报警和大盘展示。

但是默认 stub_status 暴露的指标可能过于通用了，不能满足我们的业务监控需要，官方提供的指标如下图：

![](https://tech.qimao.com/content/images/2023/11/image-24.png)

可以看到，都是一些 Nginx 连接数相关的指标，在实际的 Grafana 中看到的如下图，这些指标对我们来说并不友好，效果也只能说平平无奇。甚至可以说这样的监控几乎没有作用，我们也无法根据这个监控对业务运行情况作出判断，可以说几乎就是瞎子，这种情况下没有重要业务敢运行在上面。

![](https://tech.qimao.com/content/images/2023/11/image-25.png)

  
不过好在我们有 Lua ，通过 Lua 插件接入 Prometheus 监控。我们可以在 Host 级别上对 QPS，P99 延迟，请求状态码等详细情况进行监控，并在我们关心的某些指标异常时及时告警。下图为使用 Lua 插件实现的 Nginx 监控效果，可以看到指标明显变得更加丰富与直观了。

![](https://tech.qimao.com/content/images/2023/11/image-26.png)

**Nginx http 中代码配置**

```nginx
http {
    ........省略
    lua_shared_dict prometheus_metrics 10M;
    lua_package_path "/opt/app/openresty/site/lualib/?.lua;;";
    
    init_worker_by_lua_block {
        prometheus = require("prometheus").init("prometheus_metrics")
        metric_requests = prometheus:counter(
            "nginx_http_requests_total", "Number of HTTP requests", {"host", "status"})
        metric_latency = prometheus:histogram(
            "nginx_http_request_duration_seconds", "HTTP request latency", {"host"})
        metric_connections = prometheus:gauge(
            "nginx_http_connections", "Number of HTTP connections", {"state"})
    }
    log_by_lua_block {
        metric_requests:inc(1, {ngx.var.server_name, ngx.var.status})
        metric_latency:observe(tonumber(ngx.var.request_time), {ngx.var.server_name})
    }
    ........省略
    server {
        listen 9113;
        server_name 127.0.0.1;
        ........省略
    location /metrics {
        content_by_lua '
          metric_connections:set(ngx.var.connections_reading, {"reading"})
          metric_connections:set(ngx.var.connections_waiting, {"waiting"})
          metric_connections:set(ngx.var.connections_writing, {"writing"})
          prometheus:collect()
        ';
    }
    ........省略
}
```

启动后 Nginx 将会监听本地的 9113 端口，  在 Prometheus 中收集  https://host/metrics 中的监控指标，然后通过Grafana展示。

# 三、高并发：关键性能优化

随着业务发展，请求峰值可以达到 20 万/秒，在服务器一共 25 台的情况下，资源率使用不够均匀，偶尔有部分机器 CPU 核心跑到 100%，但是有些其他机器 CPU 核心仅 20%。因此业务整体非常不稳定，想要快速解决这个问题，只能靠扩更多的服务器获取稳定性，成本太高。

鉴于此，我们采用了如下三个方式提高我们的性能：

## 使用 reuseport

使用`reuseport`参数可以完美解决我们遇到的服务器 CPU 利用率不均匀的问题，并显著提高效率。我们先来看下 Nginx 的整体架构：

![](https://tech.qimao.com/content/images/2023/11/image-27.png)

它采用了一种主从结构来处理客户端请求。这种结构包括一个主进程（Master Process）和多个工作进程（Worker Process）：

主进程负责管理整个系统，它主要的职责是启动和停止工作进程，并进行配置文件的解析和加载。主进程还负责监听端口，接收客户端的请求，并根据配置文件的规则将请求分发给工作进程处理；

工作进程是实际处理客户端请求的进程，它们由主进程创建并管理。每个工作进程都是独立的，它们可以并行地处理多个请求。工作进程使用事件驱动的方式处理请求，这使得 Nginx 能够高效地处理大量并发连接；

主进程和工作进程之间通过进程间通信（IPC）进行通信。主进程可以向工作进程发送命令，如重新加载配置文件或停止工作进程。工作进程则将处理结果返回给主进程，由主进程再将结果返回给客户端；

这种主从结构使得 Nginx 能够充分利用多核处理器的能力，提供高性能和可伸缩性。主进程负责管理整个系统，工作进程负责处理实际的请求，它们之间的分工协作使得 Nginx 能够有效地处理大量的并发请求。

### 理解`accept_mutex`参数：

- 开启 `accept_mutex`：只有一个 worker 进程能够处理新连接，其他 worker 进程在互斥锁上等待，避免了竞争问题，但可能导致性能下降。
- 关闭 `accept_mutex`：所有 worker 进程都可以并发地处理新连接，避免了互斥锁的开销，可能获得更好的性能，但可能引发竞争问题。

### 理解 `reuseport` 参数

开启 `reuseport` 参数可以提高并发性能、分散连接负载、提高可用性，并减少连接竞争，特别适用于高并发场景下的网络应用。

由下图是开启前和开启后的流程对比：

![](https://tech.qimao.com/content/images/2023/11/image-28.png)

可以看到左边是未开启，系统只监听一个 Socket，然后把请求分给所有 worker 进程，免不了有大量大竞争，等待，cpu 上下文切换，负载不均衡等问题。右边是开启后，多个 Worker 进程都监听同一个端口，内核会将连接分发给各个进程或线程，从而实现连接负载的均衡。这有助于避免单个进程或线程成为瓶颈，提高系统的整体承载能力。总结一下开启`reuseport` 后有以下优点：

- **改善并发性能**：`reuseport` 允许多个进程或线程同时监听同一个端口，每个进程或线程都能够独立地处理连接。这样可以提高并发性能，充分利用多核系统的处理能力；
- **分散连接负载**：当多个进程或线程监听同一个端口时，内核会将连接分发给各个进程或线程，从而实现连接负载的均衡。这有助于避免单个进程或线程成为瓶颈，提高系统的整体承载能力；
- **提高可用性**：在使用 `reuseport` 时，如果一个进程或线程崩溃或出现问题，其他进程或线程可以继续处理连接，从而提高系统的可用性和容错性；
- **减少连接竞争**：启用 `reuseport` 后，每个进程或线程都拥有独立的套接字，避免了连接竞争问题。这可以减少锁竞争，降低延迟，并提高系统的整体性能；

下面是三种情况的对比：  

- Default：Accept_mutex on 的情况下性能最差；
- Accept_mutex off 的情况性能有少许提升；
- Reuseport 的情况下性能最好；

![](https://tech.qimao.com/content/images/2023/11/image-29.png)

|   |   |   |   |
|---|---|---|---|
||Latency (ms)|Latency stdev (ms)|CPU Load|
|accept_mutex on|15.65|26.59|0.3|
|accept_mutex off|15.59|26.48|10|
|reuseport|12.35|3.15|0.3|

  

## 反向代理使用 tcp 长链接

默认 proxy_http_version 版本为 1.0, Connection 默认为 close。  也就是 Nginx 每次转发流量到下游都使用会经过 TCP 的 3 次握手 4 次挥手的高开销动作，性能较差。

通过以下参数配置到 location 中，使用长链接，一次建立tcp通道，多次通讯，性能可以得到显著提升。

```yaml
proxy_http_version 1.1; 
proxy_set_header Connection ""; 
```

## 开启HTTP 2.0

`listen 443 ssl http2;`

HTTP/2 相比 HTTP/1 大大提高了传输效率、吞吐能力。

- 通过静态表和 Huffman 编码的方式，将体积压缩了近一半，而且针对后续的请求头部，还可以建立动态表，将体积压缩近 90%，大大提高了编码效率，同时节约了带宽资源；
- 多个 Stream 只需复用 1 个 TCP 连接，节约了 TCP 和 TLS 握手时间，以及减少了 TCP 慢启动阶段对流量的影响；
- 二进制格式传输数据，极大提高了 HTTP 传输效率，而且二进制数据使用位运算能高效解析；  
    

# OpenResty+Lua 的功能扩展：

我们还用 OpenResty + Lua 的形式实现过如下功能，希望可以给你带来启发。

- 根据 Header 是否存在转发流量到对应 Location
- 根据 Header 是否存在转发流量到对应端口
- Redis读写，根据 Redis 读写结果实现功能，使用连接池提高性能。
- Nginx 的 mirror 模块复制流量转发到其他
- 使用 Lua 根据安卓版本决定如何灰度
- 根据来源 IP 使用生成固定的 UUID  
    

# 不足与展望：

- lua脚本可维护性差，功能不易太过复杂，要控制代码量。上线后尽量不要在原来逻辑上做大幅修改；
- 容器场景下： 功能和灵活性不如服务网格和其他Ingress-Controller；
- 我们一直在探索使用功能更强大易用的网关： Kong、Envoy、ALB、Ingress-Nginx、Istio服务网格, 都各有所长；
- 目前Envoy作为我们主要使用的网关，主要应用在南北向流程场景中。 性能，可观测性，稳定性都有不错的表现；
- Ingress-Nginx用在对性能要求不高的场景中，将来会在Envoy之间二选一，统一架构；
- 服务网格正在探索中，以支撑越来越复杂的业务场景；
- OpenResty 应用在所有的非容器场景和百万QPS并发场景；


# 七猫分布式追踪实践

[刘海港](https://tech.qimao.com/author/liu-hai-gang/) 发表于 2023/10/19

# 背景

Metrics（指标）、Logging（日志记录）、和Tracing（追踪）通常被称为可观测性的三大支柱。在微服务架构下，分布式追踪是一种关键工具，用于帮助排查和理解服务问题，它允许跟踪请求流程并提供关键的信息，以便更容易发现和解决问题。

随着服务数量的日益增加，承接全量的 Tracing 数据所需的资源越来越多。如果全量采集，链路中产生的span 所占的存储成本将会很高。目前生产环境采用0.1% 的采集率，存储成本较低。这种策略虽然很节省资源，但其缺点在一次次线上问题排查中逐渐暴露：

- 一个进程中包含多个接口：不论按固定概率采样还是限流采样，都会导致小流量接口一直采集不到调用链数据
- 线上服务出错是小概率事件，导致出错的请求被采中的概率更小，就导致采到的调用链信息量不大，引发问题的调用链却丢失的问题

OpenTelemetry 生态中提供了一些简单的手段应对上面的问题。

# OpenTelemetry

OpenTelemetry合并了OpenTracing和OpenCensus项目，提供了一组API和库来标准化遥测数据的采集和传输。OpenTelemetry提供了一个安全，厂商中立的工具，这样就可以按照需要将数据发往不同的后端。

OpenTelemetry项目由如下组件构成：

- 推动在所有项目中使用一致的规范
- 基于规范的，包含接口和实现的APIs
- 不同语言的SDK(APIs的实现)，如 Java, Python, Go, Erlang等
- Exporters：可以将数据发往一个选择的后端
- Collectors：厂商中立的实现，用于处理和导出遥测数据

# Tail Sampling Processor

为了解决这些问题，我们可以采取一种方法，即将所有生成的数据发送到一个中间平台，该平台会先进行暂存和清洗，然后再决定是否要保留或丢弃整个跟踪信息。具体操作如下图所示：当一个请求到达时，每个服务会将其生成的跟踪信息（Span）发送到一个名为OpenTelemetry Collector的中间组件。在Collector上有一个尾部采样组件，该组件会在接收到第一个Span后，等待一段时间（例如5秒），以便继续收集来自其他服务、具有相同Trace ID的Span。等待时间结束后，大量的Span会按照它们的Trace ID进行分类汇总，然后对属于同一个Trace ID的Span进行遍历，以检查是否包含错误信息，或者累计耗时是否超过了预设的阈值等。基于这些信息，可以有依据地筛选出高价值的跟踪信息，将它们纳入后续的处理流程。这种方法有助于更有效地处理跟踪数据，识别潜在问题，以及提高整体性能。

这种采样的模式称为**尾部采样**，它由 Collector 依据完整 Trace 的信息进行决策，决策的依据比头部采样丰富很多。但是由于需要承载大量临时数据，所以对基础设施要求很高。它的效果在于：

- 持久化的数据有依据、有规律、有价值；
- 减少了展示有价值数据所需的成本，例如存储资源，并且对提高查询速度也有帮助。

需要注意的是，在实际部署中，整个架构要做到高可用，往往会存在多个 Collector 节点，而同一个 Trace 的不同 Span 是由不同服务产生的，这些服务位于不同地方，尾部采样要求他们都落入相同的 Collector 节点，那么必然需要一层负载均衡架设在 Collector 之前，依照 Trace ID 进行转发。让 otel-agent 按照 traceID 做负载均衡，使用exporters 中的loadbalancing 组件。

![](https://tech.qimao.com/content/images/2023/10/11.png)

架构图如下：

pod 通过sdk 的方式上报到otel-agent，agent 的receivers使用otel grpc的方式接收trace；processors 使用 batch 来批量处理；exporters 使用 loadbalancing 方式根据traceID 负载到下游同一个collector。

otel-collector 由grpc 接收相同traceID的span，本地批量处理，基于这些信息根据 tail_sampling 配置规则决定是否上报到阿里云analysis平台。同时可配置不同的processor 处理不同数据导出到不同的平台。

![](https://tech.qimao.com/content/images/2023/10/12.png)

## 部署和配置

loadbalancing 采用k8s的方式部署，配置如下：

```
  exporters:
    loadbalancing:
      protocol:
        otlp:
          timeout: 5s
          sending_queue: # 发送队列
            enabled: true
            num_consumers: 50 # 消费者数量
            queue_size: 5000000 # 队列长度
          retry_on_failure:
            enabled: false # 是否重试
          tls:
            insecure: true
      resolver:
        k8s: # k8s service 方式获取pod 的IP 进行负载均衡
          service: otel-collector.observable
```

> otlp 中的队列，queue_size 这个参数指定了队列的最大容量，即可以在队列中缓存等待发送的数据点的最大数量。如果队列已满，新的数据点可能会被丢弃或被替代。这个参数可以用来控制队列的内存使用和数据传输的稳定性。

配置 retry_on_failure = true，增大queue_size值，会导致内存增加；后续根据请求的qps 和span 产生量，配置合理的queue_size。

![](https://tech.qimao.com/content/images/2023/10/13.png)

使用k8s 负载均衡方式，需要创建对应的账号权限

- 创建 service account

```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-loadbalancer
  namespace: observable
```

- 创建 role

```
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: otel-loadbalancer-role
  namespace: observable
rules:
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - list
      - watch
      - get
```

- 创建role binding

```
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: otel-loadbalancer-rolebinding
  namespace: observable
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: otel-loadbalancer-role
subjects:
  - kind: ServiceAccount
    name: otel-loadbalancer
    namespace: observable
```

关注的调用链全采样：研发在分析、排障过程中想查询的任何调用链都是重要调用链。总结以下优先级高场景：

- 在调用链上error 级别日志
- 整个调用链请求耗时超过 250ms

```
processors:
  tail_sampling:
    decision_wait: 5s # 等待5秒，超过5秒后的traceid将不再处理
    num_traces: 1500000 
    expected_new_traces_per_sec: 10 # 新增的trace 数量
    policies: # 上报规则策略
      [
        {
          name: error-policy,
          type: status_code, # 状态码，err
          status_code: { status_codes: [ ERROR ] }
        },
        {
          name: timeout-policy,
          type: latency, # 耗时，超过250ms 上报
          latency: { threshold_ms: 250 }
        }
      ]
```

> num_traces 推荐的计算规则：假设每秒有100 traces(不是span)，配置 decision_wait:5，则 num_traces=100*_5_*2=1000；num_traces 越大，对应的内存也会越大。

> decision_wait 是一个全链路的上报采集的等待时间，超时则丢弃后续traceID 的链路信息；请根据业务设置相应的超时时间，但是如果超时时间设置的特别大，会导致占有内存增加。

相关issues [https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/17275](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/17275)

## 效果

部分服务的有效链路追踪信息如下：

- 系统消息，每天产生的有效链路追踪只有2条，很容易处理对应的问题

![](https://tech.qimao.com/content/images/2023/10/14.png)

![](https://tech.qimao.com/content/images/2023/10/15.png)

- 其他服务，redis操作产生的error，都被全量采集

![](https://tech.qimao.com/content/images/2023/10/16.png)

# 资源使用

以下是官方给出的资源使用情况，[测试脚本](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/cmd/oteltestbedcol/main.go)。测试配置如下：

- FlushInterval(ms) [default: 1000] (刷新间隔 1秒)
- MaxQueueSize [default: 100] （最大队列100）
- SubmissionRate(spans/sec): 100,000 （每秒产生10w 的span）

## 头部采集法

![](https://tech.qimao.com/content/images/2023/10/image-70.png)

## 尾部采集法

![](https://tech.qimao.com/content/images/2023/10/image-71.png)

尾部采样需要更多的基础设施资源，这在内存上体现得比较明显；目前生产环境的资源消耗也是符合预期的。

## 本地压测

测试代码 [https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/testbed/tests/trace_test.go#L399](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/testbed/tests/trace_test.go#L399)，需要修改为tail sampling processor，默认是没有加载的；可以参考README 来实现。

![](https://tech.qimao.com/content/images/2023/10/11-1.png)

![](https://tech.qimao.com/content/images/2023/10/12-1.png)

## 生产资源使用情况

![](https://tech.qimao.com/content/images/2023/10/11-2.png)

otel-agent 的CPU 内存资源消耗：

![](https://tech.qimao.com/content/images/2023/10/11-3.png)

otel-collector 的CPU内存 资源消耗：

![](https://tech.qimao.com/content/images/2023/10/11-4.png)

320k/8k = 40CPU，目前collector使用了30CPU，优化的点在于使用k8s方式部署，每个pod的cpu被限制使用4C，没有配置automaxprocs，导致CPU使用40C左右，使用automaxprocs后优化了 `1C/pod` 左右。对比图如下：

![](https://tech.qimao.com/content/images/2023/10/11-5.png)

# 小结

借助开源项目，我们得以通过花费极少的人力，解决当前内部调用链追踪应用的稳态分析及异常检测需求，相比较头部全量采集的存储成本，我们使用OpenTelemetry tail-based sampling 的方式大大降低了成本，存储成本降了几十倍。调用链追踪是可观测性平台的重要组件，未来将继续把一些精力放在 telemetry data 的整合上，为研发提供更全面、一致的服务观测分析体验。

# 使用ARMS+LTS监控告警应用实践

[张振宇](https://tech.qimao.com/author/zhang-zhen-yu/) 发表于 2023/10/09

鉴于之前调研的夜莺等一体化监控告警工具，我们综合总结下来，使用阿里云提供的ARMS告警系统，以减轻运维成本，目标是解决当前多而繁杂的业务系统中各式各样的告警机器人治理问题，我们需要一个聚合的，可容错的，可削峰的，可溯源的监控告警系统。

下面我们一起来看下当下在商业化广告这边应用实践

### 一、总体流程图

![](https://tech.qimao.com/content/images/2023/10/image-1.png)

### 二、过去与现在的对比图

![](https://tech.qimao.com/content/images/2023/10/image-49.png)

### 三、普罗监控侧告警配置

#### 1.编写普罗告警规则

vim pixiu.yaml

|   |
|---|
|YAML  <br>apiVersion: monitoring.coreos.com/v1  <br>kind: PrometheusRule  <br>metadata:  <br>  labels:  <br>    app: kube-prometheus-stack  <br>    app.kubernetes.io/instance: prometheus  <br>    release: prometheus  <br>  name: pixiu-ad-test {当前集群监控的名称}  <br>  namespace: monitor  <br>spec:  <br>  groups:  <br>  - name: pixiu-ad-test-request   {策略组的名称，切记不要有大写}  <br>    rules:  <br>    - alert: DspDmpRequestSuccessRate  <br>      annotations:  <br>        description: 当前p99数量：{{ $value }}  <br>        runbook_url: https://XXXXXXX:3000/d/b_ICsjx4k/pixiu-dsp-server?orgId=1&refresh=1m&from=now-5m&to=now&viewPanel=20  <br>        summary: 合作平台p99数量  <br>      expr: count(node_network_receive_bytes_total)>1  <br>      for: 1m  <br>      labels:  <br>        notice: pixiu-ad-test  <br>        severity: warning|

##### 关键参数说明：

- 当前集群监控的名称：

metadata.name: pixiu-ad-test

- 策略组的名称，切记不要有大写：

spec.groups.name: pixiu-ad-test-request

- 告警文案：

description: 当前数量{{ $value }}

- rules.alert.expr: 要执行的sql语法，记得自己grafana去验证下是否能查出数据

**使yaml生效**

- kubectl apply -f pixiu.yaml

**查看当前集群monitor状态**

- kubectl -n monitor get prometheusrule

**以及查看刚刚发布的监控配置：**

- kubectl -n monitor get prometheusrule -o yaml  pixiu-ad-test

#### 2.飞书群/个人 生效收到告警配置

![](https://tech.qimao.com/content/images/2023/10/image-3.png)

• 配置地址：[https://arms.console.aliyun.com/?spm=5176.2020520112.top-nav.47.197634c0fMr9XG#/alarm/notifyPolicy/list](https://arms.console.aliyun.com/?spm=5176.2020520112.top-nav.47.197634c0fMr9XG#/alarm/notifyPolicy/list)

选择策略对应的配置名称，就是上面yam里配置的 labels.notice: pixiu-ad-test

没有显示，就说明目前你配置的规则还没有触发阈值，当有了以后，这里就显示名称了，接下来点到通知对象中，配置接收人

![](https://tech.qimao.com/content/images/2023/10/image-4.png)

首先去配置通知对象，飞书群聊里，怎么添加机器人不用多说了，直接上货！

![](https://tech.qimao.com/content/images/2023/10/image-5.png)

![](https://tech.qimao.com/content/images/2023/10/image-6.png)

这里是通用模版，我们已经有成熟的大量模版了，都是运维大大们的功劳~感谢感谢

• 如果还需要自定义信息，可以查看这个文档来了解规则。

[https://help.aliyun.com/zh/arms/alarm-operation-center/configure-notification-templates-and-webhook-templates](https://help.aliyun.com/zh/arms/alarm-operation-center/configure-notification-templates-and-webhook-templates)

• 如果想通知到多个飞书个人，可以查看这里文档所示，作相应排班管理

[https://help.aliyun.com/zh/arms/alarm-operation-center/mention-contacts-with-at-sign-in-single-group-chat](https://help.aliyun.com/zh/arms/alarm-operation-center/mention-contacts-with-at-sign-in-single-group-chat)

![](https://tech.qimao.com/content/images/2023/10/image-7.png)

最后配置完成后保存，你将马上收到飞书的新消息， （当然最好永远收不到告警消息☺️ ）通过点击链接，可以看到你yaml中配置的面板信息。

![](https://tech.qimao.com/content/images/2023/10/image-46.png)

- **配置通知对象**

[https://arms.console.aliyun.com/#/alarm/notifyObject](https://arms.console.aliyun.com/#/alarm/notifyObject) (联系运维申请权限)

- **在阿里云配置告警策略**

[https://arms.console.aliyun.com/#/alarm/notifyPolicy/list](https://arms.console.aliyun.com/#/alarm/notifyPolicy/list) (联系运维申请权限)

![](https://tech.qimao.com/content/images/2023/10/image-47.png)

##### 3. 监控侧告警服务流程图

当存在多个集群的多个业务情况下，由于采集指标源prometheus是部署在各自的集群内部指定命名空间，这里我们采用流水线的方式来集体管控对多个prometheus源进行监控配置管理，如下所示，我们在流水线中配置多个构建发布，通过不同集群的链接配置，可以发布到各自集群内。

![](https://tech.qimao.com/content/images/2023/10/image-10.png)

普罗监控多集群化流程图

##### 4.监控告警流水线发布

![](https://tech.qimao.com/content/images/2023/10/image-43.png)

### 四、业务开发侧配置监控告警

#### 1.业务侧告警流程

![](https://tech.qimao.com/content/images/2023/10/image-12.png)

消息告警机器人流程图

![](https://tech.qimao.com/content/images/2023/10/image-44.png)

通过消费kafka消息，在LTS中可以看到所有消息内容，在这里我们可以根据筛选条件指定告警规则。

![](https://tech.qimao.com/content/images/2023/10/image-13.png)

配置好后，静等几分钟，当消息触发规则后，我们在飞书群里就会收到如下告警

![](https://tech.qimao.com/content/images/2023/10/image-45.png)

#### 2.华为云文档

- syslog数据采集

[https://support.huaweicloud.com/bestpractice-lts/lts_07_0019.html](https://support.huaweicloud.com/bestpractice-lts/lts_07_0019.html)

- 告警列表配置

[https://support.huaweicloud.com/usermanual-lts/lts_04_0060.html](https://support.huaweicloud.com/usermanual-lts/lts_04_0060.html)

### 五、总结

通过上面介绍能够得到以下结论

1. 我们可以感受到配置一个告警规则是多么简单，只需要了解yaml的编辑格式，和要查询到sql，填入后，kubectl发布一下就生效了， 当然我们可以更简单，通过流水线自动发布生效。
2. 业务侧繁杂的消息通知内容，我们可以all in one 通过kafka生产消费，实现可聚合的，可容错的，可削峰聚合的，可溯源的监控告警系统。

万丈高楼，根基最重要，在我们将这些繁杂的数据信息搭建完成后，后续的告警和监控更是鼠标随意点点就完成了。

# 通过Prometheus+grafana搭建可视化监控

[王强](https://tech.qimao.com/404/) 发表于 2023/05/09

### 1、背景

有时候我们想知道项目里面各个api的qps和耗时，或者请求第三方服务的qps、成功率、失败率等等。统计这个其实有很多方案，比如：

- envoy：可以从网络层统计（运维已支持），但是不支持业务自定义统计
- sls：pod日志接sls日志系统也能统计，目前部分项目已支持但仅有部分日志，比较耗资源，可视化支持较差
- Prometheus：支持多种监控指标，搭建简单，支持业务自定义统计，可视化支持较好

经过讨论之后，我们决定使用Prometheus，然后在运维平台的grafana中接入，再画出需要的图。本文简单的介绍下大致的流程。

### 2、项目里面接入Prometheus的监控指标

**2.1 什么是Prometheus？**

prometheus是一款开源系统监控和警报工具包，基于metric采样的监控，可以自定义监控指标，定时拉取数据，存储到一个时间序列数据库中，之后可通过PromQL语法查询。主要有以下特点：

- 多维数据模型，时间序列数据通过metric名以key、value的形式标识；
- 使用PromQL语法灵活地查询数据；
- 不需要依赖分布式存储，各服务器节点是独立自治的；
- 时间序列的收集，通过 HTTP 调用，基于pull 模型进行拉取；
- 通过push gateway推送时间序列；
- 通过服务发现或者静态配置，来发现目标服务对象；
- 多种绘图和仪表盘的可视化支持，以及报警；

**2.2 接入Prometheus包**

通过go get 命令安装相关依赖库，示例如下：

```
go get github.com/prometheus/client_golang/prometheus
go get github.com/prometheus/client_golang/prometheus/promhttp
```

首先在程序中加入如下代码，注册http服务，暴露端口和提供数据的路由地址

```
import (
   "github.com/prometheus/client_golang/prometheus/promhttp"
   "net/http"
)

func main() {
   go func() {
       http.HandleFunc("/_/metrics", promhttp.Handler().ServeHTTP)
       _ = http.ListenAndServe(":6060", nil)
   }()
}
```

运行程序，然后在浏览器中打开地址：http://localhost:6060/_/metrics，可以看到初始的metric数据，如下

```
go_gc_duration_seconds{quantile="0"} 5.8723e-05
go_gc_duration_seconds{quantile="0.25"} 5.8723e-05
go_gc_duration_seconds{quantile="0.5"} 0.000145591
go_gc_duration_seconds{quantile="0.75"} 0.000145591
go_gc_duration_seconds{quantile="1"} 0.000145591
go_gc_duration_seconds_sum 0.000204314
go_gc_duration_seconds_count 2
go_goroutines 10
......
go_info{version="go1.17.11"} 1
go_memstats_sys_bytes 1.9090192e+07
go_threads 11
promhttp_metric_handler_requests_in_flight 1
promhttp_metric_handler_requests_total{code="200"} 0
promhttp_metric_handler_requests_total{code="500"} 0
promhttp_metric_handler_requests_total{code="503"} 0
```

- go_* 为前缀的指标是关于Go运行时相关的指标，比如垃圾回收时间、goroutine数量等，其他语言的客户端库也会暴露各自语言特有的运行时指标。
- promhttp_* 为promhttp 工具包的相关指标，用于跟踪对指标请求的处理。

**2.3 添加业务所需要的自定义指标**

Prometheus有4个指标，具体如下：

- Gauges（仪表盘）：表示一个能够任意变化的指标，可增可减。
- Counters（计数器）：表示一个累积的指标数据，只增不减，除非监控系统发生了重置。
- Histograms（直方图）：在客户端把采集到的数据放到配置的bucket中，然后在服务端侧使用 histogram_quantitle() 函数通过区间来计算分位数；客户端性能低，服务端性能高，可支持聚合。
- Summaries（摘要）：客户端侧计算分位数，直接存储采集到的数据；客户端性能高，服务端性能低，不支持聚合；

由于我们项目有多个pod，需要聚合数据，所以选择Histograms指标。下面简单的封装下代码：

```

// HistogramVec 初始化全局变量
var HistogramVec = &histogram{
    mu:    &sync.RWMutex{},
    store: make(map[string]interface{}),
}

// 定义结构体
type histogram struct {
    mu    *sync.RWMutex
    store map[string]interface{}
}

// gen 通过label的方式，动态创建对象并注册
func (h *histogram) gen(name string, labels []string) interface{} {
    # 加锁，避免重复创建对象
    h.mu.RLock()
    if v, ok := h.store[name]; ok {
        h.mu.RUnlock()
        return v
    }

    h.mu.RUnlock()
    h.mu.Lock()

    opts := prometheus.HistogramOpts{
        Name:    name,                            // 指标名称
        Buckets: []float64{0.05, 0.1, 0.5, 1, 2}, // 根据实际要求定义buckets，默认单位是秒
    }

    # 创建对象
    histogramVec := prometheus.NewHistogramVec(opts, labels)
    # 注册服务
    _ = prometheus.Register(histogramVec)
    h.store[name] = histogramVec
    h.mu.Unlock()

    return histogramVec
}

// Observe 通过label的方式，写入自定义指标
func (h *histogram) Observe(name string, kv map[string]string, startAt time.Time) {
    var (
        lbNames, lbValues []string
        timeSub           = time.Now().Sub(startAt).Seconds()
    )
    for k := range kv {
        lbNames = append(lbNames, k)
    }

    sort.Strings(lbNames)
    for i := range lbNames {
        lbValues = append(lbValues, kv[lbNames[i]])
    }

    v := h.gen(name, lbNames)
    if v != nil {
        vv := v.(*prometheus.HistogramVec)
        vv.WithLabelValues(lbValues...).Observe(timeSub)
    }
}
```

需要注意的是要合理设置buckets的值，因为会把http请求，按照耗时分布在对应的bucket中（默认时间单位是秒），比如0.05s内完成的请求，会放在第一个bucket中；0.1s内完成的请求，会放在第二个bucket中，以此类推... 而服务端是通过bucket的区间来计算分位数画图的，所以所以bucket的粒度太大、或者太小都会影响准确度。可以参考官方的辅助函数prometheus.LinearBuckets() 和prometheus.ExponentialBuckets() 生成bucket。

下面启动http服务，然后添加自定义指标，代码如下：

```
// 获取gin实例
r := gin.Default()
// 中间件，监控每个路由
r.Use(func(c *gin.Context) {
    startAt := time.Now()

    c.Next()

    // 自定义指标的名称
    HistogramVec.Observe("http_request", map[string]string{
        "api":    c.Request.URL.Path,              // 每个路由的地址
        "status": strconv.Itoa(c.Writer.Status()), // 当前路由返回的http状态值
    }, startAt)
})

// 路由1，http返回200
r.GET("/api/t1", func(c *gin.Context) {
    // 随机返回http状态值，测试组合指标
    status := []int{http.StatusOK, http.StatusUpgradeRequired,,http.StatusInternalServerError}
    index := rand.Intn(len(status))
    c.JSON(http.StatusOK, gin.H{"a": "11"})
})
// 路由2，http返回非200
r.GET("/api/t2", func(c *gin.Context) {
    // 随机延迟，测试耗时
    rt := rand.Intn(1000)
    time.Sleep(time.Millisecond * time.Duration(rt))
    c.String(http.StatusInternalServerError, "pong")
})
// 启动程序
_ = r.Run(":8080")
```

运行命令go run main.go 启动程序，然后请求接口

```
curl http://localhost:8080/api/t1
curl http://localhost:8080/api/t1
curl http://localhost:8080/api/t1
curl http://localhost:8080/api/t2
curl http://localhost:8080/api/t2
curl http://localhost:8080/api/t2
```

然后在浏览器中打开地址：http://localhost:6060/_/metrics，可以看到刚刚添加的metric数据（http_request_*前缀，http_request为前面定义的值）， 如下：

```
http_request_bucket{api="/api/t1",status="200",le="0.05"} 1
http_request_bucket{api="/api/t1",status="200",le="0.1"} 1
http_request_bucket{api="/api/t1",status="200",le="0.5"} 1
http_request_bucket{api="/api/t1",status="200",le="1"} 1
http_request_bucket{api="/api/t1",status="200",le="2"} 1
http_request_bucket{api="/api/t1",status="200",le="+Inf"} 1
http_request_sum{api="/api/t1",status="200"} 5.5298e-05
http_request_count{api="/api/t1",status="200"} 1
http_request_bucket{api="/api/t1",status="500",le="0.05"} 2
http_request_bucket{api="/api/t1",status="500",le="0.1"} 2
http_request_bucket{api="/api/t1",status="500",le="0.5"} 2
http_request_bucket{api="/api/t1",status="500",le="1"} 2
http_request_bucket{api="/api/t1",status="500",le="2"} 2
http_request_bucket{api="/api/t1",status="500",le="+Inf"} 2
http_request_sum{api="/api/t1",status="500"} 0.00012158700000000001
http_request_count{api="/api/t1",status="500"} 2
http_request_bucket{api="/api/t2",status="500",le="0.05"} 0
http_request_bucket{api="/api/t2",status="500",le="0.1"} 2
http_request_bucket{api="/api/t2",status="500",le="0.5"} 3
http_request_bucket{api="/api/t2",status="500",le="1"} 3
http_request_bucket{api="/api/t2",status="500",le="2"} 3
http_request_bucket{api="/api/t2",status="500",le="+Inf"} 3
http_request_sum{api="/api/t2",status="500"} 0.46105082500000005
http_request_count{api="/api/t2",status="500"} 3
```

可以看到api和status两个参数值组合的数据，最后一列为累计计数，后缀*_bucket 为各个bucket的计数，后缀 __sum 为累积总和，后缀_ _count 为_累计计数_。如果label越多，生成的指标也会越多，所以应该控制下数量，否则内存会暴增。

### 3、采集项目中的metrics数据

**3.1 通过ServiceMonitor创建服务发现**

因为我们项目是部署在阿里云的k8s集群中，所以使用他们的ServiceMonitor服务创建自定义服务发现，会主动采集各个pod里面的metric数据，并存储。

准备工作：需要运维同学先配置好相关**基础依赖，然后在k8s集群中部署好项目（目前已直接支持）。**

首先需要先创建一个Service服务，yaml配置如下：

```
apiVersion: v1
kind: Service
metadata:
  name: demo-metrics-service   # 自定义服务名字
  namespace: default           # 命名空间
  labels:
    app: demo-metrics-service  # 自定义服务名字
spec:
  selector:
    app: http-deployment-name  # 对应Deployment服务名称
  ports:
    - name: demo-metrics-ports # 自定义服务端口名字
      port: 6060               # 上面http服务中暴露的Prometheus端口
      protocol: TCP
      targetPort: 6060
```

再创建ServiceMonitor服务，yaml配置如下：

```
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: demo-monitor-name         # 自定义monitor服务名字
  namespace: default              # 命名空间
spec:
  endpoints:
    - interval: 30s               # Prometheus对当前pod采集的周期,每隔30s拉取一次数据
      path: /_/metrics            # 采集路径，即上面http服务中定义的路由地址
      port: demo-metrics-ports    # 上面自定义服务端口名字
  namespaceSelector:
    matchNames:
      - default                   # 命名空间
  selector:
    matchLabels:
      app: demo-metrics-service   # 上面自定义服务名字
```

然后在k8s中部署好。在服务中，可以看到刚刚配置好的服务，并且能关联对应pod，表示成功，如下图所示：

![](https://tech.qimao.com/content/images/2022/12/image.png)

**3.2  也可以自己手动搭建Prometheus服务**

如果项目是单独部署在_ECS_上，也可以考虑从官网[https://prometheus.io/download/](https://prometheus.io/download/) 下载包，自己安装Prometheus server，然后在配置文件prometheus.yml中添加node_exporter（即监控地址）配置（采集的数据默认以文件形式存储在本地）,配置如下：

```
scrape_configs:
  # 添加需要收集机器的监控数据，可以添加多个节点
  - job_name: 'prometheus-node'   
    metrics_path: '/_/metrics'      #采集路径
    static_configs:
    - targets: ['127.0.0.1:6060']   #需要采集的地址
```

然后再重启Prometheus服务，就可以在可视化界面（比如http://localhost:9090） 查看到相应监控内容。如下图所示，表示成功。

![](https://tech.qimao.com/content/images/2022/12/image-8.png)

### 4、grafana上画图

指标数据有了，也能采集到了，那么接下来需要****将指标数据可视化**。**Prometheus自带UI其实也是可视化的，但其功能较为简单、无法实时关注相关监控指标的变化趋势，所以我们选择Grafana作为可视化的解决方案。

**4.1 什么是grafana？**

Grafana是一个跨平台的开源的度量分析和可视化工具，可以通过将采集到的数据查询再可视化展示出来，并及时报警通知。UI灵活，有丰富的插件，功能强大，主要有以下特点：

- 展示方式：快速灵活的客户端图表，面板插件有许多不同方式的可视化指标和日志，官方库中具有丰富的仪表盘插件，比如热图、折线图、图表等多种展示方式；
- 数据源：支持许多不同的数据源，每个数据源都有一个特定的查询编辑器，该编辑器支持对应数据源的查询语法，常用的数据源有：Prometheus，Elasticsearch，MySQL，PostgreSQL等
- 通知提醒：以可视方式定义重要指标的警报规则，在数据达到阈值时发送通知；
- 混合展示：在同一图表中混合使用不同的数据源，可以基于每个查询指定数据源；

**4.2 安装并设置数据源**

这一步运维同学已经弄好，可直接使用了。这里再简单的介绍下，从官网[https://grafana.com/grafana/download](https://grafana.com/grafana/download) 下载安装并安装好，默认是3000端口。启动grafana服务，在浏览器中输入：[http://localhost:3000](https://link.juejin.cn/?target=http%3A%2F%2Flocalhost%3A3000) 即打开了grafana的界面。点击左边的设置符号，会出现如下选项

![](https://tech.qimao.com/content/images/2022/12/image-1.png)

点击Data Sources选择数据源，这里选择prometheus数据源，然后点击进入数据源的配置页面，在url处填写prometheus server的服务器地址，即上面手动搭建prometheus server时使用的9090端口，如下：

![](https://tech.qimao.com/content/images/2022/12/WX20221211-204433@2x.png)

配置好数据源后，然后点击左侧的dashboard选项，新建一个dashboard控制面板，就是我们项目的控制面板，也可以新建分组等。

**4.3 创建**panel

在建好的控制面板中，新建一个panel，可以自定义选择图形，这里我们使用了默认的Time series图，来显示api接口的qps，如图所示：

![](https://tech.qimao.com/content/images/2022/12/image-3.png)

查询的Prometheus语法可根据需要，再自定义编写。比如下图所示，查询的是各个接口返回http状态为200的p99耗时。

![](https://tech.qimao.com/content/images/2022/12/image-4.png)

如果不确定想要啥图形，或者不知道查询语法怎么写。可以去Grafana官网下载Dashboard模板：[https://grafana.com/grafana/dashboards/](https://grafana.com/grafana/dashboards/)，搜索数据源为Prometheus的模块，把下载的json文件导入就可以了，很方便。

**4.4 **添加报警Alert Manager****

报警分为两部分：在prometheus server中添加报警规则并将报警传递给alert manager；然后alert manger再将报警发送消息通知，比如：钉钉、邮件、企业微信等。这部分当然也是运维同学配置好了，有兴趣的可以查看官方的配置文档：[https://grafana.com/docs/grafana/v9.3/alerting/](https://grafana.com/docs/grafana/v9.3/alerting/) 。然后我们在刚刚创建好的panel中点击alert创建报警规则，比如：每隔5分钟检测一次，平均耗时超过多少的，并且持续1分钟才发送报警，如图：

![](https://tech.qimao.com/content/images/2022/12/image-5.png)

然后再选择报警通知的方式（运维已配置好的）。

### 5、总结

以上，简单的介绍了golang + Prometheus + grafana整个流程，属于抛砖引玉，实际项目中的使用会比这复杂些。有了这套监控，我们可以随时查看项目的情况，比如：线上机器的负载等系统指标，业务中需要知道的数据等。印象比较深刻的是，有一次云书架改版升级，某个接口的qps从最开始的八百多，到两千多，再到新接口的六七千。从曲线图上很直观的看到了qps的变化，以及接口耗时的增加；然后进行一系列的业务降级，及数据库升配等操作，期间也灵活的临时添加了其他辅助业务的统计， 通过持续观察及对比这些指标，再配合其他资源负载情况，我们可以很容易知道当前项目的负载情况。

需要特别注意的是，Histograms指标写入的label参数最好是固定的几个值，如果值是动态的或者很多的话，内存可能会溢出（已踩过坑)。

# 系统测试中的Go代码覆盖率统计

[陈进松](https://tech.qimao.com/author/chen-jin-song/) 发表于 2023/10/12

## 背景

> 传统软件测试技术主要基于测试人员对业务的理解，但由于经验的局限性、被测系统的复杂性以及与真实业务数据的差距，肯定存在测试不充分的情况，所以，虽然整个测试流程很规范，但最终软件质量还是不尽如人意。随着分布式、微服务架构、大数据技术的出现，软件越来越复杂，迭代越来越快，测试的挑战性越来越大。引入系统测试的代码覆盖率统计，可以帮助研发识别无效代码，辅助测试提高测试覆盖度等

## 什么是代码覆盖率

1. 代码覆盖率指的是在对代码进行测试时，测试用例覆盖了代码中多少的语句、分支、函数或条件等，以百分比的形式表示。代码覆盖率是衡量测试用例覆盖效果的一种指标。
2. 代码覆盖率分为不同层次的覆盖，比如**语句覆盖率**、**分支覆盖率**、**函数覆盖率**等。不同层次的覆盖率反映了测试用例对代码执行的影响程度和覆盖程度，通常情况下，应该尽量追求达到更高的覆盖率，以保证代码的可靠性和稳定性。
3. 代码覆盖率和测试用例的设计和执行息息相关，合理的测试用例设计可以更好地覆盖代码，提高覆盖率。因此，应该充分考虑测试用例的设计，以保证更好的覆盖率和代码质量。

## 应用价值

1. 识别无效代码
2. 系统测试阶段，通过实时覆盖率报告，定位未覆盖过的代码块，确认是否需要被覆盖，完善测试用例覆盖度
3. 临时修改代码，确认是否有未覆盖的代码
4. 通过观察覆盖率报告中的代码覆盖频次，可以帮助我们找到代码中的热点，优化关键路径上的代码，提高系统的性能和运行效率

## 覆盖率统计方案

- 静态代码覆盖扫描工具 golangci-lint
    
    ```shell
    # 进入go项目根路径下执行
    golangci-lint  run --disable-all  --enable unused
    ```
    

![------_da078ac0-fed9-411f-b11c-14b6b9bbae58](https://tech.qimao.com/content/images/2023/07/------_da078ac0-fed9-411f-b11c-14b6b9bbae58.png)

- 单元测试覆盖率统计
    
    ```shell
    go test -coverprofile=coverage.out
    go tool cover -html=coverage.out -o=report.html
    ```
    
- 系统测试覆盖率统计
    
    我们知道 jacoco 是一个用于 Java 代码覆盖率度量和报告生成的工具，那么 Go 语言是否有类似的比较成熟的统计工具呢，答案是有。业内 Go 语言用的比较多的覆盖率框架有 goc 和 gopher，goc的优势在于支持覆盖率计数清零、支持基于 Pull Request 的增量代码覆盖率等特性
    

## goc简介

> goc 由七牛云团队研发，是专为 Go 语言打造的一个综合覆盖率收集系统，尤其适合复杂的测试场景，比如系统测试时的代码覆盖率收集以及精准测试。  
> 目前已开源 → [GitHub地址](https://github.com/qiniu/goc)

## 框架图

![Pasted-Graphic-1-1](https://tech.qimao.com/content/images/2023/07/Pasted-Graphic-1-1.png)

## 快速上手

### goc部署实施流程

1. goc命令行工具,两种方式获取
    
    - 自行从github上下载源码编译
    - 使用[goc cli](https://codeup.aliyun.com/qimao/qaarm/goc)上编译好的版本，基于源码v2版本做了一些改进，代码仓 → [goc-sd](https://codeup.aliyun.com/qimao/qaarm/goc-sd)
2. goc server部署，用于与被测项目交互，收集覆盖率数据等
    
    ```shell
    goc server --host 10.xxx.xxx.239:7779
    ```
    
    ==_需要确保被测项目能够访问goc server_==
    
3. 代码编译
    
    makefile方式编译的项目，可以修改项目文件的Makefile，增加goc编译方式，示例
    
    ```shell
    cbuild:
        goc build -o rta -v -a $(GOFLAGS) -tags=jsoniter cmd/main.go --gochost "10.xxx.xxx.239:7779"
    ```
    
    ==_这里的gochost填写的即[步骤2](https://tech.qimao.com/test/#goc%E9%83%A8%E7%BD%B2)中的goc server的socket地址_==  
    如果想最小化改动Go编译命令，也可以在编译被测项目时不指定gochost，在被测项目启动时添加环境变量即可
    
    ```shell
    export GOC_CUSTOM_HOST="10.xxx.xxx.239:7779"
    ```
    
4. 将以下命令添加到flow流水线任务中Go项目构建命令
    
    ```shell
    git clone https://xxx.com/qimao/qaarm/goc.git
    chmod +x goc/goc && mv goc/goc /usr/local/bin
    ```
    
    _makefile方式编译的，可直接修改makefile命令_
    
    ```shell
    # make build
    make cbuild
    ```
    
    _直接编译的项目参考[步骤3](https://tech.qimao.com/test/#%E4%BB%A3%E7%A0%81%E7%BC%96%E8%AF%91)中的命令_  
    被测项目启动后，首先会发起与goc server建立连接的请求  
    ![image-20230626223410984](https://tech.qimao.com/content/images/2023/07/image-20230626223410984.png)  
    goc server收到请求后，完成连接建立  
    ![image-20230626223915707](https://tech.qimao.com/content/images/2023/07/image-20230626223915707.png)  
    到此步骤，已经可以开始愉快的收集被测项目的代码覆盖率数据了~
    

### 收集覆盖率数据和测试报告

- 原生方式
    
    - Api
        
        - 获取被测项目注册信息  
            接口地址: /v2/agents  
            请求方法: GET  
            请求参数:
            
            |参数|类型|是否必填|描述|
            |---|---|---|---|
            |id|strings|否|筛选被测项目注册id|
            |status|int|否|筛选被测项目实例注册状态，1:disconnected,2:connected|
            
              
            请求示例:
        
        ```shell
        # 请求项目注册id为1519，状态是连接中的注册信息
        curl https://xxx.qimao.com/server/v2/agents?id=1519&status=2 
        ```
        
        ```json
        {
            "items": [
                {
                    "id": "1519",
                    "rpc_remoteip": "10.xxx.xxx.34",
                    "watch_remoteip": "",
                    "hostname": "rta-web-tuia-9f9c7fdc4-k47p4",
                    "cmdline": "/rta web --media=tuia",
                    "pid": "1",
                    "extra": "",
                    "token": "cbf28011c66246bf98f5eb9c3634d1f1",
                    "status": 2
                }
            ]
        }
        ```
        
        - 获取覆盖率数据  
            接口地址: /v2/cover/profile  
            请求方法: GET  
            请求参数:
            
            |参数|类型|是否必填|描述|
            |---|---|---|---|
            |id|strings|否|筛选被测项目注册的id|
            |skippattern|strings|否|过滤被测项目包名|
            |extra|string|否|筛选被测项目extra信息|
            
              
            请求示例:
        
        ```shell
        # 请求项目注册id为1,2,3的覆盖率数据
        # extra参数可以从/v2/agents接口返回中获取
        curl https://xxx.qimao.com/server/v2/cover/profile?id=1,2,3 
        ```
        
        ```json
        # 返回profile的值即为覆盖率的实时统计数据
        {"profile": "mode: count\nkol/api/data/data.pb.go:33.44,35.29 2 0\nkol/api/data/data.pb.go:35.29,39.3 3\n"}
        ```
        
    - 命令行
        
        ```shell
        [httpd@bigdata_platform_shanghai_test goc]$ goc2 profile get -h
        Usage:
          goc profile get [flags]
        
        Flags:
              --extra string    specify the regex expression of extra, only profile with extra information will be downloaded
          -h, --help            help for get
              --host string     specify the host of the goc server (default "127.0.0.1:7777")
              --id strings      specify the ids of the services
          -o, --output string   download cover profile
              --skip strings    skip specific packages in the profile
        
        Global Flags:
              --gocdebug   run goc in debug mode
        ```
        
- 采集工具封装  
    介绍到这里，整个系统测试覆盖率数据收集的流程跑通了，但是要真正投产，还需要解决一些实际使用过程中遇到的问题，如：
    
    - 如何完成定时采集，定时更新覆盖率
    - 重启服务，重新发版，已经收集的覆盖率数据会丢失
    - 代码变更后，如何完成新老覆盖率数据的合并  
        ...
    
    > 为了解决上述问题，更加方便的管理覆盖率数据，诞生了goc-report小工具，[codeup地址](https://codeup.aliyun.com/qimao/qaarm/goc-report)，与goc的命令行工具使用方式类似，克隆下来使用Go编译成可执行文件即可使用，goc-report原理是与goc server提供的api交互，处理覆盖率数据，并生成覆盖率报告
    
    **goc-report使用场景**
    
    - 定时采集，定时更新覆盖率
        
        ```shell
        [httpd@bigdata_platform_shanghai_test goc]$ goc-report update -h
        update goc profile
        
        Usage:
          goc-report update [flags]
        
        Flags:
          -s, --app string          指定需要处理覆盖率数据的被测服务
          -h, --help                help for update
          -i, --interval duration   执行定时任务的时间间隔 (default -1s)
          -p, --path string         工作目录(绝对路径) (default "/opt/case/goc")
              --project string      被测服务代码仓库的名称
        ```
        
        _这里有个参数-i，未传值则只更新一次，传入时间间隔，如30s，则每30s更新一次覆盖率数据_  
        处理流程
        
        1. 根据传入app参数筛选出当前活跃中的被测项目注册id
        2. 根据注册id筛选出被测项目的实时覆盖率数据
        3. 将覆盖率数据持久化到本地文件
        4. 合并上一次得到的覆盖率文件，清除已经合并的文件
        5. 更新被测项目分支代码
        6. 生成覆盖率报告到指定目录，通过nginx代理访问  
            ==_假设工作目录设置为/opt/case/goc,那么最终生成的报告路径为/opt/case/goc/report/${app}/report.html_==
    - 清除被测项目的覆盖率数据
        
        ```shell
        [httpd@bigdata_platform_shanghai_test goc]$ goc-report clean -h
        clean goc profile
        
        Usage:
          goc-report clean [flags]
        
        Flags:
          -s, --app string   指定需要处理覆盖率数据的被测项目
          -h, --help         help for clean
        ```
        
- 系统测试覆盖率报告生成和查看
    
    - 生成报告工具还是采用go原生的方式，即go tool cover命令，该命令已封装在goc-report中
        
    - 值得注意的是，在生成某个被测项目的覆盖率报告之前，需要将被测项目的测试分支下载到GOROOT的src目录下，因为go tool cover命令在执行的过程中，会从GOROOT下找到覆盖率数据中对应包的源代码，完成html报告的渲染
        
    - 最终的覆盖率报告生成的路径即上文提到的${path}/report/${app}/report.html,通过nginx代理访问  
        灰色代表未跟踪，即未纳入统计范围  
        红色代表未覆盖  
        绿色代表已覆盖，颜色越深，覆盖次数越多  
        ![系统测试覆盖率报告](https://tech.qimao.com/content/images/2023/07/------_4c44810f-094b-49a1-8466-116ce91066c0.png)
        

## goc源码简析

### goc的覆盖率数据收集的原理

#### goc build大致流程

```go
func (b *Build) Build() {
	// 1. 拷贝至临时目录
	b.copyProjectToTmp()
    // 5. 清理临时目录
	defer b.clean()
	log.Donef("project copied to temporary directory")
	// 2. 更新go.mod文件，主要是解决本地依赖问题
	b.updateGoModFile()
	// 3. 注入插桩变量
	b.Inject()
	// 4. 在临时目录进行代码编译，即执行go build
	b.doBuildInTemp()
}
```

#### 代码注入流程

> 打桩的原理是借助AST语法树遍历整个文件，在识别到if、swith、select等地方，插入一行

- 通过go list -json命令，找出被测项目所有包

```go
func (b *Build) listPackages(dir string) map[string]*Package {
	listArgs := []string{"list", "-json"}
	if goflags.BuildTags != "" {
		listArgs = append(listArgs, "-tags", goflags.BuildTags)
	}
	listArgs = append(listArgs, "./...")
	cmd := exec.Command("go", listArgs...)
	cmd.Dir = dir
	var errBuf bytes.Buffer
	cmd.Stderr = &errBuf
	out, err := cmd.Output()
    ...
	dec := json.NewDecoder(bytes.NewBuffer(out))
	pkgs := make(map[string]*Package)
	for {
		var pkg Package
		if err := dec.Decode(&pkg); err != nil {
			...
		}
		pkgs[pkg.ImportPath] = &pkg
	}
	return pkgs
}
```

- 注入覆盖率收集相关代码流程
    
    - 遍历所有项目包，找到项目中所有的main包
    - 往main包注入插桩变量-addCounters()
    - 向 main package 的依赖包注入插桩变量，这里忽略了Go标准库和第三方库
    - 为每个 main 包注入 websocket handler
    - 在项目根目录注入所有插桩变量的声明+定义
    
    ```go
    func (b *Build) Inject() {
        var seen = make(map[string]*PackageCover)
        // 所有插桩变量定义声明
        allDecl := ""
        pkgs := b.Pkgs
        for _, pkg := range pkgs {
            if pkg.Name == "main" {
                // 该 main 二进制所关联的所有插桩变量的元信息
                // 每个 main 之间是不相关的，需要重新定义
                allMainCovers := make([]*PackageCover, 0)
                // 注入 main package
                mainCover, mainDecl := b.addCounters(pkg)
                // 收集插桩变量的定义和元信息
                allDecl += mainDecl
                allMainCovers = append(allMainCovers, mainCover)
                // 向 main package 的依赖注入插桩变量
                for _, dep := range pkg.Deps {
                    if packageCover, ok := seen[dep]; ok {
                        allMainCovers = append(allMainCovers, packageCover)
                        continue
                    }
                    // 依赖需要忽略 Go 标准库和 go.mod 引入的第三方
                    if depPkg, ok := pkgs[dep]; ok {
                        // 注入依赖的 package
                        packageCover, depDecl := b.addCounters(depPkg)
                        // 收集插桩变量的定义和元信息
                        allDecl += depDecl
                        allMainCovers = append(allMainCovers, packageCover)
                        // 避免重复访问
                        seen[dep] = packageCover
                    }
                }
                // 为每个 main 包注入 websocket handler
                b.injectGocAgent(b.getPkgTmpDir(pkg.Dir), allMainCovers)
                ...
            }
        }
        // 在工程根目录注入所有插桩变量的声明+定义
        b.injectGlobalCoverVarFile(allDecl)
        // 添加自定义 websocket 依赖
        // 用户代码可能有 gorrila/websocket 的依赖，为避免依赖冲突，以及可能的 replace/vendor，
        // 这里直接注入一份完整的 gorrila/websocket 实现
        websocket.AddCustomWebsocketDep(b.GlobalCoverVarImportPathDir)
        log.Donef("websocket library injected")
        log.StopWait()
        log.Donef("global cover variables injected")
    }
    ```
    

#### 注入代码后的项目

- 我们不妨将clean方法注释掉，启动编译流程，进入到临时编译目录，看看注入插桩代码后的项目架构和go文件  
    首先会在项目根目录生成一个新包，包内注入了一份完整的 gorrila/websocket 实现，用于与goc server通信  
    ![Pasted-Graphic](https://tech.qimao.com/content/images/2023/07/Pasted-Graphic.png)  
    此外，我们还注意到包下有个cover.go文件，打开会发现里面存放的就是goc注入的插桩变量，它是一个匿名结构体  
    ![Pasted-Graphic-1](https://tech.qimao.com/content/images/2023/07/Pasted-Graphic-1.png)  
    然后将该包导入到其他项目包中，并在每个代码块入口，注入只属于自己的结构体对象，进行执行次数累加，完成统计工作  
    ![Pasted-Graphic-3](https://tech.qimao.com/content/images/2023/07/Pasted-Graphic-3.png)  
    统计结构体解析
    
    ```go
        // 643034616338343535333432代表被测项目下的包唯一键，9代表是此包下的第9个go文件，
        var GoCover_9_643034616338343535333432 = struct {
        // Count:一个长度为 2 的 uint32 数组，用于记录每个基本块（basic block）的执行次数
        Count     [2]uint32
        // Pos:一个长度为 3 * 2 的 uint32 数组，用于记录每个基本块的位置信息。每个基本块的位置信息由三个 uint32 数值表示，分别表示起始行号、起始列号和偏移量
        Pos       [3 * 2]uint32
        // NumStmt:一个长度为 2 的 uint16 数组，用于记录每个基本块中的语句数量
        NumStmt   [2]uint16
        // BlockName:表示基本块所在的文件路径和文件名
        BlockName string
    } {
        BlockName: "rta/pkg/service/exporter/stat_mock.go",
        Pos: [3 * 2]uint32{
            13, 76, 0x20048, // [0]
            80, 107, 0x20049, // [1]
        },
        NumStmt: [2]uint16{
            9, // 0
            5, // 1
        },
    }
    ```
    
    还有一些技术实现细节，如
    - 如何在被测项目启动时建立与goc server的rpc连接
    - goc server如何进行覆盖率的收集
    - 在k8s部署场景下，如何实现多实例的覆盖率数据合并  
        这里由于篇幅关系，不做过多介绍，感兴趣的小伙伴欢迎交流~

## 总结

代码覆盖率是判断代码书写的质量，识别无效代码，评判测试覆盖度的重要工具

- 静态代码  
    对于静态的代码，要识别代码没有被使用，可以使用golangci-lint工具
- 单元测试  
    可以使用go test -cover工具
- 测试环境下的系统测试  
    可使用本文介绍的goc工具完成

 
# Go单元测试综合指南

[许愿龙](https://tech.qimao.com/author/xu/) 发表于 2023/03/28

## 前言

> 为了保证软件开发的质量，我们需要借助单元测试来确保代码的正确性，从而及早发现并修复代码中的错误。此外，单元测试还可以帮助开发人员更快地完成软件开发，并减少软件开发过程中的错误，同时也可以为新人提供指导。

```
最高的效率是不返工
问：单元测试能解决多少比例的程序BUG？
CHATGPT：这个问题没有一个确切的答案，因为这取决于软件的复杂程度和测试的质量。一般来说，单元测试可以帮助开发人员发现和修复大约50％到90％的程序错误。
```

## Getting Started  
单元测试要求

由于Golang语言(下面咱们将统称为Go)较为偏向工程性设计，所以Go在单元测试对文件名、方法名、参数都有很严格的要求

例如：

1. 测试文件命名必须 xxx_test.go 命名；
2. 测试方法必须是Test[^a~z]开头,不强制要求驼峰或下划线但风格建议保持一致；
3. 测试方法参数必须 t *testing.T 或 b *testing.B;
4. 测试文件和被测试文件必须在一个包中；

## 什么时候编写单元测试？

单元测试当然越早做越好，当然在日常开发中咱们不闭严苛的按照TDD（Test-Driven Development），非要强调先什么后什么，重要的是单测的高效性和实用性，最好是每写一个功能点就进行相应的测试，并且随时补充测试用例。

## Go语言如何编写单元测试？

非常简单！！！，相比C/CPP或者是java(或者大多数的主流编程语言)他们虽然都有较为成熟的单元测试框架，例如前者的Check或者后者的JUnit，但是这些框架本质还是第三方开发产品，相比之下Go语言官方则提供了语言级的单元测试支持，即"testing"包，而且仅仅通过go工具本身就能很方便的生成覆盖率数据，开发者只需编写单元测试用例，不用额外工程搭建任何第三方就可执行单测或生成测试覆盖率，可见Go语言官方对于单元测试的重视性。

|类型|格式|作用|
|---|---|---|
|单元测试函数|函数名前缀为Test|测试程序的一些逻辑行为是否正确|
|性能基准函数|函数名前缀为Benchmark|测试函数的性能|
|示例函数|函数名前缀为Example|为文档提供示例文档|

------ 本文使用jetBrains公司的Goland进行讲解（vscode使用比例较少）

> 编写简单的单元测试

我们来创建一个示例，创建名为 calc.go的文件

```
package main

package main

func Add(a int, b int) int {
	return a + b
}
func Mul(a int, b int) int {
	return a * b
}
```

借助Goland我们为calc.go生成并编写测试用例  
您可以右键点击函数，转到 _Generate_ | _Test for file /function_（生成 | 函数测试）。  
PS：快捷键以MAC为例 Command + N  
当然最标准的当然是 [GoLand 文档](https://link.zhihu.com/?target=https%3A//www.jetbrains.com.cn/help/go/create-tests.html%23create-test) 你可以进一步了解更多详细信息。

`calc_test.go` 中的测试用例可以这么写：

![](https://tech.qimao.com/content/images/2023/02/9b9353e2-d0aa-496a-8899-94aca1e72f2d-1.png)

Goland 为我们生成了如下单测文件

```
package main

import "testing"

func TestAdd(t *testing.T) {
	type args struct {
		a int
		b int
	}
	tests := []struct {
		name string
		args args
		want int
	}{
		{"Test1", args{1, 2}, 3},
		{"Test2", args{3, 4}, 7},
		{"Test3", args{5, 6}, 11},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			if got := Add(tt.args.a, tt.args.b); got != tt.want {
				t.Errorf("Add() = %v, want %v", got, tt.want)
			}
		})
	}
}

func TestMul(t *testing.T) {
	type args struct {
		a int
		b int
	}
	tests := []struct {
		name string
		args args
		want int
	}{

		{"Test4", args{12, 13}, 2},
		{"Test5", args{15, 16}, 4},
		{"Test6", args{185, 19}, 6},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			if got := Mul(tt.args.a, tt.args.b); got != tt.want {
				t.Errorf("Mul() = %v, want %v", got, tt.want)
			}
		})
	}
}


```

运行 `go test`，该 package 下所有的测试用例都会被执行。

```
PASS
ok      calc.go 0.499s
```

或 `go test -v`，`-v` 参数会显示每个用例的测试结果，另外 `-cover` 参数可以查看覆盖率。  
当然如果只想运行其中的一个用例，例如 `TestAdd`，可以用 `-run` 参数指定，该参数支持通配符 `*`，和部分正则表达式，例如 `^`、`$`

```
go test -v
=== RUN   TestAdd
=== RUN   TestAdd/Test1
=== RUN   TestAdd/Test2
=== RUN   TestAdd/Test3
--- PASS: TestAdd (0.00s)
    --- PASS: TestAdd/Test1 (0.00s)
    --- PASS: TestAdd/Test2 (0.00s)
    --- PASS: TestAdd/Test3 (0.00s)
=== RUN   TestMul
=== RUN   TestMul/Test4
=== RUN   TestMul/Test5
=== RUN   TestMul/Test6
--- PASS: TestMul (0.00s)
    --- PASS: TestMul/Test4 (0.00s)
    --- PASS: TestMul/Test5 (0.00s)
    --- PASS: TestMul/Test6 (0.00s)
PASS
ok      calc.go 0.197s
```

我们单独针对Mul进行单测 执行：go test -run TestMul -v

```
=== RUN   TestMul
=== RUN   TestMul/Test4
    calc_test.go:46: Mul() = 156, want 1561
=== RUN   TestMul/Test5
=== RUN   TestMul/Test6
    calc_test.go:46: Mul() = 342, want 3423
--- FAIL: TestMul (0.00s)
    --- FAIL: TestMul/Test4 (0.00s)
    --- PASS: TestMul/Test5 (0.00s)
    --- FAIL: TestMul/Test6 (0.00s)
FAIL
exit status 1
FAIL    calc.go 0.304s
```

加上 -cover 得到覆盖率

```
coverage: 100.0% of statements
```

以上就是一个从编写一个函数利用Goland生成单测，并对其进行单测的整体过程

有的同学说填充单测场景需要手动耗时，让我们利用chatgpt协助我们

![](https://tech.qimao.com/content/images/2023/02/image-23.png)

## 表驱动测试

```
相信细心的同学已经发现了，在我们编写一个简单的测试用例的时候我们使用了"表驱动"的方式进行单元测试(实际使用中需要丰富断言逻辑),这样做的好处可以在一个测试函数中主次验证场景或跟踪查看调用耗时，总结来说这会带来两个明显的好处。
```

1. 表驱动测试可以重复使用相同的断言逻辑，保持单测时模块的整洁。
2. 可以清晰的选择或输入测试涵盖的内容，利用事件Names可以协助我们跟踪测试条目，以及直观的查看这一单测所表达的真正意图。

## Benchmark 基准测试

当然Go语言也提供了基准测试框架，目的是测试一段程序运行时的性能。  
这里我们创建一个基准测试例子，`int`类型转为`string`类型 标准库和cast 都具备实现能力，一起看看哪种性能更佳。  
bench_test.go

```
package main

import (
	"fmt"
	"github.com/spf13/cast"
	"strconv"
	"testing"
)

func BenchmarkSprintf(b *testing.B) {
	num := 10
	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		fmt.Sprintf("%d", num)
	}
}

func BenchmarkFormat(b *testing.B) {
	num := int64(10)
	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		strconv.FormatInt(num, 10)
	}
}

func BenchmarkItoa(b *testing.B) {
	num := 10
	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		strconv.Itoa(num)
	}
}

//测试一下常用的cast包
func BenchmarkCast(b *testing.B) {
	num := 10
	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		cast.ToString(num)
	}
}

```

cmd 执行 go test -bench=. -run=none  
// -bench=. 表示基准测试所有  
// -run=one 表示匹配没有单测的方法并过滤掉单测输出  
// -benchtime=3s 表示指定执行时长  
// -benchmem 显示内存情况  
// -count=3 表示执行3次(使得结果更均衡)

```
goos: darwin
goarch: arm64
pkg: gotest
BenchmarkSprintf-8      90362154                40.47 ns/op
BenchmarkFormat-8       1000000000               2.019 ns/op
BenchmarkItoa-8         1000000000               1.976 ns/op
BenchmarkCast-8         100000000               30.20 ns/op
PASS
ok      gotest  15.159s
```

由此我们可以直观的得到，在时间效率上  
依次是：Itoa > Format > Cast > Sprintf  
解释一下 "BenchmarkSprintf-8" 中"-8"表示运行时对应的GOMAXPROCS值,"90362154"表示迭代循环次数 "40.47 ns/op"表示每次花费纳秒

b.ResetTimer() 可以避开初始化部分开销，更公平的计算每个测试独有运行时长

  
进阶

下面我们借助golang 单元测试，使用golang 自带的mock工具来完成mock测试  
你可以在这里 github.com/golang/mock 进一步了解

首先go get 对应的工具

```
go get -u github.com/golang/mock/gomock
go get -u github.com/golang/mock/mockgen
```

让我们一起写一个dome实践一下

创建一个 audit_repo.go 内含一个接口 其内实现一个GetAuditInfo()方法，获取审核信息

```
package mymock

type AuditRepo interface {
	GetAuditInfo() string
}

```

再创建一个 audit_svc.go 再其内部调用 audit_repo 的方法，获取信息

```go
package mymock

func getUser(m AuditRepo) string {
	user := m.GetAuditInfo()
	return user
}

```

接下来借助 mock 生成 mock_audit_repo.go  
注意：mock_audit_repo.go 可不是咱们自己创建的，而是通过 mockgen 生成出来的，执行如下命令生成；我们使用 source 方式生成

```
mockgen -source=audit_repo.go -destination=mocks/audit_repo_mock.go -package=mymock
* -source 源文件方式
* -destination 目标文件
* -package 指定包名(如果不指定的话会以mock_目标文件名命名)
```

详细的文档可以通过这里 111 进一步了解  
我们用mockgen生成了在mocks下的audit_repo_mock.go如下文件

```go
// Code generated by MockGen. DO NOT EDIT.
// Source: audit_repo.go

// Package mymock is a generated GoMock package.
package mymock

import (
	reflect "reflect"

	gomock "github.com/golang/mock/gomock"
)

// MockAuditRepo is a mock of AuditRepo interface.
type MockAuditRepo struct {
	ctrl     *gomock.Controller
	recorder *MockAuditRepoMockRecorder
}

// MockAuditRepoMockRecorder is the mock recorder for MockAuditRepo.
type MockAuditRepoMockRecorder struct {
	mock *MockAuditRepo
}

// NewMockAuditRepo creates a new mock instance.
func NewMockAuditRepo(ctrl *gomock.Controller) *MockAuditRepo {
	mock := &MockAuditRepo{ctrl: ctrl}
	mock.recorder = &MockAuditRepoMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockAuditRepo) EXPECT() *MockAuditRepoMockRecorder {
	return m.recorder
}

// GetAuditInfo mocks base method.
func (m *MockAuditRepo) GetAuditInfo() string {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "GetAuditInfo")
	ret0, _ := ret[0].(string)
	return ret0
}

// GetAuditInfo indicates an expected call of GetAuditInfo.
func (mr *MockAuditRepoMockRecorder) GetAuditInfo() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "GetAuditInfo", reflect.TypeOf((*MockAuditRepo)(nil).GetAuditInfo))
}

```

借助开始的单测步骤，我们为 audit_svc.go 创建对应的单测文件 并在内部使用mock方式。

```go
package mymock

import (
    "fmt"
    "testing"
    gomock "github.com/golang/mock/gomock"
)

func Test_getAuditInfo(t *testing.T) {
	mockCtl := gomock.NewController(t)
	mockMyFunc := mymock.NewMockAuditRepo(mockCtl)
	mockMyFunc.EXPECT().GetAuditInfo().Return("getAuditInfo")
	v := getUser(mockMyFunc)
	if v == "xiaomotong" {
		fmt.Println("return info!")
	} else {
		t.Error("get err audit info")
	}
}
```

这里可能有些同学有疑问 "和单测有什么区别呢？",ok！让我们来看下用单测要如何实现。

```go
//单测实现
func Test_getAuditInfoNotMock(t *testing.T) {
	auditRepo := AuditRepo.NewGetAudit() // 这里需要自己创建一个对象
	v := getUser(auditRepo)
	if v == "getAuditInfo" {
		fmt.Println("return info!")
	} else {
		t.Error("get err audit info")
	}
}
```

如此实现来看并没有任何问题，但是在实际中，NewGetAudit是需要实现外部dbClient的这时候我们执行这个单测就只会得到 fail，一定要实现其所依赖的一切Init，否则我们的单测就被阻塞住了。  
而使用Mock 方式，我们只需要Mock一个AuditRepo对象并预期一个返回值即可。  
让我们来Run一下

```go
go test -run Test_getAuditInfo -v -cover
```

结果

```
=== RUN   Test_getAuditInfo
return info!
--- PASS: Test_getAuditInfo (0.00s)
PASS
coverage: 50.0% of statements
ok      mymock  0.372s

```

哦吼，由于我们在audit_svc.go创建了两个方法，由此我们的覆盖率得到了"50.0%"

## 单元测试的一些通常原则

```
1. 每个测试单元必须完全独立、能单独运行。
2. 一个测试单元应只关注一个功能函数，证明它是正确的；
3. 测试代码要能够快速执行。
4. 不能为了单元测试而修改已完成的代码在编写代码后执行针对本次的单元测试，并执行之前的单元测试用例。
5. 以保证你后来编写的代码不会破坏任何事情；
6. 单元测试函数使用长的而且具有描述性的名字，例如都以test_开头，然后加上具体的函数名字或者功能描述；例如：func_test.go。
7. 测试代码必须具有可读性。
```

# 小结

合理使用官方提供的单测工具加上平时开发的细心使得每一个方法都实现其预期的 结果，结合测试用例将一些问题灭杀再萌芽之中，。  
最后让我们共同成长，每天进步一点点！

# 通过Ingress解决gRPC长连接负载不均衡的问题

[秦付灵](https://tech.qimao.com/author/vikiea/) 发表于 2022/09/05

## 背景

当前推荐引擎服务集群的外部访问入口及负载均衡使用的是阿里云的SLB,然而其负载均衡,由于grpc长连接的缘故,表现不尽如人意,尽管加了定时重连的相关配置,多个pod依然会有旱的旱死,涝的涝死的情况.

刚好新建的k8s集群版模型服务这种情况也比较明显,所以相关的优化就被提上了日程.听说新版的nginx-ingress已经对grpc有了较好的支持,于是他来了...

本文将通过小白入门的方式，通过其工作原理，操作上手，相关配置，访问方式，以及如何解决我们实际中遇到的问题等方面详细的介绍下Ingress相关的一些特性

## Ingress简介

在Kubernetes集群中，Ingress作为集群内服务对外暴露的访问接入点，其几乎承载着集群内服务访问的所有流量。Ingress是Kubernetes中的一个资源对象，用来管理集群外部访问集群内部服务的方式。我们可以通过Ingress资源来配置不同的转发规则，从而达到根据不同的规则设置访问集群内不同的Service所对应的后端Pod。

Ingress对我们来说其实应该并不陌生,尤其是服务端的研发同学.然而我们大多数使用Ingress的场景都是鉴于其对HTTP协议的强大路由支持和负载均衡的能力,可能有不少同学也会像我一样,在此之前对其对于gRPC的支持能力和水平要打上一个圆满的问号,尤其是针对长连接的负载均衡,**真的可以做到基于请求级别的轮询吗?**

## 工作原理

为了使Nginx Ingress资源正常工作，集群中必须要有个Nginx Ingress Controller来解析Nginx Ingress的转发规则。Nginx Ingress Controller收到请求，匹配Nginx Ingress转发规则转发到后端Service所对应的Pod，由Pod处理请求。Kubernetes中Service、Nginx Ingress与Nginx Ingress Controller有着以下关系：

- **Service是后端真实服务(可以理解为具体的pod)的抽象**，一个Service可以代表多个相同的后端服务。
- **Nginx Ingress是反向代理规则**，用来规定HTTP/HTTPS请求应该被转发到哪个Service所对应的Pod上。例如根据请求中不同的Host和URL路径，让请求落到不同Service所对应的Pod上。
- **Nginx Ingress Controller是一个反向代理程序**(可以理解为套了壳的nginx)，负责解析Nginx Ingress的反向代理规则。如果Nginx Ingress有增删改的变动，Nginx Ingress Controller会及时更新自己相应的转发规则，当Nginx Ingress Controller收到请求后就会根据这些规则将请求转发到对应Service的Pod上。

Nginx Ingress Controller通过API Server获取Ingress资源的变化，动态地生成Load Balancer（例如Nginx）所需的配置文件（例如nginx.conf），然后重新加载Load Balancer（例如执行`nginx -s load`重新加载Nginx）来生成新的路由转发规则。

![](https://tech.qimao.com/content/images/2022/09/nginx-ingress.png)

Nginx Ingress Controller可通过配置LoadBalancer类型的Service来创建SLB，因此可以从外部通过SLB访问到Kubernetes集群的内部服务。根据Nginx Ingress配置的不同规则来访问不同的服务。

## Nginx Ingress一些特点

- 支持金丝雀部署,蓝绿部署等
- 支持网关高度定制化场景,类似原生nginx一样所有参数可配置
- 提供七层流量处理能力与丰富的高级路由功能。
- 强大的路由功能
    - 基于内容、源IP的路由。
    - 支持HTTP标头改写、重定向、重写、限速、跨域、会话保持等。
    - 支持请求方向转发规则和响应方向转发规则，其中响应方向转发规则可以通过扩展Snippet配置实现。
- 支持HTTP,HTTPS,WebSocket,WSS和gRPC等协议
- 非证书变更时可支持配置的热更新
- 具有较好的可观测能力
    - 支持通过Access Log采集日志。
    - 支持通过Prometheus进行监控和告警配置
- 较好的运维能力.自行维护组件,可配置HPA进行扩缩容等
- 良好的服务治理能力
    - 服务发现支持K8s。
    - 服务灰度支持金丝雀。
    - 服务高可用支持限流。

## 操作上手

### 安装Nginx Ingress Controller组件

如果是使用阿里的ack集群,可直接在"运维管理"-"组件管理"-"核心组件"里选择该组件直接进行安装,默认且推荐安装在kube-system命名空间下

![](https://tech.qimao.com/content/images/2022/09/1662014473468.png)

如果是自主搭建的K8s集群,可以去[GitHub](https://github.com/kubernetes/ingress-nginx)下载对应的插件版本,直接安装配置即可,这里就不展开了.

安装完成后,默认会在kube-system下生成一个名为`nginx-ingress-controller`的`deployment`,以及一个对外的`service LoadBalancer`,默认叫`nginx-ingress-lb` ,其所负责的职责就是对外提供访问入口,对内将请求负载到`nginx-ingress-controller`

### 创建Ingress

安装的部分通过我们强大运维大佬们都能很快的响应和支持,一般不需要我们操心.而我们主要关注的部分,就是`nginx-ingress`资源(大致就相当于nginx.conf),通过配置`Ingress`资源,我们可以进行相应的路由匹配,负载均衡,请求转发等操作,大体上跟`nginx`类似

#### 两种创建`Ingress`的方式

##### Web方式

第一种,可以通过web的方式创建,如阿里的ack里,可以在"网络"-"路由"里新建

![](https://tech.qimao.com/content/images/2022/09/1662029793314.png)

##### 通过yaml方式创建

如下:

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "GRPC"
    nginx.ingress.kubernetes.io/proxy-read-timeout: '1'
    nginx.ingress.kubernetes.io/proxy-send-timeout: '1'
    nginx.ingress.kubernetes.io/service-weight: ''
  name: rec-gateway-ingress
  namespace: rec-test1
  labels:
    name: rec-gateway-ingress
spec:
  ingressClassName: nginx
  rules:
    - host: rec-test1.demo.com
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: rec-gateway-headless
                port:
                  number: 9090
  tls:
    # This secret must exist beforehand
    # The cert must also contain the subj-name grpctest.dev.mydomain.com
    # https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/PREREQUISITES.md#tls-certificates
    - secretName: demo.com
      hosts:
        - rec-test1.demo.com
```

- `name`：Ingress的名称，本例为rec-gateway-ingress。
- `host`：指定服务访问域名。
- `path`：指定访问的URL路径。SLB将流量转发到`backend`之前，所有的入站请求都要先匹配`host`和`path`。
- `backend`：由服务名称和服务端口组成。
- 服务名称：Ingress转发的`backend`服务名称。
- 服务端口：服务暴露的端口。

上面的配置中,重点需要关注的是`nginx.ingress.kubernetes.io/backend-protocol: "GRPC"`,这个是使nginx支持grpc的基础.且由于grpc是基于`HTTP2.0`的,且`nginx Ingress controller`只运行在`HTTPS`端口上,所以`tls`证书也必须要配置.具体申请和配置证书什么的可以让运维大佬们配合,方法可在网上自行查找.

`nginx.ingress.kubernetes.io/ssl-redirect: "true"`可以使请求重定向到加密协议.

`rules`就是我们主要关注的东西了,其相当于`nginx`里的`location`

### 路由规则

路由规则是我们通过URL请求不同后端服务所必不可少的东西,通过nginx-Ingress强大的路由规则,可以满足我们针对规则转发的各种想象

```yaml
...
rules:
    - host: rec-test1.demo.com
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: rec-gateway-headless
                port:
                  number: 9090
```

#### pathType

Ingress 中的每个 `path` 都需要有一个对应的 `pathType`，共有三种类型：

1. `ImplementationSpecific`: 这种类型的路径匹配规则依赖具体的 Ingress Controller 实现，具体实现可以将此类型作为一个单独的类型来对待，也可以将其视为 `Prefix` 或 `Exact` 类型
2. `Exact`: 完全匹配，区分大小写
3. `Prefix`: 按前缀匹配，区分大小写，前缀部分（可补充或去掉结尾的 `/`）需完全匹配,这个也是**默认类型**

我们一般就选`Prefix`即可.

#### 正则匹配和重定向

可以通过增加 `nginx.ingress.kubernetes.io/use-regex: "true"`注解的方式开启正则路由匹配,如

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: "/$2"
  name: rec-{{ .Values.app.name }}-ingress-api
  namespace: {{ .Values.namespace }}
  labels:
    name: rec-{{ .Values.app.name }}-ingress-api
spec:
  ingressClassName: nginx
  rules:
    - host: {{ .Values.app.name }}-{{ .Values.namespace }}.wtzw.com
      http:
        paths:
          - path: /api(/|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: rec-{{ .Values.app.name }}-headless
                port:
                  number: 8080
```

`nginx-Ingress-controller`会自动将path里的正则表达式转换为location里的路由规则,同时还可以通过`nginx.ingress.kubernetes.io/rewrite-target: /$2`注解的方式对请求进行重定向.这在通过路由前缀转发请求到后端但又不想对后端服务暴露该路由前缀时会很有用

#### backend

backend就相当于我们nginx里的`proxy_pass`,`fastcgi_pass`等,将匹配到路由的请求转发到对应的后端服务.在k8s中,这些后端服务就是各个指向具体服务的service,可以是另一个`SLB`,也可以是`NodePort`,`Headless`,甚至是`hostNetwork`等,这里我比较推荐使用`headless`,成本低,效果好,可以做到基于`gRPC`长连接级别的轮询

### 通过helm管理

在k8s集群里,一般`deployment`,`service`,`ingress`等都是由我们业务方自己来维护的,而`helm`能对这些资源的管理有着更好的支持,当然这个不在我们这次的讨论范围之内,有兴趣的可以主动去了解下.我比较推荐将`deployment`,`service`,`ingress`放到一个文件进行管理,刚好yaml对这种方式也有较好的支持,如下

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: {{ .Values.namespace }}
  name: {{ .Values.app.name }}
  labels:
    app: {{ .Values.app.name }}
    chart: {{ .Chart.Name }}
    instance: {{ .Release.Name }}
    managed-by: {{ .Release.Service }}
spec:
...
{{- if .Values.service.headless }}
---
# 配置 headless service
apiVersion: v1
kind: Service
metadata:
  namespace: {{ .Values.namespace }}
  name: rec-{{ .Values.app.name }}-headless
  labels:
    app: rec-{{ .Values.app.name }}-headless
spec:
  clusterIP: None
  selector:
    app: {{ .Values.app.name }}
    instance: {{ .Release.Name }}
  type: ClusterIP
  ports:
    - name: grpc
      port: {{ .Values.app.ports.grpc }}
      protocol: TCP
      targetPort: {{ .Values.app.ports.grpc }}
{{- end }}

{{- if .Values.service.ingress }}
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "GRPC"
    nginx.ingress.kubernetes.io/proxy-read-timeout: '1'
    nginx.ingress.kubernetes.io/proxy-send-timeout: '1'
    nginx.ingress.kubernetes.io/service-weight: ''
  name: rec-{{ .Values.app.name }}-ingress
  namespace: {{ .Values.namespace }}
  labels:
    name: rec-{{ .Values.app.name }}-ingress
spec:
  ingressClassName: nginx
  rules:
    - host: {{ .Values.app.name }}-{{ .Values.namespace }}.wtzw.com
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: rec-{{ .Values.app.name }}-headless
                port:
                  number: 9090
  tls:
    # This secret must exist beforehand
    # The cert must also contain the subj-name grpctest.dev.mydomain.com
    # https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/PREREQUISITES.md#tls-certificates
    - secretName: wtzw.com
      hosts:
        - {{ .Values.app.name }}-{{ .Values.namespace }}.wtzw.com
```

### 如何访问

#### 公网SLB访问

如果是通过阿里云容器服务管理控制台创建的Kubernetes集群在初始化时会自动部署一套Nginx Ingress Controller，默认其挂载在公网SLB实例上,请求时绑定ip访问即可

![img](https://cdn.jsdelivr.net/gh/vikiea/my_pic@master/uPic/2022/09/02/1662112707467.png)

#### 私网SLB访问

如果不同服务在同一个VPC下,也可以通过内网的方式进行直接访问,可以提供更高的效率和性能.我们可以修改(或者让运维配合)将公网的SLB调整为仅同VPC下可访问,重点增加`service.beta.kubernetes.io/alicloud-loadbalancer-address-type: intranet`注解

![img](https://cdn.jsdelivr.net/gh/vikiea/my_pic@master/uPic/2022/09/02/1662113162686.png)

#### 同时使用公网和私网SLB访问

当然,有时候我们可能期望集群内的服务既能允许公网访问，同时又希望能被同一个VPC下的其他服务直接访问（不经过公网）,这个时候就需要两者兼顾了.不过也很简单,额外部署一个`kube-system/nginx-ingress-lb-intranet`服务就行了

![img](https://cdn.jsdelivr.net/gh/vikiea/my_pic@master/uPic/2022/09/02/1662113152375.png)

### 如何建立GRPC连接

针对http请求,直接绑定slb的IP,通过host提供的域名以及路由直接请求访问即可,那么对于grpc请求,我们该如何处理呢?

#### 使用bloomRPC

如图,仅需开启TLS传输,并选择服务端证书即可

![image-20220902180926495](https://cdn.jsdelivr.net/gh/vikiea/my_pic@master/uPic/2022/09/02/1662113366641.png)

使用postman也是如此.

#### 通过SDK连接

端口选择443,传递客户端证书,如果server端关闭了校验客户端证书,则直接传空证书即可(空证书不等于空)

```go
var target = "rec-test1.demo.com:443"
var credential = insecure.NewCredentials()
if o.enabledSecurity {
  credential = credentials.NewTLS(nil)
}

dialOptions := []grpc.DialOption{
  grpc.WithTransportCredentials(credential),
  grpc.WithWriteBufferSize(writeBufSize),
  grpc.WithReadBufferSize(readBufSize),
  grpc.WithDefaultCallOptions(
    grpc.MaxCallRecvMsgSize(maxRecvBytes),
    grpc.MaxCallSendMsgSize(maxSendBytes),
  ),
  grpc.WithDefaultServiceConfig(`{"loadBalancingConfig": [{"round_robin":{}}]}`),
  grpc.WithChainUnaryInterceptor(chain...),
  grpc.WithKeepaliveParams(keepalive.ClientParameters{
    Time:                10 * time.Second,
    Timeout:             time.Second,
    PermitWithoutStream: true,
  }),
}
conn, err := grpc.DialContext(ctx, target, dialOptions...)
```

## Ingress与SLB比较

|服务|优点|缺点|
|---|---|---|
|SLB|1, 本质是一套转发规则,不占用实际物理资源  <br>2, 仅需一个公网IP即可对外提供访问入口,成本很低|1, 对于长连接支持不够友好,无法较好的做到负载均衡.如新pod起来时,由于长连接未断开,会造成请求很少转发到新的pod  <br>2, 对于滚动更新的响应不够及时.当服务滚动升级时,可能会由于SLB的endpoints列表没有及时更新,长连接未断开等原因导致无法平滑升级  <br>3, 对外暴露IP和端口,不够直观,且安全性较低|
|Ingress|1, 较好的负载均衡  <br>2, 路由转发功能强大,一个Ingress可以替代多个service的功能  <br>3, 人类友好的方式访问  <br>4, 其他优点见前面提到的内容|1, 需要部署一套Ingress-controller,会占用物理资源,增加一些维护成本  <br>2, 在高负载的场景下需要手动调优,对维护人员要求较高|

## **负载均衡对比**

下面验证一下其是否真的是基于请求级别的负载均衡.

前置条件:4个pod,1个client连接,分别以slb和Ingress的方式请求同一接口,持续70秒,看请求具体的响应情况

### **SLB请求**

![](https://tech.qimao.com/content/images/2022/09/1662346031745.png)

如图,由于client设置了长连接的活跃周期是60秒,在60秒内,请求只落在gateway-5bfbd6686f-c4vfg，一分钟后重新选择，又只落在gateway-5bfbd6686f-fvn2m上,虽然在连接数较多的时候也可能达到长连接的负载均衡，但是多次实验中,发现其在重连接时很可能会与先前的pod重新建立连接,另外由于我们的服务是对内的,实际的连接数并不多,所以这种负载均衡不能给我们带来很好的保障,如下图:

![](https://tech.qimao.com/content/images/2022/09/1662347027937.png)

### **Ingress请求**

![](https://tech.qimao.com/content/images/2022/09/1662347107480.png)

替换成Ingress请求后,发现其始终是均匀的,所有的pod都可以做到雨露均沾.且经试验新增或者删除pod时,请求也能平滑过渡,而阿里的SLB会出现偶尔的请求不可用问题,这是由于其endpoints没有及时更新或与ipvs不同步导致的.

**至此验证了Ingress对于基于gRPC的长连接确实做到了请求级别的轮询支持**，也基本解决了我们服务请求负载不均衡的问题

## 总结

Ingress是一套经过了时间的考验非常成熟的解决方案,nginx-Ingress又依托于nginx的强大历史背景,其性能和带来的价值都是让人值得称赞的.推荐引擎此次接入Ingress,不但可以解决让我们头疼已久的负载均衡问题,而且让我们节省了多个SLB的公网IP开销,同时新的通过域名请求的方式,对我们日常开发,性能问题排查,需求测试等都带来了较大的便利


# MySQL 主备延迟优化案例

[许松](https://tech.qimao.com/author/dean/) 发表于 2021/09/22

> 背景介绍

本文根据MySQL主备延迟问题解决、分析的过程，描述数据库主备延迟可能的原因和实际的处理方法。业务差异造就了数据库的场景不同、配置不同、参数不同，但是技术上都有共通的原理，希望各位同学能从本文中得到借鉴。附件中总结了主备同步的基础知识，以供参考。

# 1 问题描述

DB日常巡检过程中，发现阿里云RDS For MySQL实例主备两节点出现较长时间的延迟。通过排查不同的时间段，发现主备延迟一直存在，每天成规律的上下波动（图1），从昨天开始主备延迟呈持续上升状态（图2）。

![](https://tech.qimao.com/content/images/2021/09/1-2.png)

图1

![](https://tech.qimao.com/content/images/2021/09/2.png)

图2

高可用版RDS数据库主备出现严重的延迟，高可用特性就会丢失。如遇主库突发故障，就会发生主备之间无法正常切换，导致业务的中断。实际生产环境中高可用是业务连续性运行的保障，主备库之间不能出现长时间的延迟情况，否则将造成不可评估的影响。

**注：**阿里云RDS高可用版主备延迟现阶段不支持报警监控（只读实例有主备延迟监控），备库节点访问权限未开放，只能人工去检查实例监控指标。

# 2 环境介绍

a、数据库版本：阿里云MySQL8.0 rds20200430。  
b、配置信息：常规实例-高可用版32C 256G，最大IOPS：72000。  
c、数据量：2.29/3T(对MYSQL单实例来说是个大实例)。  
d、架构：高可用版RDS For MySQL主备两节点提供高可用架构。参考下图3：

![](https://tech.qimao.com/content/images/2021/09/3-1.png)

图3

# 3 实例信息

首先明确数据库负载情况，根据监控查看发现主库IO、网络、CPU、内存等指标都没有瓶颈问题。和研发人员交流获取相关信息，对实例实际承载业务情况和运行的负载情况进行梳理记录。

## 3.1  主库有大量insert语句

7*24小时“单条”insert写入（业务架构原因，DB前端没有削峰系统，insert不能批量插入），低峰每秒4000次插入，高峰每秒8000次插入。并且随着业务量的变化，每秒的SQL插入指标还在递增过程中。

## 3.2  主库有批量delete语句

a、业务在结构设计初期做了分库、分表。  
b、一个实例24个库，每个库256张表，全实例6144张表。  
c、对于所有表，每天定时删除七天前的数据，删除方式为根据表名、时间戳获取删除数据的最大id，单表、单条delete进行一次性删除。

## 3.3  主库有少量update语句

每秒大约70次和业务相关的update操作，update为单行更新。极少数情况会更新多行数据，多行更新的行数均为个位数（该点在问题排查的过程中也引起过一定的怀疑，最后通过binlog日志的解析进行了明确）。

## 3.4  主库表分析

实例因为表的delete删除操作导致每张表都存在较大的碎片率，全库前一天做了大量Alter Table Table_Name Engine=Innodb操作，数据库负载有了一定上升，但主机的相关资源没有瓶颈（MySQL表分析是重操作，一定结合实际业务情况决定操作区间，建议在业务低峰期进行。）。

## 3.5  主机负载

a、CPU平均使用率30%，无过大波动。  
b、内存初始化值80%，无波动。  
c、IOPS峰值1.6W，平均1W，最大IOPS：72000。  
d、网络流量平均50MB/s，无过大波动。

## 3.6  备份情况

设置阿里云RDS For MySQL 默认备份每天一次全备，备份动作实际在备库进行，每日全备开始时间为4点，完成时间大约17点，备份时间较长。

# 4 问题分析

## 4.1  全库表分析排查

### 4.1.1  场景分析

数据库延迟从昨天开始持续性的增加，并未像往常一样每日阶段性的主备延迟为零。该实例昨天唯一做的变更就是进行了表分析操作。根据以往经验数据库百分之90%以上都是因为变更引发的，先对表分析操作进行排查。

前面我们也提到了，因为业务模型决定所有业务表每天需要定时删除七天前的数据。MySQL使用delete进行数据删除之后，占用的空间不会进行释放（Truncate Table会释放空间）。尽管占用的空间一定程度上会被重新使用，但是表的碎片率会随着删除次数的增多，越来越高。会造成空间资源严重浪费、统计信息不准确，从而造成SQL走了错误的执行计划，造成数据库出现大量慢查询。因此MySQL需要定期进行表分析来降低表的碎片率。

然而Mysql主备同步是基于binlog 进行的。我们知道传统的半同步复制在2PC的commit stage阶段后会将sql传输到从库，该模式下主库宕机，可能会造成主备数据差异。无损的半同步复制在2PC的sync stage阶段会将SQL传输到从库（阿里云RDS默认无损复制）。当主库在进行表分析没有完成的情况下，从库是无法获取执行表分析的SQL语句，这就造成了主备之间存在延迟，当实例主库进行大批量的进行表分析操作，主备延迟时间确实会增高，但正常应该很快会追平。

我们发现主备存在延迟，且延迟没有像往常一样呈现周期性波动，而是延迟的时间越来越长（如下图4所示），第一想到的就是昨天对数据库的变更操作“表分析”。查看数据，发现延迟数据上升的时间段和主库进行表分析的时间段重合（表分析操作还在持续运行中）。猜想是因为表分析造成的，因为表分析的语句需要执行完之后，才能传输到备库，所以会造成主备延迟（正常主库表分析完成、备库再执行，应该很快会追平）。

![](https://tech.qimao.com/content/images/2021/09/4.png)

图4

根据现有情况判断主备之间的延迟可能是因为主库在进行大批量的表分析造成的。根据前面查看主备延迟监控，该实例的主备延迟是周期性的波浪线，那么正常状况下主备延迟会归零。考虑到我们人工进行了表分析，所以等待表分析被备库执行完成之后，再进行问题分析。

### 4.1.2  实际结果

等待期间在测试环境中进行相关测试。我们在主库进行大批量表分析，发现表分析确实会造成主备延迟的增加，但是正常情况表分析完成之后，主备库的延迟会逐步追平。

针对生产环境，我们联系阿里云后台人员，在备库查看正在运行的sql、解析备库最新的Binlog日志，发现表分析已经完成。等待一段时间后，主备延迟并没有像期望那样追平归零，甚至增大的趋势都没有降低，主备的差距越来越大。

## 4.2  备库备份分析

### 4.2.1  场景分析

表分析确实会造成主备延迟，现象是明确的，正常情况下延迟应该会追平，但现阶段延迟持续增加，就应该是其他因素影响的了。

重新进行检查，因为无法直接观察获取备库DB运行状况（无权限），尝试对延迟的数据信息进行统计。检查发现备库做全备的时候，主备延迟增加的速率会明显增加，备份完成之后，延迟增加速率会相对减缓。考虑该实例的数据量较大，备份的时间相对较长，且DB在备份的过程会获取相关表的锁资源，一定程度会影响备库的SQL重做的效率。

**两点猜测**：一是因为备份文件太多，太大，导致的恢复无法追平。二是备库在备份的时候主机资源不足，影响SQL重做的效率。备份影响主备延迟的猜想测试分析过程如下表：

|   |   |   |   |   |   |   |   |   |   |   |
|---|---|---|---|---|---|---|---|---|---|---|
|时间|20210531|20210601|20210602|20210603|20210604|20210605|20210606|20210607|20210608|20210609|
|备份计划时间|4-5点|4-5点|4-5点|4-5点|4-5点|无备份|无备份|7-8点|4-5|4-5|
|备份开始时间|||4:56-17:01点|4:46-15:34点|4:46-16:02点|||7:05-17:02点|4:22-15:05点||
|延迟转折区间|||3:00-19:20点|4:55-15:05点|4:25-15:35点|5:00-11:20点|4:00-10:20点|2:20-08:55点|4:30-12:15点||
|延迟区间差值|||16小时20分|11小时40分|10小时45分|6小时20分|6小时20分|6小时25分|7小时45分||
|对应延迟时间|||180000-<br><br>197550|183381-<br><br>193204|179892-<br><br>194244|184333-<br><br>198219|180007-<br><br>194582|174021-<br><br>188587|181222-<br><br>194482||
|172800s 两天|3600 1小时||||||||||
|Delete脚本执行|||2点半执行的|2:00-4:16点|2:00-4:20点|2点开始|2:00-4:51点|2:00-5:07点|2:00-4:36点||

### 4.2.2  实际结果

通过调整备份的开始时间、修改备份的频率，完成上表，对比发现备份过程确实会造成延迟问题。但是不进行备份的情况下，延迟并没有降低，只是在无备份操作的情况下，主备延迟增加速率有一定减缓。备份操作，只是主备延迟持续增加速度的一个影响因素。主备延迟增加问题还是无法改善、修复。

对于备库主机资源的不足的猜想，根据阿里云后台提供的数据，主机资源没有瓶颈、DB资源也没有瓶颈。如下图5所示：

![](https://tech.qimao.com/content/images/2021/09/--5-2.png)

图5

## 4.3  大事务delete影响分析

### 4.3.1  场景分析

查看DB监控和从开发了解到，该实例的特点是7*24小时大量单行的insert、每天的定时delete、少量的update。现阶段我们明确知道，表分析会造成主备延迟，备份影响延迟增长的速率。根据以往经验大事务造成主备延迟的情况非常的多。

主备发生延迟，正常优化的第一步也应该是将大事务的sql进行拆分。该实例因为业务原因，大事务delete操作SQL拆分、修改需要一定时间，排除其他问题现象后。同步进行的delete大事务的拆分、修改也已经通过测试。沟通阿里云后台人员提供了相关参考建议：创建删除依赖的索引，改成多线程并发删除。参考如下：

|   |
|---|
|create table batch_delete(batch_id,pk_id) as select round(pk_id/100,0) batch_id,pk_id from src_tbl where data_to_be_deleted;<br><br>create index batch_delete_bid_idx on batch_delete(batch_id);  <br>多个线程并行  <br>  <br><br>delete from src_tbl where pk_id in (select pk_id from batch_delete where batch_id = xxx；commit；|

考虑到该实例有6千多张表，如果创建索引，创建的对象太多，并且在不明确的情况下，不期望在生产环境主库上做较大的变动。换个思路我们尝试将delete每次删除的数据量降低、并将单次删除的并发提高。逐步进行测试。

（1）delete并发从6持续增加到20。  
（2）delete从按照条件获取删除ID，每次删除1W、5K、1K、0.5K）。

### 4.3.2  实际结果

并发的提高，单个delete 的删除量的调整。delete 任务的完成时间随着调整测试在不断的发生变化。但是对于该实例的主备延迟的优化，效果甚微，主备延迟的还在持续缓慢增长。（怀疑人生，正常的主备问题通过该方案能有缓解，亦或解决，但是该实例问题顽固存在）。

## 4.4  真假大事务Update语句分析

### 4.4.1  场景分析

检查DB引擎监控发现，update语句从监控上看是大事务，update_ps 27，监控对应的innodb_rows_updated 8611，并不是研发最开始所说的单行更新。如下图6所示：

![](https://tech.qimao.com/content/images/2021/09/image-14.png)

图6

研发人员告知update每次都是单行更新，特殊场景下会更新两三行，不是大事务。但从监控指标来看，单个update的SQL确实对应更新几百行数据。

### 4.4.2  实际结果

原因不明确，监控指标含义不清。实际测试分析，根据监控指标的时间点，找到实际binlog日志。进行解析分析。证实Update都是单行更新。该监控指标并不是实际的更新行数。所以主备延迟和update语句无关（后续实验证明，insert指标和delete指标都是准确的。Update指标不对应sql语句实际更新行数，而是其他指标意义）。

## 4.5  PK与UK的排查

### 4.5.1  场景分析

怀疑是否是因为表对象没有PK或者UK造成的？因为在DML SQL发生时，主键或者唯一键会影响到SQL的性能。（问题排查过程，没有方向，先排除可能影响的因素）。

### 4.5.2  实际结果

捞取频率较高的SQL语句，检查发现Update与Delete在删除的时候都有合理的主键和唯一键，主库没有慢SQL。

## 4.6  中间过程与问题发现

### 4.6.1  中间过程：

经过现象分析，我们能确定的：

**a、主备主机资源没有瓶颈。  
b、网络资源不是瓶颈。  
c、表分析不是主要原因。  
d、备库备份不是主要原因。  
e、delete语句经过优化后，实例无大的事务。  
f、主键、唯一键都是最优创建，没有全表扫的情况。**

按照经验主备延迟在上述问题排查完全之后，应该能够初步解决主备延迟的问题。但是实际并没有解决。回头再次查看mysql 关于主备同步的相关参数，回顾到了并行恢复优化的相关参数-WRITESET（很早就出来的参数，但是一般场景都使用不到，详情见附件1）。

### 4.6.2  问题发现

主备并行恢复基于组提交的方式，binlog_transaction_dependency_tracking=commit_order（阿里云RDS默认参数），binlog生成按照组提交的方式。从库slave_parallel_type=logical_clock；基于组提交的并行恢复，同一组提交的事务的恢复没有锁冲突。同一个Group内的事务将会在Slave上并行回放。

解析binlog日志发现发现：sequence_number不同，last_committed相同的情况很少。数量基本都是个数级别。从现象上分析从库的并行恢复正常进行，但是恢复的速率非常不好（前期问题排查，从库SQL运行正常，没有锁相关的事件，没有考虑到并行恢复的效果）。实例binlog解析内容,参考如下图7：

![](https://tech.qimao.com/content/images/2021/09/6-1.png)

图7

## 4.7  WRITESET并行恢复测试

### 4.7.1  场景分析

对该实例解析了binlog，发现binlog中last_commited的数值几乎都不同，猜想从库的并行恢复的效果不会太好。结合前期的测试和问题排查，数据库每时每刻都有大量的单条insert插入语句。现在证实组提交的效果不好，从库相当于是单线程的SQL恢复。询问阿里云工程师明确，writeset参数在阿里云RDS上还没有大规模的使用，且不是默认参数。主备延迟持续增加确实可能是因为并行恢复效果不好。RDS默认参数为commit_order，需要修改成writeset进行相关测试(writeset相关知识可参考附件1中的内容)。

|   |
|---|
|主库环境修改：<br><br>（1） set global transaction_write_set_extraction=XXHASH64;         <br><br>（2） set global binlog_transaction_dependency_tracking=WRITESET;<br><br> ## 下三参数设置是为了防止主备切换后环境变化。<br><br>（3） set global slave_parallel_type='logical_clock';              <br><br>（4） set global slave_parallel_workers=16;                         <br><br>（5） set global slave_preserve_commit_order=ON;                                                       <br><br>测试环境从库环境修改：<br><br>（1）  stop slave;<br><br>##下二参数设置是为了防止主备切换后环境变化。<br><br>（2） set global transaction_write_set_extraction=XXHASH64;     <br><br>（3） set global binlog_transaction_dependency_tracking=WRITESET;<br><br>（4） set global slave_parallel_type='logical_clock';              <br><br>（5） set global slave_parallel_workers=16;                         <br><br>（6） set global slave_preserve_commit_order=ON;<br><br>（7） start slave;<br><br>（8） show processlist; 查看并行复制是否开启。<br><br>（9） show slave status\G  查看集群运行状况。|

### 4.7.2  实际结果

通过测试修改binlog_transaction_dependency_tracking = WRITESET， binlog组提交的效果呈现单个last_committed 对应上千的sequence_number。组提交效果明显（图8所示）。

![](https://tech.qimao.com/content/images/2021/09/7.png)

图8

通过调整主库参数binlog_transaction_dependency_tracking 为WRITESET和和备库slave_parallel_workers的并行参数，主备的延迟情况得到了解决。主备之间能够实时同步，高可用架构可用状态，业务连续运行有了保障。主备实时同步如下图9：

![](https://tech.qimao.com/content/images/2021/09/11.png)

图9

# 5 问题揭晓

使用WRITESET的方式并通过调整并行恢复参数，主备之间的延迟问题得到了解决，如图9所示，主备延迟基本控制在1秒以内。综合分析，该实例主备延迟是因为单条insert插入数据量太大，大量插入导致主库组提交效果不明显，备库并行恢复效果不佳造成的。

回顾最开始发现的现象：

**（1）  主备延迟呈现周期性波动：**  
主备之间延迟的周期性波动主要的原因是，主库在业务高峰期insert插入太多导致备库无法及时重做，业务低峰期又能追平差异造成的现象。

**（2）  进行表分析后开始主备延迟持续性增加**：  
表分析确实造成了主备的延迟，但实际在本次分析过程中，只占很小的影响因素。最后经过统计业务的插入量，本月内缓慢增加了百分之三十左右。增加的这段插入量的过程，主备同步性能达到瓶颈，备库始终无法及时进行SQL的重做，就是导致主备延迟持续增加的罪魁祸首。  
反向也说明了当前业务场景下MySQL数据库在commit_order的参数模式下，主备数据同步有一定的瓶颈。而writeset参数控制下，能有效的解决该业务场景下主备同步的问题。

# 6 问题总结

本次优化通过各个方面的细化分析，踩了很多的坑，走了很多的弯路，最终结合现有业务特征，选择了合适的复制模式，解决了该场景下主备复制的延迟问题。通过本次主备延迟的问题分析总结如下建议：

1.  熟知自己管理资源的情况，了解云环境或物理环境的主机负载。

2.  使用云环境需要明确云环境默认参数，明确主备复制的模式是什么。

3.  深入了解业务场景、SQL的类型。对比业务场景和主备复制的特点，选择合适复制技术（不明确的前提下，可在上线前进行压测）。

4.  业务允许的情况下对数据库大事务SQL进行提前拆分。

5.  架构设计前考虑前端添加削峰系统，避免大批量单行SQL 的操作。

6.  根据业务量的预估，提前进行分库分表规划（mysql 单表上千万就可考虑分库分表）。

7.  最最最重要的一点，解决问题，对于一切不明确现象，都要靠自己静下心去验证、来思考，不能只听他人诉说，时刻牢记实践出真知。

**注：**本文只是结合自有环境，实际问题解决过程进行了记录。主要从资源使用、业务场景、并行恢复等方面进行了分析、验证。主备同步还可根据其他方面进行分析、优化，如：MYSQL sync_binlog 与innodb_flush_log_at_trx_commit的双一设置对DB性能的影响。

# 7 附件1 主备同步基础知识总结

## 7.1  复制基础

首先关于mysql数据库主备延迟，我们需要先了解它的主备复制的方式。如下图10所示：

![](https://tech.qimao.com/content/images/2021/09/--11.jpg)

图10

MySQL主备复制是基于Binlog的复制，简略流程如下：

> 主库Commit提交，生成Binlog日志；  
> 从库IO Thread获取到binlog日志生成Relay Log；  
> 从库SQL Thread 获取读取Relay Log 文件中的日志，解析成Sql语句；  
> 从库进行重放操作。

## 7.2  复制方式

### 1.2.1  传统复制

传统复制模式，基于binlog文件和binlog位点进行复制，因为在**树形复制**的场景下，**位点信息不具备全局唯一性**，容易造成数据丢失。

### 1.2.2  GTID复制

GTID复制模式，基于binlog文件和GTID号（uuid：seqno）。GTID在复制集群中具有全局唯一性的特点，相比传统复制更能保证数据安全。使用GTID可以范围的向主库请求binlog，这样当主库宕机，程序就可以选择更加合理的备库，其他的从库也可以指向新主库，与新主库创建主备关系。

## 7.3  复制模式

### 1.3.1  同步模式

当主事务提交一个事务，在主向前端反馈commit成功结果“之前”，必须保证所有的从库已经提交了这个事务，所有的从库不仅接收，还必须保证应用了该事务。代表产品：**MySQL Cluster。**

### 1.3.2  异步模式

当主事务Commit后将事务写到binlog日志，不需要关心从库是否接收或者什么时候接收ack的消息。主库直接返回成果结果给客户端，不会与从库发生ack之类交互操作。

### 1.3.3  半同步模式

> rpl_semi_sync_master_wait_point=after commit;

  
半同步复制（5.7.2版本之前默认唯一的一个参数值after_commit），主库上客户端发出commit指令，事务提交到存储引擎后，等待从库传递回来ack，在向前端返回数据操作成功的状态。与无损复制的区别就是，如果主库上的事务已经提交到了存储引擎，而正在等待从的ack过程中，这个时候发生crash，则主库上的这个事务已经认为commit了。而从库还没有commit，当这个时候切换到从后，就会发生回滚最后的这个事务，这个时候主备数据就会不一致。

### 1.3.4  增强半同步复制（无损复制）

> rpl_semi_sync_master_wait_point=after_sync;  
> rpl_semi_sync_master_wait_for_slave_count=1；  
> rpl_semi_sync_master_wait_no_slave=on

主库上客户端发出提交指令，事务写入到binlog buffer，IO thread获取binlog数据传入到了从库，（事务写入到relay log,且flush to disk 中持久化到了磁盘上），然后从库给主库返回一个ack，master才会把事提交到存储引擎，且返回到client一个commit成功的指令。无损复制一定程度上避免了主备不一致的情况。

当同步复制发生超时时（由rpl_semi_sync_master_timeout参数控制，单位是毫秒，默认为10000ms，即10s），会暂时关闭半同步复制，转而使用异步复制。当master dump线程发送完一个事务的所有事件之后，如果在rpl_semi_sync_master_timeout内，收到了从库的响应，则主备又重新恢复为半同步复制。

## 7.4  复制技术

### 7.4.1  单线程复制

主库进行数据变更，生成binlog之后，dump线程工作，I/O线程获取数据在从库生成relay log,从库sql线程从relay log 中读取接收到的主库binlog变更事务，然后将所有事件都进行一次“串行执行”。而binlog的写入时机是事务完成对数据的修改，且事务commit之后。也就是说，从库的事务具有限制性，主库提交之后，然后产生的binlog发送到从库执行。理论而言，一前一后肯定有复制延迟，特别是再包含大事务的时候。

### 7.4.2  多线程（并行）复制

#### 7.4.2.1 多线程复制的背景：

MySQL的复制是基于binlog的。MySQL复制包括两部分，IO线程 和 SQL线程。IO线程主要是用于拉取接收Master传递过来的binlog，并将其写入到relay log。SQL线程主要负责解析relay log，并应用到slave中。不管怎么说，IO和SQL线程都是单线程的，然后master却是多线程的，所以难免会有延迟，为了解决这个问题，多线程应运而生了。

IO没必要多线程，因为IO线程并不是瓶颈（阿里云云盘请注意不同级别的最大带宽规格）。

SQL可以多线程，目前最新的5.6，5.7，8.0 都是在SQL线程上实现了多线程，来提升slave的并发度。

#### 7.4.2.2 多线程复制的重点：

并行复制的重点，能不能进行并行复制，关键点还是在多个事务之间是否有锁冲突。

#### 7.4.2.3 DATABASE并行复制

**基于库的并行复制。不同库的事务没有锁冲突。**

> **Master:**  
> binlog_transaction_dependency_tracking = COMMIT_ORDER

> **Slave:**  
> stop slave sql_thread;  
> set global slave_parallel_type='DATABASE';  
> set global slave_parallel_workers=4;  
> start slave sql_thread;

#### 7.4.2.4 LOGICAL_CLOCK并行复制

**基于组提交的并行复制方式（基于事务）。**

slave-parallel-type=LOGICAL_CLOCK ：Commit-Parent-Based模式（旧）：同一组的事务last-commit相同，表示没有锁冲突. SQL可以并行重做。Lock-Based模式（新）：即便不是同一组的事务，只要事务之间没有锁冲突，就可以并发进行SQL重做。 不在同一组，只要N个事务可以重叠，说明没有锁冲突。

> **Master:**  
> binlog_transaction_dependency_tracking = COMMIT_ORDER

> **Slave:**  
> stop slave sql_thread;  
> set global slave_parallel_type='LOGICAL_CLOCK';  
> set global slave_parallel_workers=4;  
> start slave sql_thread;

#### 7.4.2.5 WRITESET并行复制（WRITESET_SESSION）

**基于主键的冲突检测，修改的行的主键或非空唯一键没有锁冲突，即可并行执行SQL重做。**

> WRITESET: 基于写集合决定事务依赖；WRITESET_SESSION: 基于写集合，但是同一个session中的事务不会有相同的last_committe。

> **Master:**  
> binlog_transaction_dependency_tracking = WRITESET  
> transaction_write_set_extraction = XXHASH64

> **Slave:**  
> slave-parallel-type = LOGICAL_CLOCK  
> slave-parallel-workers = 32

#### 7.4.2.6 主备应用事务顺序不一样的问题

mysql 5.7应用事务顺序和realy log记录事务顺序可能不一致。mysql 5.7可以实现更小粒度的并行复制，但需要将slave_parallel_type设置为logical_clock，但仅仅设置为logical_clock也会存在问题，因为此时在slave上应用事务的顺序时无序的，和relay log中记录的事务顺序不一样，这样数据的一致性无法保证的，为了保证事务按照relay log中记录的顺序来回放，就需要开启参数slave_preserve_commit_order。

 
# Reference
https://tech.qimao.com/openresty-lua-shi-xian-hui-du-fa-bu/
https://tech.qimao.com/qi-mao-fen-bu-shi-zhui-zong-shi-jian/
https://tech.qimao.com/shi-yong-arms-ltsjian-kong-gao-jing-ying-yong-shi-jian/
https://tech.qimao.com/ce-shi-2/
https://tech.qimao.com/test/
https://tech.qimao.com/dan-yuan-ce-shi/
https://tech.qimao.com/mysql-zhu-bei-yan-chi-you-hua/
https://tech.qimao.com/tui-jian-yin-qing-jie-ru-ingressfang-an/
