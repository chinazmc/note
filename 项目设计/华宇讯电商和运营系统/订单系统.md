# 【积微成著】性能测试调优实战与探索（存储模型优化+调用链路分析）

性能测试之于软件系统，是保障其业务承载能力及稳定性的关键措施。以软件系统的能力建设为主线，系统能力设计工作与性能测试工作，既有先后之顺序，亦有相互之影响。以上，在性能测试的场景决策，架构分析、流量分析、压测实施和剖解调优等主要环节中，引发对于系统能力底盘夯实和测试策略改进的诸多思考。

在性能测试阶段，剖析系统能力实现及调优方案，探索更优解及性能测试策略的提升空间。

# 二、热点数据存储模型压测实战及思考

通过性能测试，推测SKU库存预占场景，在不同存储模式下的性能瓶颈及风险。

数据架构升级后，SKU库存预占效率（TPS）提升2300%↑。

测试驱动，结合系统实现，论证缓存预热的必要性，并借助大数据分析，探索科学的缓存预热及保温策略。

结合新业务模式，思考更加科学的测试数据构建思路和测试过程提效方案。﻿

## 1、压测场景

库存预占，是指在订单接单环节，为单据提供SKU库存短暂预留。物流仓配订单接单环节，会发起SKU维度的库存预占行为。

库存中心通过“库存预占主应用”中的预占接口，对外提供SKU库存预占标准能力。主要通过“库存扣减逻辑管控及数据库层交互”、“缓存层交互”，以及“任务调度”三个关键应用，承载库存逻辑计算及存储层交互能力。

数据模型视角，对预占能力实现分为两种：

▪事业部维度库存预占主要通过Redis缓存层承载。

▪批次库存预占直接由数据库承载。

当大促仓配单量进入爆发期，热点SKU预占请求快速增长，且库存预占请求直达数据库，系统TP99会出现跳点甚至持续升高，严重情况下造成接单超时。

以上，计划针对性构造压测场景及数据模型，确认系统的峰值承载能力及调优策略的有效性。

## 2、首压及分析

◦压测目标：“库存预占主应用”下的“预占接口”，在数据库承载热点SKU预占请求模式下，探索目标TP99（≤3000ms）可承载的峰值流量，并验证调优后的峰值承载能力（目标 TP99≤500ms）。

◦压测方案：单个热点SKU持续发压预占，发压起始QPS=10，并以QPS+10递增，探索可承载请求的性能上限。

◦压测过程及结论

▪在QPS=50时，系统可稳定支撑库存预占业务（TP99≈100ms）。

▪“库存预占”主应用：CPU使用率≤15%，内存使用率≤35%

▪“库存扣减逻辑管控及数据库层交互”应用：CPU使用率≤18%，内存使用率≤65%

▪数据库：CPU使用率≤7.8%（无慢SQL）

▪基于当前的系统性能体现，具备持续加压的条件。

▪以QPS+10递增加压至60时，TP99在2分钟左右快速增长至7000ms，“库存预占”主应用TPS≤60，预判系统能力达到瓶颈，停止加压。

_“库存预占”主应用 TP99+TPS趋势_

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahZBuvOkjZTHibNlPZ2qt1tuDg6fPicuibEoFCGkfsiaHicUph4ic6Im1fHeIA/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahXQNzCltsXZtObA1SWQXmgHiaPQHl705roClA8KUgOdnBazMHENVN3Tg/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

_“库存预占”主应用 硬件资源趋势_

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbah2GzG0pIv0ibtjqJmrHibrPrVicxxtEBeibAOQb98TribGMD5mN9yW5MUib1Q/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahEXIeFWzkTcurjKwC1ycPkPicmDicZTXqNh6ORkq45TVFibvIaOdaic9jJQ/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

_数据库 关键指标（CPU）_

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahAicZd5LPw82coFg1RSgx65VgyjdNhKsicWvZyMWmhzCsPSwCFkicjsuxQ/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

_数据库 关键指标（慢SQL）_﻿

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahrianxkU67YLLTq3y3ayXuXYiaUAuStOsZZkWFomceiaWicGMhWib2br98Hw/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

_数据库 关键指标（内存）_﻿

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbah5bCuicu0J0FRGI59zcAIiaadFlrCv3jNPYYiarjcCXTlbBrwdl2ghxN5g/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

◦瓶颈预判：单据维度的库存预占是以先查（可用库存）后写（预占库存）的方式进行，在对热点SKU高频次下单过程中，数据库会对该行记录长时间持续读写，数据库层面会通过行锁机制保证单笔交易的原子性，行级锁引发的锁竞争大概率会导致系统处理能力达到瓶颈，制约系统的执行效率。同时从应用层到存储层，未出现硬件资源瓶颈，排除硬件资源不足的影响。

## 3、调优及复压

◦存储层改造（_见 库存中心-库存预占场景 系统架构简图_）：经首轮压测及分析，为解决已知性能瓶颈，从数据架构层面，将批次库存预占由数据库直接承载请求压力，升级为由Redis缓存主要承载请求压力。利用Redis高性能吞吐能力，解决并发场景下的数据读写效率问题，由Redis前置承载热点商品的主要流量。

◦一致性保障（_见 库存中心-库存变化监控机制简图_）

▪为确保缓存层与数据库层数据一致性，在缓存命中的情况下，通过建立调度任务或MQ方式异步回写数据库。

▪在缓存击穿时，通过先读（数据库）后写（Redis）再反馈（API）预占结果，之后异步回写数据库，确保数据一致性。

_库存中心-库存预占场景 系统架构简图_

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahy173YEiby19EJ5zX1gSRLt44QYTDjdQ8r53c1qV0wLbsPvR1YOnUcqg/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

_库存中心-库存变化监控机制简图_﻿

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahvMfibRwECsES3pxvC1ej073tVETSYrfW2JtEYpb7np1QFW46r5p8wSA/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

◦复压结论

▪完成数据架构升级及热点SKU缓存预热后，初始QPS=1100并以100递增，TPS上探至1200时，TP99≈130ms，系统可稳定支撑批次库存预占业务。

▪当TPS上探至1300时，TP99出现明显波动（毛刺≈420ms），且“缓存层交互”应用CPU占用率飙升至90%+，核心链路稳定性劣化，停止加压。

▪相较数据库承载模式，缓存化升级后，TP99满足预期（≤500ms），TPS承载能力大幅提升2300%=(1200-50)/50。

_“库存预占”主应用 TP99+TPS趋势_﻿

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahyLmKRyZCJLMrnq5yicvJKicC6fZribBDG0UkFy2jaCh1gxRKv3VDRpic2A/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahXbApQ1GiaYRtrAFWETS0eOy1Xu03JicGUmyk9jDDS8wD4bnwameCNpLA/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

_“库存预占”主应用 硬件资源趋势_

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahw6Xx7jBK7VrvvtXLbSqnzl1Ezic0iaRWDMfDkvmgHyXpWLwLgq3u4RNw/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahYogElrxQS5FqBd8R6FJaU2qLdMqpluuWh0ibwouhRh2ovRPjdBvgBsQ/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

_数据库 关键指标(CPU)_﻿

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahurxiboc5bF5qKELuBLhcS4QO3FtBEKHZNAJN8AwuG4GUWpzMbhuLmbQ/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

_数据库 关键指标（慢SQL）_﻿

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahK2lmoJLgkrCxCCTiaPuzdHzVgW8QJF4yd4XBTbFibick5AcZEXCarneGQ/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

_数据库 关键指标（内存）_

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahGWMibUWaS4vFQiaq7RnncAsCwsMoSdaCx8Dia2Fy3fpIJaTd2u6mxomeQ/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

_Redis集群 关键指标_﻿

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahaASgjc028b5MFrTf4WHMKSl0gA4ibElkHr3aC6s1uT05pLPwlLVrO8A/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

## 4、系统健壮性思考

◦全量缓存的弊端：供应链模式中的不同行业，SKU品类生命周期存在较大差异（如服饰行业≈3个月），全量缓存模式会导致Redis中存在大量无效品类，资源消耗膨胀不可控，增加资源成本，有必要设计更有效的缓存方案。

◦缓存预热及保温的必要性：缓存命中率，与预热机制和保温策略紧密相关。

▪必要性：常规大促节奏，起售期会触发首次缓存初始化，促销品类与日常销售品类的重合度，决定了首次缓存击穿的概率。目前的Key有效期=7天，大促起售期→开门红→高峰期间隔均大于7天，缺少必要的保温策略，会增加下个促销节点前缓存失效的可能性。

_大促开门红至11.11 缓存命中率趋势_

_系统整体可平稳承载流量，__同时缓存命中率曲线，有一定的提升空间_

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahYCiaqOf8h9mwEmy0RCnerscWNsQDkMQXNfTa6j6kbpc0N6Yp7SXkBew/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

▪预热思路：如何尽可能保持在大促等特定时段的缓存有效性，提升缓存命中率（降低击穿概率），可通过前置的多维度分析调研，包括但不限于基于大数据的大促前集中采购品类分布分析、历次大促及关键节点促销品类密度及分布分析 以及 关键客户促销计划调研等方式，结合技术手段，前置预判、预热及保温。

◦缓存预热实践：通过对某客户大促前集中采购期及大促节点SKU品类重合度分析，发现以下规律

▪集采入视角：大促集采期SKU品类，相对开门红品类重合度≈69%，相对11.11品类重合度≈75%。

▪销售出视角：起售期SKU品类，相对开门红重合度≈94%，开门红相对11.11品类重合度≈75%。

▪以上数据证明，通过在开门红以及11.11大促等关键促销节点前，将集采期及前一促销期的SKU可用库存数据，进行缓存预热，有助于提升预占请求的缓存命中率。

_大促主要环节 SKU品类重合度分析_

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahjXz5RWdgtXwn5l16xc9zWXo9A4ovRzibxITyVeEZPIVTM34WrwndYHA/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

◦异常场景识别：库存场景对数据三性（准确性、及时性、完整性）要求较高，在数据库与缓存的双向同步过程中，需避免因一致性问题引发的业务异常。

▪超卖异常识别：大促单量峰值期，为保护主数据库安全，通过缓存同步限流减缓主库压力，造成缓存至数据库同步延迟，同一SKU在数据库层未及时扣减，如此时叠加缓存Key到期情况，接口直接返回MySQL数据，可能会引发超卖业务异常。

▪系统优化思路

▪静态方案：单量高峰期期间，延长Key效期，覆盖大促关键环节间隔。

▪动态方案：增加热点SKU缓存效期延时策略，Key到期T-1天，日均预占请求量大于1的SKU，自动延长Key有效期。

## 5、测试策略改进思考

◦场景拓展

▪直播电商模式主流化趋势强劲（2023年前三季度全国直播电商销售额达1.98万亿元，增长60.6%，占网络零售额的18.3%，直播电商拉动网零增速7.7个百分点），相较传统电商，其限时促销模式叠加社交传播扩散属性，单品瞬时流量大，不同促销场次品类重合度更低，促销频次高，对系统性能提出了不同的要求。

▪反推性能测试策略，从平台视角出发，需要尽可能提升选用SKU的多样性，同时降低压测单次请求SKU的品类重合度，识别真实复杂场景下的性能隐患。

◦效率提升：复杂场景的仓配订单性能测试工作，需要前置基础数据的大量储备（商品、库存），以及高复杂度接口请求数据准备。如何确保商品和库存等基础数据快速就绪？同时下单请求的报文体根据SKU密度和复杂度需要，自动化快速构建组装？需要在现有压测框架基础上，开发扩展功能，以支撑从基础数据到复杂单据的一键快速初始化构造能力，降低复杂场景构建难度，提升测试工作效率。

# 三、无效调用量分析、识别及调优实战

在性能测试的流量分析阶段，结合业务场景调研，前置识别性能瓶颈疑点。

推动排查及调整核心链路调用逻辑后，在标定的业务窗口期，核心接口调用总量降低60%↓。

深入细分业务场景，推演潜在的调优空间。

## 1、背景

   物流系统在订单出库后，由 订单明细查询应用，提供订单及其关联包裹明细信息的对外查询能力。主要由外部系统（Top2量级调用方：接入回传67%、履约回传11%）调用，在单据出库后，输出出库货品的数量和包裹详情等订单基础信息。

_关键（Top2）调用方拓扑_﻿

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahoysJKd2VGnJSYqeWlGTsGR8Xsic0hYZ4hIueH9ok5icHH6Bck7YRT01w/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

﻿﻿2、场景调研及疑点识别

◦场景调研及风险预判（生产流量分析）

▪对“订单包裹明细查询接口”进行调用量趋势分析，取样23年10.12 06:30~23:00（流量分析期），环比最近一次促销同时段（最近一次大促请求高峰期），Top2调用方峰值调用总量激增305%。

▪基于前期调研，从调用量看，常规情况下仓库出库能力均值≈400000单/分钟，仓库出库高峰时段为每日08:00~18:00，仓出库次数:“订单包裹明细查询接口”峰值调用量≈1:10为“常规比例”。

▪通过对10月12日线上数据观测，仓出库次数:“订单包裹明细查询接口”调用峰值（400000/6532200）≈1:16，相较“常规比例”偏差较大。

▪以上，通过生产流量分析工作，识别出在仓库出库高峰时段，“订单包裹明细查询接口” 调用量存在疑点，并进一步深入分析。

_最近一次促销期 关键应用调用量_

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahibIrr5rZgic9R5pkgMzZD1jiaNiaJtS5OUIwrcdiaL5iclkLpM1MnibepuI5Q/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

_2023年10.12 关键应用调用量_

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahfNH00gKJA7xaJOeYt8MrDT7bIkZHxyhUkkJlEqh4gPLINiaR9zmZ8HQ/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

◦调用链粗筛

▪仓配出库单据维度，履约回传应用，向订单系统推送出库明细时，会调用仓明细查询接口。

▪接入回传应用，在回传订单信息时，会调用仓明细查询接口。

▪履约状态回传调用峰值 / 接入回传调用峰值 ≈ 1:9，接入回传调用峰值明显偏大，逐步锁定疑点系统（接入回传应用）。

◦疑点深剖

▪经深入排查，首先确认前期对异常流量和疑点系统的判断基本准确。

▪技术架构层面，接入回传应用在未判断订单状态情况下，调用目标接口。导致单据在未出库且没有出库明细时，发生大量无效调用。

▪同时发现，因AB测试环境别名配置错误，导致生产流量误叠加。

## 3、调优策略

◦调用逻辑调整

▪“I” 业务场景订单回传阶段，如单据状态为出库前，不发起“订单包裹明细查询接口”调用，剔除无效查询。

▪根据最终的回传内容（是否需要明细信息），判断调用的必要性，剔除非必要查询。

◦调整AB测试环境别名配置，避免测试流量对生产环境产生非必要压力。

_优化前接入回传应用逻辑_﻿

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahml332I9VnYL2ESjehNBjdibYxpIcYMIp87KysLmmCwql4OtgnLuWnxw/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

_优化后接入回传应用逻辑_﻿

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahCkGMB6G1DzH24Foub1YhWVQJeQEiamSEctHw4b8ViaINSOiazibiaqMDUxg/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

## 4、调优效果

◦相对调优前（10.12），“接入回传应用” 调用总量降低60%↓（前：2397252500 后：925890100），峰值调用量降低64%↓（前：5921500 后：2121800）。

_下图分别为调整前、后调用量分布，用以对比_

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahaG7spdk6EVSYVVe9EJWadtWAeGUSeb7lGDibwlV0S4viaoG9wSe6YlFg/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahcibMyFmfFZQxVe35ia9YnX8Kfw41iavD2AxVOsQDRMg3vPlMGKTCDd1Zg/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

## 5、性能风险前置识别

◦压测实施阶段不是发现性能隐患的唯一阶段，如果有能力在流量分析阶段识别性能风险并推动论证，问题发现越早，风控代价（资源）越小，质量风险越低。

## 6、OpsReview常态化

◦流量异动观测：流量分析及性能风险识别，需要结合实际的生产运营特征，以及接口的关键调用链，定义系统调用量的普遍规律。被调用方有必要不断积累识别调用来源和常规量级，盘点外部调用策略，在调用量出现异动时，排查风险。

◦编码规范：对于接口调用逻辑，有必要抽象为标准方法，避免团队协同开发过程中出现因人而异的Coding差异，降低无效查询发生概率。

◦定制化逻辑排查：系统内非标业务存在较多的定制化逻辑，有必要针对特殊逻辑排查无效查询风险。

## 7、潜在调优空间推演

◦基于测试经验，经过业务场景梳理，发现 “I场景” 下存在细分的非标定制化流程，以及与 “I场景” 并列的 “P场景” 标准流程。

◦联动研发深入分析 “I场景” 中的非标定制化流程 以及 “P场景” 中的标准流程，已确认，存在进一步优化空间，并明确优化方案（如下图）。﻿

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/9K73WSRq6BUJ4atHjIFmt4k3kibKEDbahicmATA3SzQ0ibBGmu5lqoib9PibTKF9NXkJEibDCRhFIXwgNKicr0gvicUNWg/640?wx_fmt=jpeg&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

﻿

# 四、总结

   性能测试作为系统能力巩固升级的关键措施，通过对典型案例的陈述和思考，探索系统能力和性能测试策略的提升空间。确保核心系统链路稳定高效承载业务峰值流量，同时从容应对极端场景。

​




# 交易日均千万订单的存储架构设计与实践

王卫东 京东零售技术 _2023-11-03 19:01_ _发表于北京_

**一、订单系统概述**

  

  

## **【 1.1 业务范围 】**

服务业务线：快递、快运、中小件、大件、冷链、国际、B2B合同物流、CLPS、京喜、三入三出（采购入、退货入、调拨入、销售出、退供出、调拨出）等

## **【 1.2 订单中心价值 】**

**1、解耦（提升系统稳定性）**

**原系统**：交易与生产耦合在一起，业务新增需求，涉及各上下游多个系统。ECLP、外单、运单、终端系统等。多条业务线的逻辑耦合在一起，单一业务条线的需求改动，涉及原系统中其他业务线的关联改造。

**新系统**：交易与生产运营解耦：交易相关的需求在订单的域内解决；生产侧的需求，在生产域内解决，减少上下游的相互影响。

业务条线解耦：不同业务线，业务流程不同，单一业务条线的需求改动，只在具体的流程中做迭代更新，不影响其他业务线。提升整个流程和业务的稳定性。

**2、提升新业务接入速度**

订单中心向前台提供可复用的标准能力，提升新业务的导入速度。

订单中心将原系统中的大应用，拆分、抽象为多个小的应用组合，并支持不同场景下按需编排业务流程。新业务通过对中台公共标准能力的复用，可快速接入订单中心，避免相同功能的重复建设。

**3、提供全局化统一数据模型**

**原系统**：订单分属于多个系统，外单、ECLP、大件系统，有多套数据库，业务语义不统一，不便于数据化建设。

**新系统**：订单中心统一定义订单的标准数据模型，让不同业务的数据，沉淀在同一系统，减少订单域相关功能的重复建设，避免资源浪费，打破部门壁垒。使得数据和流程可以集中得以管理和优化，为集团经营分析、**预测京东未来的创新空间**，提供订单域的标准数据。

**二、架构介绍**

  

  

## **【 2.1 整体架构设计 】**

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUdYPc1QSf2ES0bSvMdflYjXIeUWx5KicicH6xpHdaAW2BsJ3htcuUSJmiaBqYdRRAhiaUcck4wtXj0Mg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

﻿﻿通过技术中台架构升级项目，将交易体系以新的接入-交易-履约-执行四层架构进行重新搭建。其中交易订单负责物流与客户之间产生物流服务契约的单据流量收口，同时承载向下游OFC（订单履约层）分发的职责。

## **【 2.2 实时数据层架构设计 】**

### **2.2.1 系统交互图**

系统交互如下：

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUdYPc1QSf2ES0bSvMdflYjYFgUYTP4z56Rt5shmzSRQog4BlMq8oSvVc1D52voWKtxnm0pOMHhxw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

﻿﻿订单中心的标准接口在上层做了单据收口，同时我们在数据层也做了统一的收口。

将**业务架构与数据解耦**，分布式数据库、缓存、一致性等高可用、高性能设计从业务架构范畴剥离，使业务架构聚焦在业务自身。

**持久化系统**：用于支撑接单、订单修改、订单取消、订单删除等数据持久化。

**搜索系统**：提供订单详情查询、订单列表查询、订单状态流水查询、判断是否百川订单等服务。

**中继系统**：数据枢纽，通过消费消息队列将订单数据写入Elasticsearch、HBase、MySQL。

**数据对账系统**：用于对比多套存储中间件的数据是否一致，以保障数据最终一致性。

**数据同步系统**：将订单列表查询所需的查询条件和列表展示字段从老系统同步至订单中心，用于解决因切量过程中订单数据存在于新老系统中而分页困难的问题。

### **2.2.2 技术架构图**

﻿![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUdYPc1QSf2ES0bSvMdflYjPEDSSicCU0KURibXY57UFicw8so5bzJTAoapBfdl3kQOicicEopvFdax6mw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- ﻿﻿【读写分离架构】采用读写分离架构模式（CQRS），将订单读写流量分离，以提高查询性能和可扩展性，同时达到读、写解耦。
    
- 【缓存】使用分布式缓存Redis缓存热门订单数据以及与订单相关的信息提高并发和响应速度减少对HBase的访问，同时，通过主、备、临时3套高性能缓存以提升系统容灾能力。
    
- 【消息队列】使用消息队列JMQ实现异步处理订单提升系统吞吐量，同时流量削峰减轻直接请求ES、HBase、数据库的压力。将不同业务场景(如下单、回传)使用不同的Topic进行隔离，可以更好地管理和维护；将不同业务使用不同的Topic隔离，可以实现消息的并行处理和水平扩展，提高系统的吞吐量和性能。
    
- 【复杂查询】使用搜索引擎Elasticsearch解决订单复杂查询，先通过Elasticsearch获取订单号，然后根据订单号查询分布式缓存Redis+列式数据库HBase。
    
- 【低成本持久化存储】采用HBase列式数据库以支持海量数据规模的存储和极强的扩展能力。
    
- 【数据一致性】通过强事务、最终一致、幂等、补偿、分布式锁、版本号等实现
    
- 【多租户架构】系统中采用多租户数据模型，将租户的数据分离存储，以确保数据的隔离性和安全性。根据不同租户的需求动态扩展系统的容量和资源，可以支持系统的水平扩展。通过共享基础设施和资源，多租户架构实现了更高的资源利用率和降低成本。
    

**【 2.3 设计优势 】**  

### **2.3.1 高可用**

- 应用服务器、MySQL、Redis、HBase、JMQ等均跨机房部署；ES单机房部署，搭建ES主备双机房集群
    
- 隔离、限流、熔断、削峰、监控
    

### **2.3.2 高性能**

- 高性能缓存
    
- 异步化
    

### **2.3.3 海量数据处理**

- 分库分表
    
- 冷热分离
    
- 列式存储(HBase)
    

### **2.3.4 数据安全**

敏感信息加密存储，Log、Redis、ES、MySQL、HBase等均采用加密存储，“谁存储谁加密，谁使用谁解密”。

**三、订单数据模型**

  

  

**【 3.1 PDM模型 】**

在订单模型设计上，基于统一业务属性、抽象通用模型、归纳共性实体的原则，将订单模型主要分成了订单的主档信息、订单的货品信息、订单的物流服务信息、订单的营销信息、订单的财务信息、订单的客户渠道信息、订单的收发货信息、订单的操作信息、订单的扩展信息等几类。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUdYPc1QSf2ES0bSvMdflYjh1ldvU1eA4DiaofQq64dShWx8eLJsMooKgMUXaf3JJbtoYOBDtTYHPA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUdYPc1QSf2ES0bSvMdflYjz1ibsABEIAicc9Im6vxj0Q5lSLgZbdTImf8TVAxiczMtQcSposd5tVgBQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**【 ﻿﻿3.2 模型扩展性 】**

### **3.2.1 标准模型扩展性设计**

订单中存在几十上百个标识字段，若每次都采用新增字段形式，订单业务属性、数据模型会大量膨胀，腐蚀模型，同时开发效率较低，故采用KV形式承接和存储。将标识划分到各个业务域中，如订单标识、货品标识、营销标识等。

**3.2.2 个性化业务模型扩展性**  

针对个性化业务，提供了一套可配置的数据库字段管理方案，通过开箱即用的一些设置，订单在提交、修改、查询时，可以根据业务身份+业务类型+业务字段找到不同的数据模型以及数据扩展编码，即找到存储到哪张表哪个字段。在每张表都预留N个扩展属性，同一个扩展属性，不同的业务身份+业务类型表示不同的含义，以此实现扩展存储。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUdYPc1QSf2ES0bSvMdflYjZaye3szibr3JIosIH2Fyibb2ReUsXABtGyYjJItuiaJV19ObibRYSgV5dg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**四、未来及挑战**

  

  

## **【 4.1 订单个性化查询 】**

个性化查询需求增多，如模糊查询、根据查询条件实时聚合等需求，若ES索引都放在同一个集群中，会影响整体集群稳定性，但拆分后该业务数据无法与其他业务一块查询展示。﻿

## **【 4.2 单元化架构 】**

当前接单持久化TP99是47ms，在非跨机房情况下TP99是20ms，从数据来看，跨机房对性能影响很大。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/9K73WSRq6BUdYPc1QSf2ES0bSvMdflYjsB6xFA9S2lgar97VK1vIZykZXwps0Y2ImvwibdbnkwDFOI9HVlr3VYg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

﻿﻿单元化，可以让同一个用户的相关请求，只在一个机房内完成所有业务「闭环」，不再出现「跨机房」访问。单元化的部署方式，可以让每个机房部署在任意地区，随时扩展新机房。通过单元化，持续加强订单平台的基座稳固。

## **【 4.3 硬件成本控制 】**

订单日均单量不断上升，数据量越来越大，随之而来是硬件成本的增加，如何控制硬件成本增加，是当下及未来的一项挑战。我们计划通过数据归档、冷热温数据分层等方式来降低数据存储成本。

 


目前的订单系统架构
![[Pasted image 20231127140857.png]]
![[Pasted image 20231127140921.png]]

高并发系统设计类似于治理大规模水流问题，我们需要巧妙的方案来抵抗巨大的流量，提供更好的用户体验。这些方案可以看作是操纵流量，使其更平稳地被系统处理，就像古代治水方式一样。

1. 拓宽河道（Scale-out，横向扩展）：这是常见的方法，类似于古代大禹拓宽河道，通过分布式部署将流量分散到多个服务器上，每个服务器处理一部分并发和流量，以降低单个服务器的负载。
    
2. 利用缓存：就像清除淤沙来使水流更畅顺一样，我们可以使用缓存来提高系统性能。缓存可以存储已经处理过的数据，以减轻数据库或其他服务的压力，从而更有效地处理请求。
    
3. 采用异步处理：有时候，我们可以让请求先返回，而不必等待处理完成。类似于将水储存在水库中，然后缓慢释放水来提高下游的抗洪能力。在某些场景下，未处理完成的请求可以在后台处理，然后通知请求方，这样可以在单位时间内处理更多的请求。

高并发系统设计的三大目标：高性能、高可用、可扩展

解决问题的方法也很多，比如使用MQ消息队列做解耦，异步的处理流程。这样的回答，只是问题的“术”。我们要升华到底层的思维逻辑，这是“道”。  

  

1. 谈复杂来源  
    
2. 谈解决方案
    
3. 谈评估标准
    
4. 谈技术实现

这个是非常重要的！！！
![[Pasted image 20231127220125.png]]


![[Pasted image 20230818102319.png]]
Canal这种异步通信的设计要求你的系统必须具备可回溯、重试、幂等、延迟特点。

**问题一：（网络环境问题）kafka不可用**
![](https://pic2.zhimg.com/80/v2-8215c66fbce1017cabc9b575ea1fba95_720w.webp)

在实践中，我们遇到第一个比较有代表性的问题是kafka集群不可用，直接导致ES数据断层，从而影响到商家的履约体验。
首先，kafka集群所在的网络环境和机器主机发生问题，deployer的store数据存满，直接导致delay了8个小时。提示音没有提示，也会有电脑端的管理系统同步订单，但是需要人工刷新，所以，过了很久我们才发现这样一个问题。紧急把访问切到之前的ES热集群，之后，我们重新把kafka服务部署到可用状态，数据虽然慢慢追上了，但是原来在kafka中没有被adapter消费的一部分数据却丢掉了，这主要还是因为设置的kafka落盘频率问题。
![](https://pic3.zhimg.com/80/v2-ff32efe8cc791712dce2c02095449d5e_720w.webp)

丢数据在数据异构的需求中是不可容忍的事情，索性这次事故基本上锁定了丢数据的原因，所以，我们将Zookeeper中的jouralName和position设置到对应的事故之前的位置，将数据重新跑到ES中，至此问题解决。

- 总结一  

![](https://pic4.zhimg.com/80/v2-623536b6ad6f5c50844f7a0afa8b0c57_720w.webp)

- 总结二
![](https://pic1.zhimg.com/80/v2-8100acf5ffc4dc6204a96bef961a50d0_720w.webp)

至此，总结以上两点，数据异构的实践在问题监控、报警、及时降级方面是非常重要的。希望这样的总结经验能够让大家少走弯路。  
**问题二：Deployer故障，自动HA**
![](https://pic1.zhimg.com/80/v2-da488b7708716673cf8999b63a32c06c_720w.webp)
Deployer机器发生故障，系统自动HA到备机，任务得以继续消费。总起来说，问题二并没有给我们的业务带来任何的损耗，但是，还是比较经典的一个案例。这主要反应出来，对于数据异构这样一个需求。它的链路上所有环节，基本上都是有高可用的要求的。
![](https://pic4.zhimg.com/80/v2-2db38e630082d5bd0b287d4ffad005ef_720w.webp)
Canal一共提供两种HA，其中Deployer的HA是靠Zookeeper的临时节点和重试机制实现的，而Mysql的HA则是靠一个单独的线程不断的Detect来实现的。
但是MySQL的HA，只能用GTID的模式，这是因为Mysql master和slave的binlogfile name、position是不一样的。如果用master的binlogfilename和position去slave发送dump协议，这会出现无法匹配的问题。但是GTID是全局有序的，这也就保证了Mysql的HA只在GTID模式下才可用。

![](https://pic3.zhimg.com/80/v2-d3b2419429677fa2eb842243d3f8ad7e_720w.webp)

谈到高可用，提出上述总结。这里我与大家互动了一个问题：“单机器部署两台Canal实例是否算是高可用？”答案是：“不算高可用，原因是单机部署了两台Deployer，但是机器如果故障，两台Deployer均不可用。”

问题比较有代表性，也有一些同学掉进了坑里。这里我与大家一起回顾一下高可用的范围：多机器、多机房、多地区、多国家。范围越大，高可用自然越是稳定。但是带来的成本和数据传输要求也越高，一般都是根据业务量级和重要程序进行取舍的。

- 总结三  

![](https://pic2.zhimg.com/80/v2-3921e116dc081a68851a07efcf9fd4f5_720w.webp)

以上就是我在数据异构中的一些经验教训。下面我们将问题向上抽象，聊一聊面向复杂度的架构设计方法论。
**七、面向复杂度的架构设计方法论**
**1.4R模型**
![](https://pic4.zhimg.com/80/v2-8cc1c4baf083f2a427f1dd8c22a45823_720w.webp)

大家是否发现，我在和大家聊Canal或是到家的数据异构方案时，更多的都是以角色、关系、规则这种描述方法。相信大家也不是第一次碰到这种描述方式，在很多的架构中，都是这样的一种描述方法。就像上图中，说到的Parser、Sink、Store。这些角色的职责是什么？他们是如何配合完成Canal这样的一个产品功能的呢？
4R模型本质上就是一个视角，它是Rank、Role、Relation、Rule这4个单词的首字母构成，它强调了描述方法、也强调了我们要用这样的视角来看待我们的系统。这样整体来看，系统会更加清晰和简洁。
**2.区分复杂度**

![](https://pic4.zhimg.com/80/v2-36fddc707dbb6c5bf19deeccf637dd0b_720w.webp)

如上图，将复杂度问题分为技术方向和业务方向两个部分，其中灰色的部分，一般都有一些开源软件来帮我们解决，比如Dubbo、Spring、Canal等。而红色的部门正是我们日常工作中所不能避免的复杂度。

这些复杂度问题，如果平时不加以重视，忽视掉的复杂度问题最终则会演变成为不可维护的技术债务，最终打掉系统的可维护性，只能重新推倒重来了。很多重构行为都是因为复杂度的忽视累积而成的后果。所以，学会如何区分复杂度就是比较重要的点了。比如这次Canal的数据异构，同时面临了数量级复杂度和写入复杂度两个。

**3.复杂度的架构设计环**
![](https://pic1.zhimg.com/80/v2-e8c1310bbb23b358be1122899b1db8bc_720w.webp)

同样，面向复杂度的架构设计方法论，最终会归结到业务实现上。下面描述一下具体的步骤含义：
- **1-需求：**产品同学提出需求描述，或一句话需求、或完善的PRD文档
- **2-判断：**对需求进行判断，需要什么样的数据量，什么样的峰值，是否要高可用等等，如果不能理解清楚，则找对应的需求人员不断澄清，直到清晰为止
- **3-复杂度识别：**将需求精确化以后，对需求的复杂度问题进行识别，比如业务复杂度问题、数据量复杂度问题
- **4-拆解到备选架构：**针对识别出来的复杂度设计出多个具体的架构方案。比如采用ES存储数据屏蔽分库分表的数据量复杂度、采用数据异构的方式写入数据，从而屏蔽数据写入的复杂度。
- **5-取舍：**对备选架构进行取舍，任何的架构方案都有好的一面和坏的一面。在不同的时间都有不同的选择，这里建议大家从简单、合适、演进三个架构原则来进行方案取舍,选择最适合自己的那一套方案。
- **6-架构方案4R：**用4R视角来设计系统分层、角色、关系、规则。以这种视角设计出来一套抽象模型
- **7-实现需求：**将4R架构模型实践即可。

![](https://pic4.zhimg.com/80/v2-dcf2ef8c708890643cf865d95d7701e3_720w.webp)

同样，将本需求的一个架构设计环案例呈现给大家。(由于部分设计有保密性，4R此处用Canal4R代替)
**>>>>**
**Q&A**
**Q1：订单表中，如果有一些商品id，那么同步到ES中也是id吗，不会关联出name打成宽表存到ES吗?**
**A1：**具体的映射字段需要在Adapter中配置映射即可，存入到ES中的情况也与配置的映射是直接且唯一关系。是否宽表要在实际应用中把控字段的个数。

**Q2：Canal部署deploy主从和canal-adapter有没有遇到官方的bug？有，改动了哪些？**
**A2：**遇到过Column not match的异常.具体看Canal的TSDB来解决。

**Q3：这套复杂度方法论如何落地到实际应用？**
**A3：**需要对系统进行4R视角拆分、识别复杂度类型并按照架构设计环的方式来评定需求。

**Q4：平时的Canal有消息延迟吗？*
**A4：**有一定延迟的，binlog的数量、网络等因素，都会造成一定的延迟，所以，建议异构还是要建立在业务数据可延迟的基础上的。

**Q5：我主要用canal-adapter读取Kafka中的binlog日志然后写到数据库中，Kafka中有多个表的日志，我rdb目录下的yml文件只配置了一个表的为什么其他的表也会同步？**
**A5:**Yml的作用是配置映射关系，具体的过滤职能在Deployer的Sink配置。

**Q6：异构数据是直接同步原表吗，还是做了关联？**
**A6:** 做了关联，直接在Adapter中配置对应映射关系即可。

**Q7：请问为什么不直接增加热集群的节点和分片，而是重新建一套ES集群呢?**
**A7:** 这里主要还是一个数据拆分的思想，如果通过提高配置来解决访问量问题，那么，随着业务量级增加，流量混在一起，对应的ES集群流程会呈现不可评估的情况。本质上还是一个数据存储职责的问题。

**Q8：如何保证Zookeeper的高可用？**
**A8:** Zookeeper本身就是高可用的，如果想在机房或异地方面做高可用，建议做主备同步、多集群部署等手段。

**Q9：新集群的查询请求峰值是多少?**
**A9:** 大约2000-4000 QPS。

**Q10：怎么把握冗余尺度呢？**
**A10:** 冗余的维度在机器、机房、地区、国家是不断增加的。维度越大，对应的高可用方案越可靠，但是，对应的费用以及实现复杂度也会变高。因为这种冗余方案肯定会有数据copy。


从最初的单机应用到现在的大型互联网分布式系统，数据源一直在业务系统中扮演着重要的角色，多样的需求造就了Mysql、Memcached、Redis、Elasticsearch等这些耳熟能详的存储组件，因各自的特点，在系统中承担着不同的职能。因为数据量、访问量等挑战，我们时常又面临着各种维度的分库分表，数据冗余复杂度。本文从京东到家提示音的需求出发，探讨多数据源的职责分工，数据异构同步实践和问题总结，大致分为以下三个部分
- 提示音业务背景
- 履约系统的数据源职责分工
- 数据异构的实践、问题和总结

- Redis
Redis在履约系统中主要承载的一个职责是worker跑批任务的存储和查询。因履约系统中大量运用了跑批任务来实现最终一致性设计，而Redis的Zset结构比较匹配这样的需求，将时间作为分值，不断的提供近期任务的查询是Redis在履约系统中充当的最大职能。为什么Redis没有承载过多的查询职能呢？一般Redis是应用于缓存场景，得益于其高性能，多样的数据结构特点。但是，在数据量和复杂的查询条件上，没有Es支持的好，关键点是业务系统查询条件复杂度是比较高的，所以，Redis没有承载过多的查询职能

- Mysql
Mysql在履约系统存储中的职能是持久化存储订单数据，主要还是使用其强大的事务机制，以保障我们的数据写入正确性、可靠性。从履约流程上来看，将数据做冷热分离，热点数据是我们在履约中的订单.(也就是未完成的订单)，而完成的订单，由于其使用率较低，我们称之为冷数据。这样的一个拆分也就是上图中对应的业务库和历史库。业务库是热库，而历史库则是冷库。冷热分离思想，使单库单表数据量维持在千万级别。从而避免了对应的分库分表复杂度。

从部署架构上看，我们对业务库进行了大量的主从分割。  

biz slave是业务库从库，它也承载一些履约中的订单查询职能。

big data slave集群是大数据抽数据用做统计分析的职能。

delay slave设置延迟一定时间消费binlog是为了防止master被误操作而兜底的。比如错误执行了删除db的命令，这样的延迟消费机制就可以利用binlog进行兜底回滚。

- Elasticsearch
Es在数据存储中承担了大量的查询职责，这主要取决于它优秀的查询能力，并有天然分布式的特点。在数据量复杂度解决方案上，避免了mysql分库分表的复杂度。这里我们一共有3个Es集群。其中HOT ES和FULL ES也是进行了冷热分离，这样对查询流量进行拆分，保证生产履约流程的独立性，从而保证履约系统的稳定性。

第三套ES集群Remind Elastic Cluster则是为了解决上述提示音的问题。在部署提示音集群之前，所有的提示音查询流量都是打到热集群的。也正是这样的访问量请求，导致了热集群时有发生CPU飙高，接口响应缓慢，卡顿业务线程，影响主流程生产。所以，我们对热集群进行了进一步的拆分，即提示音单独集群的方案。
### 数据写入复杂度问题

---

![](https://mmbiz.qpic.cn/mmbiz_png/PNm6V9cebJV8rrQzrNzO1UnFwWy8sKBv3WvY1CgicG2jrTTIa9Mo6TaRUibh9FBJHJbgTzNWz7LPjCxsVxzM3NtQ/640?wx_fmt=png)

当确定冗余一套提示音集群以后，面临的问题就转变为了上图的写入复杂度问题，从图上来看，我们在拆分这套集群之前，订单中心每次操作订单写入。面临的是三个数据源的写入工作，现在提出第四套数据源 ES REMIND，如果仍然采用直接写入的方式，维护难度过大，这对研发人员是非常不友好的。于是考虑用异构中间件的方式来去写入ES REMIND的数据

异构中间件的优势是屏蔽了数据同步的复杂度，但是随之而来的是数据写入链路可靠性、及时性等问题。而且，数据传输本身一般都具有高可用的需求，之前，高可用在业务应用上，因为业务应用的集群方式本身是计算高可用的。但异构中间件则要在高可用、可靠性、及时性三个维度上满足我们的要求，于是我们了进行如下调研。

|特性\产品|Canal|Maxwell|Databus|CloudCanal|
|---|---|---|---|---|
|社区活跃度|高|中|高|商业化产品|
|可用性|高|低|高|高|
|产品熟练度|高|低|低|高|

首先，在常用的数据存储支撑上没有太大差别，常用的存储组件，这些异构中间件都是支持的。所以，我们更加着眼于以上3个指标。

- 社区活跃度代表了后续的维护性以及开源产品问题的快速响应
- 可用性方面的需求是非常强烈的，数据传输是不能中断的，提示音数据允许少量延迟，但不允许中断
- 最终采用Canal的根本原因还是在学习成本和熟练度上。
# question
关于canal高可用有个问题，当canal-server与zk间网络出现断开，但是消费binlog的相关逻辑正常跑。此时zk检测该server断开，去掉节点。起另一个server此时又变成两个server在消费了。这个情况如何解决呢。可以请张磊老师回答一下吗

现在的qps：2k~4k
canal消息延迟怎么解决：首先考虑业务能否延迟
## 从2000ms缩短到50ms，亿级ES数据搜索性能调优实践

**一、背景**

2020年以来内容标注结果搜索就是社区中后台业务的核心高频使用场景之一，为了支撑复杂的后台搜索，我们将社区内容的关键信息额外存了一份到Elasticsearch中作为二级索引使用。随着标注业务的细分、迭代和时间的推移，这个索引的文档数和搜索的RT开始逐步上升。

下面是这个索引当前的监控情况。

![图片](https://dbaplus.cn/uploadfile/2023/0531/20230531095342561.png)

本文介绍**社区利用IndexSorting，将亿级文档搜索性能由最开始2000ms优化到50ms的过程**。如果大家遇到相似的问题和场景，相信看完之后一定能够一行代码成吨收益。

**二、探索过程**

**1.初步优化**

最开始需求很简单，只需要**取最新发布的动态分页展示**。这时候实现也是简单粗暴，满足功能即可。查询语句如下：

```
GET /content-alias/_search
```

由于首页加载时没加任何筛选条件，于是变成了**从亿级内容库中找出最新发布的10条内容**。

针对这个查询很容易发现问题出现在大结果集的排序，要解决问题，自然的想到了两条路径：

- 去掉sort
    
- 缩小结果集
    

经过用户诉求和开发成本的权衡后，当时决定“先扛住，再优化”：在用户打开首页的时候，默认增加“发布时间在最近一周内”的筛选条件，这时语句变成了：

```
GET /content-alias/_search
```

这个改动上线后，效果可以说是立竿见影，**首页加载速度立马降到了200ms以内**，**平均RT60ms**。这次改动也为我们减小了来自业务的压力，为后续的优化争取了不少调研的时间。

虽然搜索首页的加载速度明显快了，但是并没有实际解决根本问题——**ES大结果集指定字段排序还是很慢**。对业务来说，结果页上的一些边界功能的体验依旧不能尽如人意，比如导出、全量动态的搜索等等。这一点从监控上也能够较明显的看出：慢查询还是存在，并且还伴随着少量的接口超时。

![图片](https://dbaplus.cn/uploadfile/2023/0531/20230531095354423.png)

老实说这个时期我们对于ES的了解还比较基础，只能说会用、知道分片、倒排索引、相关性打分，然后就没有了。总之我们有了方向，开始奋起直追。

**2.细致打磨**

**1)  知识积累**

带着之前遗留的问题，我们开始开始重新出发，从头学习ES。要优化搜索性能，首先我们要知道的是搜索是怎么做的。下面我们就以一个最简单的搜索为例，拆解一下整个搜索请求的过程。

**① 搜索请求**

```
GET /content-alias/_search
```

框精确查询category_id为"xxxxxxxx"的文档，取10条数据，不需要排序，不需要总数。

总流程分3步：
- 客户端发起请求到Node1
- Node1作为协调节点，将请求转发到索引的每个主分片或副分片中，每个分片在本地执行查询。   
- 每个节点返回各自的数据，协调节点汇总后返回给客户端


如图可以大致描绘这个过程：

![图片](https://dbaplus.cn/uploadfile/2023/0531/20230531095406148.png)

我们知道ES是依赖Lucene提供的能力，真正的搜索发生在Lucene中，还需要继续了解Lucene中的搜索过程。

**② Lucene**

Lucene中包含了四种基本数据类型，分别是：
- Index：索引，由很多的Document组成。
- Document：由很多的Field组成，是Index和Search的最小单位。
- Field：由很多的Term组成，包括Field Name和Field Value。
- Term：由很多的字节组成。一般将Text类型的Field Value分词之后的每个最小单元叫做Term。

在介绍Lucene index的搜索过程之前，这里先说一下组成Lucene index的最小数据存储单元——Segment。

Lucene index由许许多多的Segment组成，每一个Segment里面包含着文档的Term字典、Term字典的倒排表、文档的列式存储DocValues以及正排索引。它能够独立的直接对外提供搜索功能，几乎是一个缩小版的Lucene index。

**③ Term字典和倒排表**

![图片](https://dbaplus.cn/uploadfile/2023/0531/20230531095418133.png)

上图是Term字典和其倒排表的大致样子。

当然这里还有些重要数据结构，比如：

- FST：term索引，在内存中构建。可以快速实现单Term、Term范围、Term前缀和通配符查询。
- BKD-Tree：用于数值类型(包括空间点)的快速查找。
- SkipList：倒排表的数据结构。

这里面的细节比较多，感兴趣的可以单独了解，这里不影响我们的整体搜索流程，不过多赘述。

有了Term字典和倒排表我们就能直接拿到搜索条件匹配的结果集了，接下来只需要通过docID去正排索引中取回整个doc然后返回就完事儿了。

这是ES的基本盘理论上不会慢，我们猜测慢查询发生在排序上。那给请求加一个排序会发生什么呢？比如：

```
GET /content-alias/_search
```

通过倒排表拿到的docId是无序的，现在指定了排序字段，最简单直接的办法是全部取出来，然后排序取前10条。这样固然能实现效果，但是效率却是可想而知。那么Lucene是怎么解决的呢？

**④ DocValues**

倒排索引能够解决从词到文档的快速映射，但需要对检索结果进行分类、排序、数学计算等聚合操作时需要文档号到值的快速映射。而正排索引又过于臃肿庞大，怎么办呢？

这时候各位大佬可能就直接想到了列式存储，没有错，Lucene就引入了基于docId的列式存储结构——DocValues。

![图片](https://dbaplus.cn/uploadfile/2023/0531/20230531095445321.png)

比如上表中的DocValues=[2023-01-13, 2023-01-12,2023-03-13]

如果列值是字符串，Lucene会把原来的字符串值按照字典排序生成数字ID，这样的预处理能进一步加快排序速度。于是我们得到了DocValues=[2, 1, 3]

Docvalues的列式存储形式可以加快我们的遍历的速度。到这里一个常规的搜索取前N条记录的请求算是真正的拆解完成。这里不讨论词频、相关性打分、聚合等功能的分析，所以本文对整个过程和数据结构做了大幅简化。如果对这部分感兴趣，欢迎一起讨论。

此时排序慢的问题也逐渐浮出了水面：尽管Docvalues又是列式存储，又是将复杂值预处理为简单值避免了查询时的复杂比较，但是依旧架不住我们需要排序的数据集过大。

看起来ES尽力了，它好像确实不擅长解决我们这个场景的慢查询问题。

不过有灵性的各位读者肯定想到了，**如果能把倒排表按照我们预先指定的顺序存储好，就能省下整个排序的时间。**

**2）IndexSorting**

很快ES官方文档《How to tune for search speed》中提到了一个搜索优化手段——索引排序(Index Sorting)出现在了我们的视野中。

从文档上的描述我们可以知道，索引排序对于搜索性能的提升主要在两个方面：

- 对于多条件并列查询（a and b and ...），索引排序可以帮助我们把不符合条件的文档存在一起，跳过大量的不匹配的文档。但是此技巧仅适用于经常用于筛选的低基数字段。
    
- 提前中断：当搜索排序和索引排序指定的顺序一样时，只需要比较每个段的前 N 个文档，其他的文档仅需要用于总数计算。比如：我们的文档中有一个时间戳，而我们经常需要按照时间戳来搜索和排序，这时候如果指定的索引排序和搜索排序一致，通常能够极大的提高搜索排序的效率。
    

提前中断！！！简直是缺什么来什么，于是我们开始围绕这一点展开调研。

**① 开启索引排序**

```
"sort.field": "publish_time", // 可指定多个字段
```

如上面的例子，文档在写入磁盘时会按照 publish_time 字段的递减序进行排序。

在前面的段落中我们反复提到了docID和正排索引。这里我们顺带简单介绍下他们的关系，首先Segment中的每个文档，都会被分配一个docID，docID从0开始，顺序分配。在没有IndexSorting时，docID是按照文档写入的顺序进行分配的，在设置了IndexSorting之后，docID的顺序就与IndexSorting的顺序一致。

下图描述了docID和正排索引的关系:

![图片](https://dbaplus.cn/uploadfile/2023/0531/20230531095459109.png)

那么再次回头来看看我们最开始的查询：

```
GET /content-alias/_search
```

在Lucene中进行查询时，发现结果集的倒排表顺序刚好是publish_time降序排序的，所以查询到前10条数据之后即可返回，这就做到了提前中断，省下了排序开销。那么代价是什么呢？

**② 代价**

IndexSorting和查询时排序不一样，本质是在写入时对数据进行预处理。所以排序字段只能在创建时指定且不可更改。并且由于写入时要对数据进行排序，所以也会对写入性能也会有一定负面影响。

之前我们提到了Lucene本身对排序也有各种优化，所以如果搜索结果集本身没有那么多的数据，那么就算不开启这个功能，也能有不错的RT。

另外由于多数时候还是要计算总数，所以开启索引排序之后只能提前中断排序过程，还是要对结果集的总数进行count。如果能够不查总数，或者说通过另外的方式获取总数，那么能够更好的利用这个特性。

**③ 小结**

- 针对**大结果集的排序取前N条的场景**下，索引排序能显著提高搜索性能。
- **索引排序只能在创建索引时指定，不可更改**。如果你有多个指定字段排序的场景，可能需要慎重选择排序字段。
- **不获取总数**能更好的利用索引排序。
- 开启索引排序会一定程度**降低写性能**。这里贴一条ElaticsearchBenchmarks的数据截图供大家参考。


**3.效果**

由于我们的业务远远没有达到ES的写入瓶颈，而且也少有频繁变更排序字段的场景。在经过短暂的权衡之后，确定索引排序正是我们需要的，于是开始使用线上真实数据对索引排序的效果进行简单的性能测试。

**1) 性能测试：首页**

![图片](https://dbaplus.cn/uploadfile/2023/0531/20230531095532401.png)

**2)性能测试：其他**

这里开启索引排序后，随机几个常规条件和时间窗口的搜索组合测试。

![图片](https://dbaplus.cn/uploadfile/2023/0531/20230531095545868.png)

可以看到效果非常明显，没有以前的那种尖刺，RT也很稳定，于是我们决定正式上线这个功能。

**3) 线上效果**

- 慢查询
    

![图片](https://dbaplus.cn/uploadfile/2023/0531/20230531095556894.png)

- 整体前后对比
    

![图片](https://dbaplus.cn/uploadfile/2023/0531/20230531095616112.png)

和我们预期的基本一样，**搜索RT大幅降低，慢查询完全消失**。

**4.后续优化**

在探索过程中，其实还发现了一些其他的优化手段，鉴于开发成本和收益，有些我们并没有完全应用于生产环境。这里列出其中几点，希望能给大家一些启发。

- 不获取总数：大部分场景下，不查询总数都能减少开销，提高性能。ES 7.x之后的搜索接口默认不返回总数了，由此可见一斑。
- 自定义routing规则：从上文的查询过程我们可以看到，ES会轮询所有分片以获取想要的数据，如果我们能控制数据的分片落点，那么也能节省不少开销。比如说：如果我们将来如果有大量的场景都是查某个用户的动态，那么可以控制按照用户分片，这样就避免了分片轮询，也能提升搜索效率。
- keyword：不是所有的数字都应该按照数值字段来存，如果你的数字值很少用于范围查询，但是经常被用作term查询，并且对搜索rt很敏感。那么keyword才是最适合的存储方式。
- 数据预处理：就像IndexSoting一样，如果我们能够在写入时预处理好数据，也能节省搜索时的开销。这一点配合_ingest/pipeline 也许能发挥意想不到的效果。

**三、写在最后**

相信看到这里的大家都能看出，我们的优化中也没有涉及到十分高深的技术难点，我们只是在解决问题的过程中，逐步从小白转变成了一个初学者。来一个大牛也许从一开始就能直接绕过我们的弯路，不过万里之行始于足下，最后这里总结一点经验和感受分享给大家，希望能给与我们一样的初学者一些参考。

**ES在大结果集指定字段排序的场景下性能不佳，我们使用时应该尽量避免出现这种场景。如果无法避免，合适的IndexSorting设置能大幅提升排序性能。**

# 水平分库分表排雷帖，骚操作全都精准拿捏了！

**一、背景**

提起分库分表，对于大部分服务器开发来说，其实并不是一个新鲜的名词。随着业务的发展，我们表中的数据量会变的越来越大，字段也可能随着业务复杂度的升高而逐渐增多，我们为了解决单表的查询性能问题，一般会进行分表操作。

同时我们业务的用户活跃度也会越来越高，并发量级不断加大，那么可能会达到单个数据库的处理能力上限。此时我们为了解决数据库的处理性能瓶颈，一般会进行分库操作。不管是分库操作还是分表操作，我们一般都有两种方式应对，一种是垂直拆分，一种是水平拆分。

关于两种拆分方式的区别和特点，互联网上参考资料众多，很多人都写过相关内容，这里就不再进行详细赘述，有兴趣的读者可以自行检索。

此文主要详细聊一聊，我们最实用最常见的水平分库分表方式中的一些特殊细节，希望能帮助大家避免走弯路，找到最合适自身业务的分库分表设计。

【注1】本文中的案例均基于Mysql数据库，下文中的分库分表统指水平分库分表。

【注2】后文中提到到M库N表，均指共M个数据库，每个数据库共N个分表，即总表个数其实为M\*N。

**二、什么是一个好的分库分表方案？**

**2.1 方案可持续性**

前期业务数据量级不大，流量较低的时候，我们无需分库分表，也不建议分库分表。但是一旦我们要对业务进行分库分表设计时，就一定要考虑到分库分表方案的可持续性。

**那何为可持续性？**其实就是：业务数据量级和业务流量未来进一步升高达到新的量级的时候，我们的分库分表方案可以持续使用。

一个通俗的案例，假定当前我们分库分表的方案为10库100表，那么未来某个时间点，若10个库仍然无法应对用户的流量压力，或者10个库的磁盘使用即将达到物理上限时，我们的方案能够进行平滑扩容。

在后文中我们将介绍下目前业界常用的翻倍扩容法和一致性Hash扩容法。

**2.2 数据偏斜问题**

一个良好的分库分表方案，它的数据应该是需要比较均匀的分散在各个库表中的。如果我们进行一个拍脑袋式的分库分表设计，很容易会遇到以下类似问题：

a、某个数据库实例中，部分表的数据很多，而其他表中的数据却寥寥无几，业务上的表现经常是延迟忽高忽低，飘忽不定。

b、数据库集群中，部分集群的磁盘使用增长特别块，而部分集群的磁盘增长却很缓慢。每个库的增长步调不一致，这种情况会给后续的扩容带来步调不一致，无法统一操作的问题。

这边我们定义分库分表最大数据偏斜率为 ：（数据量最大样本 - 数据量最小样本）/ 数据量最小样本。一般来说，如果我们的最大数据偏斜率在5%以内是可以接受的。

![图片](https://dbaplus.cn/uploadfile/2023/0414/20230414105704275.png)

**三、常见的分库分表方案**

**3.1 Range分库分表**

顾名思义，该方案根据数据范围划分数据的存放位置。

举个最简单例子，我们可以把订单表按照年份为单位，每年的数据存放在单独的库（或者表）中。如下图所示：

通过数据的范围进行分库分表，该方案是最朴实的一种分库方案，它也可以和其他分库分表方案灵活结合使用。时下非常流行的分布式数据库：TiDB数据库，针对TiKV中数据的打散，也是基于Range的方式进行，将不同范围内的[StartKey,EndKey)分配到不同的Region上。

下面我们看看该方案的缺点：

- a、最明显的就是数据热点问题，例如上面案例中的订单表，很明显当前年度所在的库表属于热点数据，需要承载大部分的IO和计算资源。
    
- b、新库和新表的追加问题。一般我们线上运行的应用程序是没有数据库的建库建表权限的，故我们需要提前将新的库表提前建立，防止线上故障。
    

这点非常容易被遗忘，尤其是稳定跑了几年没有迭代任务，或者人员又交替频繁的模块。

- c、业务上的交叉范围内数据的处理。举个例子，订单模块无法避免一些中间状态的数据补偿逻辑，即需要通过定时任务到订单表中扫描那些长时间处于待支付确认等状态的订单。
    

这里就需要注意了，因为是通过年份进行分库分表，那么元旦的那一天，你的定时任务很有可能会漏掉上一年的最后一天的数据扫描。

**3.2 Hash分库分表**

虽然分库分表的方案众多，但是Hash分库分表是最大众最普遍的方案，也是本文花最大篇幅描述的部分。

针对Hash分库分表的细节部分，相关的资料并不多。大部分都是阐述一下概念举几个示例，而细节部分并没有特别多的深入，如果未结合自身业务贸然参考引用，后期非常容易出现各种问题。

在正式介绍这种分库分表方式之前，我们先看几个常见的错误案例。

**常见错误案例一：非互质关系导致的数据偏斜问题**

  
 

```
public static ShardCfg shard(String userId) {
```

上述方案是初次使用者特别容易进入的误区，用Hash值分别对分库数和分表数取余，得到库序号和表序号。其实稍微思索一下，我们就会发现，以10库100表为例，如果一个Hash值对100取余为0，那么它对10取余也必然为0。

这就意味着只有0库里面的0表才可能有数据，而其他库中的0表永远为空！

类似的我们还能推导到，0库里面的共100张表，只有10张表中(个位数为0的表序号)才可能有数据。这就带来了非常严重的数据偏斜问题，因为某些表中永远不可能有数据，最大数据偏斜率达到了无穷大。

那么很明显，该方案是一个未达到预期效果的错误方案。数据的散落情况大致示意图如下：

![图片](https://dbaplus.cn/uploadfile/2023/0414/20230414105730727.png)

事实上，只要库数量和表数量非互质关系，都会出现某些表中无数据的问题。

证明如下：

![图片](https://dbaplus.cn/uploadfile/2023/0414/20230414105743133.png)

**那么是不是只要库数量和表数量互质就可用用这种分库分表方案呢？比如我用11库100表的方案，是不是就合理了呢？**

答案是否定的，我们除了要考虑数据偏斜的问题，还需要考虑可持续性扩容的问题，一般这种Hash分库分表的方案后期的扩容方式都是通过翻倍扩容法，那11库翻倍后，和100又不再互质。

当然，如果分库数和分表数不仅互质，而且分表数为奇数(例如10库101表)，则理论上可以使用该方案，但是我想大部分人可能都会觉得使用奇数的分表数比较奇怪吧。

**常见错误案例二：扩容难以持续**

如果避开了上述案例一的陷阱，那么我们又很容易一头扎进另一个陷阱，大概思路如下；

我们把10库100表看成总共1000个逻辑表，将求得的Hash值对1000取余，得到一个介于[0，999)中的数，然后再将这个数二次均分到每个库和每个表中，大概逻辑代码如下：

```
public static ShardCfg shard(String userId) {
```

该方案确实很巧妙的解决了数据偏斜的问题，只要Hash值足够均匀，那么理论上分配序号也会足够平均，于是每个库和表中的数据量也能保持较均衡的状态。

![图片](https://dbaplus.cn/uploadfile/2023/0414/20230414105808282.png)

但是该方案有个比较大的问题，那就是在计算表序号的时候，依赖了总库的数量，那么后续翻倍扩容法进行扩容时，会出现扩容前后数据不在同一个表中，从而无法实施。

如上图中，例如扩容前Hash为1986的数据应该存放在6库98表，但是翻倍扩容成20库100表后，它分配到了6库99表，表序号发生了偏移。这样的话，我们在后续在扩容的时候，不仅要基于库迁移数据，还要基于表迁移数据，非常麻烦且易错。

看完了上面的几种典型的错误案例，那么我们有哪些比较正确的方案呢？下面将结合一些实际场景案例介绍几种Hash分库分表的方案。

**常用姿势一：标准的二次分片法**

上述错误案例二中，整体思路完全正确，只是最后计算库序号和表序号的时候，使用了库数量作为影响表序号的因子，导致扩容时表序号偏移而无法进行。

事实上，我们只需要换种写法，就能得出一个比较大众化的分库分表方案。

```java
public static ShardCfg shard2(String userId) { 
	// ① 算Hash 
	int hash = userId.hashCode(); 
	// ② 总分片数 
	int sumSlot = DB_CNT * TBL_CNT; 
	// ③ 分片序号 
	int slot = Math.abs(hash % sumSlot); 
	// ④ 重新修改二次求值方案 
	int dbIdx = slot / TBL_CNT ; 
	int tblIdx = slot % TBL_CNT ; 
	return new ShardCfg(dbIdx, tblIdx); 
}
```

大家可以注意到，和错误案例二中的区别就是通过分配序号重新计算库序号和表序号的逻辑发生了变化。它的分配情况如下：

![图片](https://dbaplus.cn/uploadfile/2023/0414/20230414105825713.png)

那为何使用这种方案就能够有很好的扩展持久性呢？我们进行一个简短的证明：

![图片](https://dbaplus.cn/uploadfile/2023/0414/20230414105836538.png)

通过上面结论我们知道，通过翻倍扩容后，我们的表序号一定维持不变，库序号可能还是在原来库，也可能平移到了新库中(原库序号加上原分库数)，完全符合我们需要的扩容持久性方案。

![图片](https://dbaplus.cn/uploadfile/2023/0414/20230414105848746.png)

**【方案缺点】**

1、翻倍扩容法前期操作性高，但是后续如果分库数已经是大几十的时候，每次扩容都非常耗费资源。

2、连续的分片键Hash值大概率会散落在相同的库中，某些业务可能容易存在库热点（例如新生成的用户Hash相邻且递增，且新增用户又是高概率的活跃用户，那么一段时间内生成的新用户都会集中在相邻的几个库中）。

**常用姿势二：关系表冗余**

我们可以将分片键对应库的关系通过关系表记录下来，我们把这张关系表称为"路由关系表"。

```
public static ShardCfg shard(String userId) {
```

该方案还是通过常规的Hash算法计算表序号，而计算库序号时，则从路由表读取数据。因为在每次数据查询时，都需要读取路由表，故我们需要将分片键和库序号的对应关系记录同时维护在缓存中以提升性能。

上述实例中**selectRandomDbIdx**方法作用为生成该分片键对应的存储库序号，这边可以非常灵活的动态配置。例如可以为每个库指定一个权重，权重大的被选中的概率更高，权重配置成0则可以将关闭某些库的分配。当发现数据存在偏斜时，也可以调整权重使得各个库的使用量调整趋向接近。

该方案还有个优点，就是理论上后续进行扩容的时候，仅需要挂载上新的数据库节点，将权重配置成较大值即可，无需进行任何的数据迁移即可完成。

如下图所示：最开始我们为4个数据库分配了相同的权重，理论上落在每个库的数据概率均等。但是由于用户也有高频低频之分，可能某些库的数据增长会比较快。当挂载新的数据库节点后，我们灵活的调整了每个库的新权重。

![图片](https://dbaplus.cn/uploadfile/2023/0414/20230414105911151.gif)

该方案似乎解决了很多问题，那么它有没有什么不适合的场景呢？当然有，该方案在很多场景下其实并不太适合，以下举例说明。

a、每次读取数据需要访问路由表，虽然使用了缓存，但是还是有一定的性能损耗。

b、路由关系表的存储方面，有些场景并不合适。例如上述案例中用户id的规模大概是在10亿以内，我们用单库百表存储该关系表即可。但如果例如要用文件MD5摘要值作为分片键，因为样本集过大，无法为每个md5值都去指定关系（当然我们也可以使用md5前N位来存储关系）。

**c、饥饿占位问题，如下详叙：**

我们知道，该方案的特点是后续无需扩容，可以随时修改权重调整每个库的存储增长速度。但是这个愿景是比较缥缈，并且很难实施的，我们选取一个简单的业务场景考虑以下几个问题。

**【业务场景】：**以用户存放文件到云端的云盘业务为例，需要对用户的文件信息进行分库分表设计，有以下假定场景：

①假定有2亿理论用户，假设当前有3000W有效用户。

②平均每个用户文件量级在2000个以内

③用户id为随机16位字符串

④初期为10库，每个库100张表。

我们使用路由表记录每个用户所在的库序号信息。那么该方案会有以下问题：

**第一、**我们总共有2亿个用户，只有3000W个产生过事务的用户。若程序不加处理，用户发起任何请求则创建路由表数据，会导致为大量实际没有事务数据的用户提前创建路由表。

笔者最初存储云盘用户数据的时候便遇到了这个问题，客户端app会在首页查询用户空间使用情况，这样导致几乎一开始就为每个使用者分配好了路由。随着时间的推移，这部分没有数据的"静默"的用户，随时可能开始他的云盘使用之旅而“复苏”，从而导致它所在的库迅速增长并超过单个库的空间容量极限，从而被迫拆分扩容。

解决这个问题的方案，其实就是只针对事务操作(例如购买空间，上传数据，创建文件夹等等)才进行路由的分配，这样对代码层面便有了一些倾入。

**第二、**按照前面描述的业务场景，一个用户最终平均有2000条数据，假定每行大小为1K，为了保证B+数的层级在3层，我们限制每张表的数据量在2000W，分表数为100的话，可以得到理论上每个库的用户数不能超过100W个用户。

也就是如果是3000W个产生过事务的用户，我们需要为其分配30个库，这样会在业务前期，用户平均数据量相对较少的时候，存在非常大的数据库资源的浪费。

解决第二个问题，我们一般可以将很多数据库放在一个实例上，后续随着增长情况进行拆分。也可以后续针对将满的库，使用常规手段进行拆分和迁移。

**常用姿势三：基因法**

还是由错误案例一启发，我们发现案例一不合理的主要原因，就是因为库序号和表序号的计算逻辑中，有公约数这个因子在影响库表的独立性。

那么我们是否可以换一种思路呢？我们使用相对独立的Hash值来计算库序号和表序号。

```java
public static ShardCfg shard(String userId) { 
	int dbIdx = Math.abs(userId.substring(0, 4).hashCode() % DB_CNT ); 
	int tblIdx = Math.abs(userId.hashCode() % TBL_CNT); 
	return new ShardCfg(dbIdx, tblIdx); 
}
```

如上所示，我们计算库序号的时候做了部分改动，我们使用分片键的前四位作为Hash值来计算库序号。

这也是一种常用的方案，我们称为基因法，即使用原分片键中的某些基因（例如前四位）作为库的计算因子，而使用另外一些基因作为表的计算因子。该方案也是网上不少的实践方案或者是其变种，看起来非常巧妙的解决了问题，然而在实际生成过程中还是需要慎重。

笔者曾在云盘的空间模块的分库分表实践中采用了该方案，使用16库100表拆分数据，上线初期数据正常。然而当数据量级增长起来后，发现每个库的用户数量严重不均等，故猜测该方案存在一定的数据偏斜。

为了验证观点，进行如下测试，随机2亿个用户id（16位的随机字符串），针对不同的M库N表方案，重复若干次后求平均值得到结论如下：

```
8库100表 
min=248305(dbIdx=2, tblIdx=64), max=251419(dbIdx=7, tblIdx=8), rate= 1.25% √ 
16库100表 
min=95560(dbIdx=8, tblIdx=42), max=154476(dbIdx=0, tblIdx=87), rate= 61.65% × 
20库100表 
min=98351(dbIdx=14, tblIdx=78), max=101228(dbIdx=6, tblIdx=71), rate= 2.93%
```

我们发现该方案中，分库数为16，分表数为100，数量最小行数仅为10W不到，但是最多的已经达到了15W+，最大数据偏斜率高达61%。按这个趋势发展下去，后期很可能出现一台数据库容量已经使用满，而另一台还剩下30%+的容量。

该方案并不是一定不行，而是我们在采用的时候，要综合分片键的样本规则，选取的分片键前缀位数，库数量，表数量，四个变量对最终的偏斜率都有影响。

例如上述例子中，如果不是16库100表，而是8库100表，或者20库100表，数据偏斜率都能降低到了5%以下的可接受范围。所以该方案的隐藏的"坑"较多，我们不仅要估算上线初期的偏斜率，还需要测算若干次翻倍扩容后的数据偏斜率。

例如你用着初期比较完美的8库100表的方案，后期扩容成16库100表的时候，麻烦就接踵而至。

**常用姿势四：剔除公因数法**

还是基于错误案例一启发，在很多场景下我们还是希望相邻的Hash能分到不同的库中。就像N库单表的时候，我们计算库序号一般直接用Hash值对库数量取余。

那么我们是不是可以有办法去除掉公因数的影响呢？下面为一个可以考虑的实现案例：

经过测算，该方案的最大数据偏斜度也比较小，针对不少业务从N库1表升级到N库M表下，需要维护库序号不变的场景下可以考虑。

**常用姿势五：一致性Hash法**

一致性Hash算法也是一种比较流行的集群数据分区算法，比如RedisCluster即是通过一致性Hash算法，使用16384个虚拟槽节点进行每个分片数据的管理。关于一致性Hash的具体原理这边不再重复描述，读者可以自行翻阅资料。

这边详细介绍如何使用一致性Hash进行分库分表的设计。

我们通常会将每个实际节点的配置持久化在一个配置项或者是数据库中，应用启动时或者是进行切换操作的时候会去加载配置。配置一般包括一个[StartKey,Endkey)的左闭右开区间和一个数据库节点信息，例如：

![图片](https://dbaplus.cn/uploadfile/2023/0414/20230414105931425.png)

示例代码：

我们可以看到，这种形式和上文描述的Range分表非常相似，Range分库分表方式针对分片键本身划分范围，而一致性Hash是针对分片键的Hash值进行范围配置。

正规的一致性Hash算法会引入虚拟节点，每个虚拟节点会指向一个真实的物理节点。这样设计方案主要是能够在加入新节点后的时候，可以有方案保证每个节点迁移的数据量级和迁移后每个节点的压力保持几乎均等。

但是用在分库分表上，一般大部分都只用实际节点，引入虚拟节点的案例不多，主要有以下原因：

- a、应用程序需要花费额外的耗时和内存来加载虚拟节点的配置信息。如果虚拟节点较多，内存的占用也会有些不太乐观。
    

- b、由于mysql有非常完善的主从复制方案，与其通过从各个虚拟节点中筛选需要迁移的范围数据进行迁移，不如通过从库升级方式处理后再删除冗余数据简单可控。
    

- c、虚拟节点主要解决的痛点是节点数据搬迁过程中各个节点的负载不均衡问题，通过虚拟节点打散到各个节点中均摊压力进行处理。
    

而作为OLTP数据库，我们很少需要突然将某个数据库下线，新增节点后一般也不会从0开始从其他节点搬迁数据，而是前置准备好大部分数据的方式，故一般来说没有必要引入虚拟节点来增加复杂度。

**四、常见扩容方案**

**4.1 翻倍扩容法**

翻倍扩容法的主要思维是每次扩容，库的数量均翻倍处理，而翻倍的数据源通常是由原数据源通过主从复制方式得到的从库升级成主库提供服务的方式。故有些文档将其称作**"从库升级法"**。

理论上，经过翻倍扩容法后，我们会多一倍的数据库用来存储数据和应对流量，原先数据库的磁盘使用量也将得到一半空间的释放。如下图所示:

![图片](https://dbaplus.cn/uploadfile/2023/0414/20230414105946148.png)

具体的流程大致如下：

**①、时间点t1：**为每个节点都新增从库，开启主从同步进行数据同步。

**②、时间点t2：**主从同步完成后，对主库进行禁写。

此处禁写主要是为了保证数据的正确性。若不进行禁写操作，在以下两个时间窗口期内将出现数据不一致的问题：

- a、断开主从后，若主库不禁写，主库若还有数据写入，这部分数据将无法同步到从库中。
    

-  b、应用集群识别到分库数翻倍的时间点无法严格一致，在某个时间点可能两台应用使用不同的分库数，运算到不同的库序号，导致错误写入。
    

**③、时间点t3：**同步完全完成后，断开主从关系，理论上此时从库和主库有着完全一样的数据集。

**④、时间点t4：**从库升级为集群节点，业务应用识别到新的分库数后，将应用新的路由算法。

一般情况下，我们将分库数的配置放到配置中心中，当上述三个步骤完成后，我们修改分库数进行翻倍，应用生效后，应用服务将使用新的配置。这里需要注意的是，业务应用接收到新的配置的时间点不一定一致，所以必定存在一个时间窗口期，该期间部分机器使用原分库数，部分节点使用新分库数。这也正是我们的禁写操作一定要在此步完成后才能放开的原因。

**⑤、时间点t5：**确定所有的应用均接受到库总数的配置后，放开原主库的禁写操作，此时应用完全恢复服务。

**⑥、启动离线的定时任务，**清除各库中的约一半冗余数据。

为了节省磁盘的使用率，我们可以选择离线定时任务清除冗余的数据。也可以在业务初期表结构设计的时候，将索引键的Hash值存为一个字段。

那么以上述常用姿势四为例，我们离线的清除任务可以简单的通过sql即可实现（需要防止锁住全表，可以拆分成若干个id范围的子sql执行）：

```
delete from db0.tbl0 where hash_val mod 4 <> 0; 

```

具体的扩容步骤可参考下图：

![图片](https://dbaplus.cn/uploadfile/2023/0414/20230414110000996.gif)

**总结：**通过上述迁移方案可以看出，从时间点t2到t5时间窗口呢内，需要对数据库禁写，相当于是该时间范围内服务器是部分有损的，该阶段整体耗时差不多是在分钟级范围内。若业务可以接受，可以在业务低峰期进行该操作。

当然也会有不少应用无法容忍分钟级写入不可用，例如写操作远远大于读操作的应用，此时可以结合canel开源框架进行窗口期内数据双写操作以保证数据的一致性。

该方案主要借助于mysql强大完善的主从同步机制，能在事前提前准备好新的节点中大部分需要的数据，节省大量的人为数据迁移操作。

但是缺点也很明显，一是过程中整个服务可能需要以有损为代价，二是每次扩容均需要对库数量进行翻倍，会提前浪费不少的数据库资源。

**4.2 一致性Hash扩容**

我们主要还是看下不带虚拟槽的一致性Hash扩容方法，假如当前数据库节点DB0负载或磁盘使用过大需要扩容，我们通过扩容可以达到例如下图的效果。

下图中，扩容前配置了三个Hash分段，发现[-Inf，-10000）范围内的的数据量过大或者压力过高时，需要对其进行扩容。

![图片](https://dbaplus.cn/uploadfile/2023/0414/20230414110014786.png)

主要步骤如下：

**①、时间点t1：**针对需要扩容的数据库节点增加从节点，开启主从同步进行数据同步。

**②、时间点t2：**完成主从同步后，对原主库进行禁写。

 此处原因和翻倍扩容法类似，需要保证新的从库和原来主库中数据的一致性。

**③、时间点t3：**同步完全完成后，断开主从关系，理论上此时从库和主库有着完全一样的数据集。

**④、时间点t4：**修改一致性Hash范围的配置，并使应用服务重新读取并生效。

**⑤、时间点t5：**确定所有的应用均接受到新的一致性Hash范围配置后，放开原主库的禁写操作，此时应用完全恢复服务。

**⑥、启动离线的定时任务，**清除冗余数据。

可以看到，该方案和翻倍扩容法的方案比较类似，但是它更加灵活，可以根据当前集群每个节点的压力情况选择性扩容，而无需整个集群同时翻倍进行扩容。

**五、小结**

本文主要描述了我们进行水平分库分表设计时的一些常见方案。

我们在进行分库分表设计时，可以选择例如范围分表，Hash分表，路由表，或者一致性Hash分表等各种方案。进行选择时需要充分考虑到后续的扩容可持续性，最大数据偏斜率等因素。

文中也列举了一些常见的错误示例，例如库表计算逻辑中公约数的影响，使用前若干位计算库序号常见的数据倾斜因素等等。

我们在实际进行选择时，一定要考虑自身的业务特点，充分验证分片键在各个参数因子下的数据偏斜程度，并提前规划考虑好后续扩容的方案。

## 50亿数据轻松同步，业务一致性核对平台架构设计实践

凌安 2022-10-13 09:40:43

**本文根据凌安老师在〖deeplus直播：****业务数据一致性核对平台的架构设计与实践********〗线上分享演讲内容整理而成。******（文末有回放的方式，不要错过）****

![](https://dbaplus.cn/uploadfile/2022/1013/20221013094313399.png)

  
**分享概要**

一、数据一致性核对的背景与困局

二、业务一致性核对平台概述

三、实时数据核对系统设计

四、10min千万级离线数据核对系统设计

五、50亿数据同步经验分享

**一、数据一致性核对的背景与困局**

**1、背景**

缺少高效易用的工具/平台能在第一时间发现业务与数据问题、能实现快速接入业务实时校验。

![](https://dbaplus.cn/uploadfile/2022/1013/20221013094432525.jpg)

**2、困局**

**1）关注不到位**

- 影响数据一致性、正确性和时效性的因素非常多，再严谨的设计都无法避免人/物的出错。
    

- 严重依赖内部流程自检和业务反馈，不主动做跨系统明细级别的检查。
    

**2）解决难度大**

- 大多出现数据问题的场景隐秘，没有一定经验的专业人员不易发现和解决。
    

- 传统大数据量下的核对思路是使用大数据组件来解决，比如：使用hive的表join、flink的流join等。执行复杂、时效性低、成本相对高。
    

**二、业务一致性核对平台概述**

![](https://dbaplus.cn/uploadfile/2022/1013/20221013094445107.jpg)

图 1-业务一致性核心平台功能架构

业务一致性核对平台针对离线和实时数据，提供不同的核对方式。“使用简单”是平台建设目标之一，下图描述接入步骤：

![](https://dbaplus.cn/uploadfile/2022/1013/20221013094456272.jpg)

图2-业务一致性核对平台接入步骤

**三、实时数据核对系统设计**

**1、设计目标**

- **低侵入：**业务接入方便、改动小、无异常影响、极低性能影响。
    

- **低延时：**数据上报延迟控制在秒级。
    

- **高吞吐：**满足业务高峰期间核对需求。
    

- **实时告警：**告警检测高度灵敏。
    

- **主动熔断：**实时向业务平台发送熔断信号。
    

- **租户隔离：**避免不同流量下的租户数据相互影响。
    

![](https://dbaplus.cn/uploadfile/2022/1013/20221013094507717.jpg)

图 3-实时核对架构

**1）第一阶段**

数据先从上游使用flume-agent采集后，通过mq上传到实时核对接收模块并放入redis缓存中。缓存数据结构采用zset，报文作为元素(member)，时间作为分数(score)写入zset中保存。

**2）第二阶段**

下游数据产生并按照同样的方式上传到核对模块内部。先使用完整报文去上游缓存中匹配，如果找不到在根据业务指定唯一键检索缓存队列（zscan模式迭代寻找）。

**3）第三阶段**

根据下游匹配结果，产生“核对通过”、“疑似差错”、“确定差错”三种结果。

**4）第四阶段**

根据差错告警规则，决定是否通知用户。

**2、实时核对异常应对**

**1）case1：下游先于上游到达**

- 如果下游数据中的事件时间和当前时间差值很小，使用同步锁的方式迭代zset，避免大量zscan操作导致redis cpu飙高。
    

- 下游写入暂存队列时上游刚好到达。
    

当下游数据在“上游暂存”队列中找不到id相同的记录时，会写入“下游暂存”队列。这个“get then set”的操作会和上游数据到达时先查询“下游暂存”队列，不存在则写入“上游暂存”队列的“get then set”操作可能同时发生。可以使用同步锁来解决。

**2）case2：上游数据消费速度远远快于下游**

导致上游redis缓存队列积压“溢出”，此时需要降低上游flume agent发送频率或者停止消费上游kafka队列消息。

**3）case3：过期移出**

上下游暂存队列可能出现一直未被匹配移出的情况，这时候需要定时移出写入时间最早的N个记录，并持久化到差错记录表中。同时定时执行差错重对。

**3、实时核对性能**

**1）吞吐量高** 

采集端通过采集本地磁盘文件数据后批量上送到kafka，中间基本不涉及数据转换（转换流程在业务应用中）。

**2）核对速度快** 

在上游暂存队列积压100w数据量下，下游核对耗时：

- 完全通过2s
    

- 差错4s
    

**四、10min千万级离线数据核对系统设计**

**1、大数据量核对思路**

- 分块checksum
    

- 归并比较
    

![](https://dbaplus.cn/uploadfile/2022/1013/20221013094523283.jpg)

**2、方案对比**

**1）分块checksum**

- **优势：**完全一致时checksum的速度最快。
    

- **不足：**a.性能不够稳定  b.需要设计可靠的顺序无关的checksum算法。
    

**2）归并比较**

- **优势：**性能相对稳定，实现上也相对简单很多。
    

- **不足：**
    

a.需要先排序  

b.排序耗时最短可以是O(NlgN)  

c.比较耗时是O(N)

- **结论：**从实现难易程度和时间稳定性方面考虑，最终采用基于归并比较的方式实现。
    

**3、离线核对1.0**

![](https://dbaplus.cn/uploadfile/2022/1013/20221013094534210.jpg)

图 4-离线核对1.0架构

使用流读的方式，从数据库中快速查询返回**经过排序后**的数据，在内存**逐行**比较。

**注意：**只需要两边数据源各读一行记录即可比较，无需所有记录在内存中再核对（回看归并比较图解）

核对结果中，只对存在差异的记录进行落地保存。

**1）离线核对1.0特点**

**① 核对速度快**  

针对mysql driver使用stream result set方式读取；oracle等支持游标通过设置一定大小的fetch size，能保证在10min内完成千万级的数据核对及差异落地。

**② 告警实时性高**  

kafka 0.10.x之后支持流计算，基于其中滑动时间窗口DSL实现的实时告警，能保证触发时延小于1s。

**③ 支持自动拆分任务** 

通过解析一端的核对sql，找到第一个唯一键的数据分布，拆分成多个查询sql，对应改写另一端的sql，进而拆分成多个核对子任务，实现更大量的数据核对。

**2）离线核对1.0存在的问题**

- 只支持mysql协议和oracle协议的数据库，适用性过于狭窄。
    

- 需要在业务库中排序，排序键过多且数据过大时会导致业务库内存消耗过大且导致二级索引失效。
    

- 字段映射只能通过sql函数实现，增加db压力且专业性过高（希望懂一点点sql的产品也能使用）。
    

**4、离线核对2.0**

**1）目标**

- 突破数据源类型限制
    

- 源端性能影响降到最小
    

**2）大体思路**

数据先查询回应用本地后，然后在本地排序，再基于本地有序数据做归并比较。

![](https://dbaplus.cn/uploadfile/2022/1013/20221013094547239.jpg)

图 5-离线核对2.0架构

**3）优化点**

- 查询接口插件化
    

设计查询器接口，支持自定义和优化改造后的DataX流式查询器。

- 流程标准化
    

提取出数据查询、字段规则转换和KV数据构造标准数据处理流程。

- 本地排序替代数据源排序
    

借助rocksdb内部key的有序性实现二路归并比较。

**5、**RocksDB**是什么**

![](https://dbaplus.cn/uploadfile/2022/1013/20221013094558978.jpg)

图 6-rocksdb架构

**1）特点**

- KV结构
    

- 默认按照key升序保存
    

- 不同列族之间数据隔离
    

- 支持GET/ITERATOR/PUT/DELETE操作
    

- 最大支持数TB数据量的存储
    

- ......
    

**6、RocksDB在2.0中的应用**

![](https://dbaplus.cn/uploadfile/2022/1013/20221013094653151.jpg)  
图 7-数据在rocksdb中的结构

**1）具体设计方案**

- 每个应用节点上的任务共享一个RocksDB实例（一个目录），不同任务数据使用列族隔离。
    

- 源端和目标端的数据各用一个列族（column family）存储，命名：{taskId}_{0/1}。
    

- 行转kv：key=k1,k2.... ;  value=o1,o2,o3....
    

- 单独一个列族存储唯一键重复的记录，并且在每个key后边添加自增id避免再次冲突。
    

- 通过在内存上加一层布隆过滤器，提前过滤肯定不重复的“key”（绝大部分），疑似重复的再查询一次RocksDB确认，提升重复键检测性能。
    

**2）对RocksDB的优化调整**

- 取消WAL日志，降低磁盘消耗和压缩日志带来的CPU消耗，宕机任务自动失败。
    

- 取消内置布隆过滤器，使用自定义的（应用层基于历史任务预估写入数据量创建的布隆过滤器更加准确和高效）。
    

- 限制整个RocksDB最大写入缓存使用，降低堆外内存溢出风险（部分版本rocksdbjni有缺陷）。
    

**7、离线核对2.0带来的突破**

**1）更容易支持多样数据源** 

通过拆解Datax Reader，兼容数据核对流程，利用开源的力量支持更多样的异（同）构数据源间两两核对。

**2）降低对业务库的影响和要求** 

1.0让数据在用户的业务库中排序后，再在平台中“归并”比较。2.0引入RocksDB，让数据在平台内（LSM的SST有序性）排序后再通过迭代“归并”比较，极大降低业务库内存消耗（内存排序）和解除对特定数据源的限制（之前要求必须支持order by asc）

**3）支持任意形式的数据API** 

- 由于不需要数据源支持order by，所以核对数据源不再局限于数据库，而是任意可以获取数据的API(数据库、接口、文件、mq等) 。
    

- 双边各1000W数据核对依旧能保持在7分钟内。
    

**五、50亿数据同步经验分享**

**1、背景**

业务平台A需要在支付时，查询财务凭证数据，而这些凭证数据保存在系统B中。凭证数据情况如下:

- 存量超过50亿（两表关联后），需要拉取字段约70个；
    

- 日增（新增/更新）量500w-1500w；
    

- 使用oracle存储。
    

**2、同步方案**

**1）整体步骤**

- 先做全量迁移，记录起始时间T1和结束时间T2。
    

- 等全量迁移完成后，再做增量迁移，增量时间起点是T1。
    

- 进行数据核对。
    

**2）全量同步任务切分**

- 以业务日期（yyyyMMdd）字段budat作为切分条件，先以年为界限分成2005-2021共计7个任务{job1,job2,job3,job4,job5,job6,job7}，不同job并发执行。
    

- 每个job再以“天”为切分条件，分成356个实例task{task1,task2,...,task365}，task串行执行。
    

**3）增量同步任务**

- 使用更新时间update_time作为查询条件，配置定时10分钟调度任务，每次同步范围是update_time between t-15 and t-5，t的初始值是T1。
    

- 当一个调度时间nextScheduleTime > now()的时间，[T1,now()]内的增量数据已经全部同步完毕！
    

**4）数据一致性检查任务**

- 和全量任务相似，用budat作为条件每次核对1天数据，直到下一个调度时间大于当前时间则停止。
    

- 增量核对任务改成每天核对T-1的增量数据。
    

**3、经验总结**

- **分治**
    

找到能使数据拆分（相对）均衡的字段，以该字段为核心继续细分数据直到单次任务的耗时可接受。

如果倾斜严重，可以多选择几个字段或者改成where k=xx and hash(k)%N = n

- **顺序**
    

先全量，再增量。因为历史数据是可能被修改的，如果刚好全量同步任务和增量任务对查询到同一条记录的不同版本，可能会出现写入冲突或者旧版本数据覆盖性版本数据等情况

- **增量范围**
    

增量任务的起始时间一般要比全量任务早几分钟

增量任务的截止时间小于当前时间，避免由于上游（一般拉从库）主从延迟过高导致数据丢失

- **数据核对是必要的**
    

**Q&A**

**Q1：多租户下集群资源怎么隔离及分配，怎么保证任务高峰时间任务的稳定运行？后续会考虑支持Yarn调度吗？**

**A1：**保证任务高峰时间任务的稳定运行有几种方式：实时核对mq每个租户独立主题，缓存空间也独立；离线核对是共享，因为cpu承担主要压力；离线核对的任务会依赖自行实现的均衡调度算法，让每个节点的资源消耗相对均衡。后续是否使用Yarn还在评估。

**Q2：数据实时同步场景下怎么核对明细数据一致性？比如MySQL实时同步进MongoDB。**

**A2：**首先，不建议在实时同步的情况下核对最近的热点数据，但是可以过滤相对稳定的状态数据，比如：“成功”、“失败”、“完成”等；其次，离线核对可以支持异构数据源之间的核对。

**Q3：这么多校验任务是否会将线上库查挂？**

**A3：**每次核对只会创建一个核对任务，是否定时核对取决于用户，所以多校验任务不会将线上库查挂。

**Q4：数据实时同步，上游数据可能随时发生变化，有没有办法找到一致性时间点，然后做数据核对？**

**A4：**不建议在实时同步的情况下核对最近的热点数据，但是可以过滤相对稳定的状态数据，比如：“成功”、“失败”、“完成”等。

**Q5：数据同步如果是带时间字段，发现现有数据时间大于同步过来的时间，就直接丢弃同步过来的数据，可行吗？**

**A5：**这一做法有风险，可能会导致核对漏数的情况。

**Q6：数据在本地排序，再进行归并比较，是否会大幅占用本地内存？**

**A6：**不会大幅占用本地内存，数据在本地排序并进行归并比较具体依赖的是RocksDB的实现性能。

**Q7：使用stream resultset数据库长连接着有没有影响？**

**A7：**目前尚未发现对源系统有较大影响，具体情况可以关注一下DataX的使用问题。

**Q8：实时核对中一侧数据长时间未到达，这种情况该怎么处理？**

**A8：**如果一侧数据长时间未到达会触发过期淘汰策略，记为”疑似差错”并落库，后续定时进行差错重对。

**Q9：大数据场景下支持先group后sum、count之类的聚合函数吗？**

**A9：**是否支持此类聚合函数与数据源有关，与数据核对无关。

**Q10：用了DataX就能保证数据一致性？**

**A10：**DataX只是ETL工具，并不是数据一致性校验工具，但是DataX会记录同步出错数据。

## 1、为什么要做服务之间的数据一致性

作为互联网公司的研发工程师，微服务的架构思想对于各位读者朋友来说，已经不是陌生东西。我们当中的大多数人，或多或少经历过从单体应用到微服务化的系统拆分和演进过程。我们按照庞大系统的业务功能和特征，将其从一个单体的大应用，逐渐地拆分成很多的子系统的协同配合完成业务功能，甚至拆分后的某些子系统服务，还可能再拆分出来更多的更细颗粒度的子系统服务。拆分后的服务之间，采用PRC调用方式的通信，也就越来越多。随之而来的，跨系统服务之间的数据一致性的问题就会越来越突出了。比如电商系统中营销活动系统的积分和优惠券的发放和扣减，比如电商系统的核心下单核心链路上，首页瀑布流，商详页，下单页等等商品价格全链路一致性等等，支撑这些业务功能的实现，往往可能需要依赖来自N个不同的业务系统服务提供的数据读写服务能力来完成。

## 2、如何实现服务之间的数据一致性

说到数据一致性这个话题，我们可以想到的最常用最熟悉的解决问题的方式就是事务处理了。它存在的意义是为了保证系统中所有的数据都是符合预期的，并且存在关联关系的数据之间不会产生矛盾，即数据状态一致性。事务的概念，起源于数据库，发展到今天，几乎在每一个业务系统中都会涉及到。尤其在一些大型复杂的分布式系统中，事务的概念已经不仅局限于数据库，还可以延伸为一切需要保证数据一致性的应用场景，包括但不限于数据库、事务内存、缓存、消息队列、分布式存储等等，这些都有可能会用到事务处理。

今天我们探讨的主题是服务之间数据一致性问题。当然，实际生产落地中，在不同业务背景下，具备可行性的方案也是非常多的，各有优劣和适用场景。我们从中选择一两个具体实现，聊一些相关的设计和实践。

## 3、几个核心名词

为了方便正在阅读本篇文章的同学对后续内容的阅读和理解，我们先对文章中使用到的几个名词的语义做一些解释和约定：

业务侧系统：指的是发起执行业务操作的一方系统服务，可以简单理解为消费方。

平台侧系统：指的是为发起执行业务操作而提供基础能力的一方系统服务，可以简单理解为服务方。

执行业务操作：指的是对数据的读写操作需要依赖多个系统服务的协同完成，业务侧系统发起，平台侧系统执行最终的数据读写操作。这样场景中就普遍存在着服务之间的数据一致性问题（备注 ：业务侧系统和平台侧系统是一个逻辑概念，并非一定要存在具体的应用服务与之对应）。

业务操作标识：指的是业务操作应该归属的业务类型或者业务场景的标识，它是平台侧系统创建并且统一管理的，然后发放给业务侧系统，在业务侧系统的服务调用平台侧系统提供服务能力的身份信息。业务标识可以设计为单层结构，也可以设计为多层结构，符合当前系统和业务的需求即可。

业务操作唯一ID：指的是某个具体业务操作在某一次或者多次重复执行的唯一性标识。生产实践中，它一般是由业务侧系统的服务自定义实现和管理的，也可以是基于平台侧系统提供有约束性质和方便管理的规则限制下，再由业务侧系统的服务自定义实现，后者是比较推荐的方式。

业务操作记录表：指的是记录业务操作的日志流水表。生产实践中，一般是由业务侧系统的服务创建并且管理。

## 4、方案一：业务侧系统保证最终一致性

### 4.1 核心思想

通过业务侧系统的服务保证数据的最终一致性，其核心思想就是业务侧系统记录下来每一次具体业务操作的执行流水日志信息，并且对没有全部成功的变更结果，触发执行数据一致性的校验核对工作。

### 4.2 设计原则

- 平台侧系统服务，提供支持执行业务操作的基础服务能力的接口。特别强调一点，这里是需要根据业务操作标识和业务操作唯一ID来实现接口的幂等设计。为什么我们有了唯一ID，同时还是需要有业务操作标识？因为在实际的生产实践中，在各种内因和外因的背景下，需要兼顾系统的稳定性和业务迭代的灵活性，很难做到绝对的全局性唯一ID的生成。更多时候，只需要在某个业务侧系统的内部，保证全局唯一性即可，这也是符合实际情况的系统设计。类似的解决问题的思路，在其他的系统设计场景，也是有非常高的借鉴价值的。
- 平台侧系统服务，提供执行业务操作后的结果查询接口，支持根据业务操作标识和业务操作的唯一性ID查询能力。
- 业务操作记录表，支持记录和识别业务操作的标识和每次执行的唯一ID。
- 业务侧系统服务，触发对业务操作记录表的数据一致性的检查核对工作，执行核对的方式，比如实时的同步检查核对、准实时的异步检查核对、定时任务的异步检查核对等等，为了保证自己和平台侧系统的数据最终一致性。

### 4.3 流程图

#### 4.3.1 数据一致性的校验核对同步执行流程

![图片](https://s4.51cto.com/oss/202306/07/43f06e3456ae8e84e437714e05c50ccf14df18.png "图片")

#### 4.3.2 数据一致性的校验核对异步核对链路

![图片](https://s2.51cto.com/oss/202306/07/147681044917ac6f731652331e36ebc271ea2c.png "图片")

## 5、方案二：平台侧系统保证最终一致性

### 5.1 核心思想

- 通过平台侧系统的服务保证数据的最终一致性，核心思想是平台侧系统的每一次的数据变更，都主动地寻找业务侧系统，来确认本次数据变更结果是否符合预期。

### 5.2 设计的基本原则：

平台侧系统，提供支持业务操作执行的基础服务能力的接口，需要根据业务操作标识和唯一ID做幂等设计。它和方案一的一致性原则类似，省略不再赘述。

平台侧系统，提供业务操作的执行结果确认的回调SPI，可以方便业务侧系统来实现，根据业务操作标识和业务操作的唯一ID。

业务侧系统，提供根据业务操作标识和业务操作的唯一ID，来判断两边的数据是否具备一致性的回调实现。

### 5.3 流程图

#### 5.3.1 数据一致性的校验核对同步核对链路

![图片](https://s4.51cto.com/oss/202306/07/654ca2860575f1dc1e7170d57ace3fee17fa27.png "图片")

#### 5.3.2 数据一致性的校验核对异步核对链路

![图片](https://s8.51cto.com/oss/202306/07/28bc20c89ffb71e80941100c27b439f1e3920d.png "图片")

## 6、实践过程中一些经验分享

这一部分，我将会对平台侧系统和业务侧系统的接口设计的部分细节，做一些简单的扩展阐述。希望为大家后续的研发工作提供一些思路。后续的文章中，将会针对其中一些具体的解决方案，做更详细的阐述。

首先，接口幂等性设计，将从如下角度进行阐述:  数据结构，状态存储，异常处理，返回结果唯一等等角度做一些总结分享。

### 6.1 数据结构设计

接口幂等性设计，是基于业务操作的标识(这里是称之为Tag)和业务操作的唯一ID来实现的。业务操作标识的设计，可以是单层的设计，也可以是多层的设计。其中，多层的设计是为了满足业务侧系统的存在复杂并且多业务场景的诉求。业务操作的唯一ID的生成方式，可以是没有任何业务含义的自增趋势的不可重复的ID，比如MySQL的自增主键ID，分布式ID生成器等等方式，也可以是业务侧系统的某些特定的业务字段 ，比如用户的userId，订单的orderId，商品的spuId，skuId等等。在实际实践中，后者是我们比较推荐的常用方式，可以实现在不增加系统复杂度和额外依赖资源的同时，又可以和业务侧系统达到高度的契合。

### 6.2 状态存储设计

在一般情况下，建议把MySQL存储当做我们首选的存储，MySQL提供非常完善的数据一致性保证能力，最简单的方式是基于数据库的联合唯一索引设计，多次层Tag + 唯一ID的业务唯一键。但是也是有缺陷的，比如MySQL自身的性能瓶颈和昂贵的存储成本。性能上的瓶颈，可以通过访问MySQL的幂等校验之前，增加访问Redis的幂等校验，校验不通过抛出异常，在MySQL幂等校验通过以后，异步刷数据到Redis中，这样保证Redis校验通过的同时MySQL校验一定是通过的。我们可以接受Redis的幂等校验的不准确性，仅仅是期望它成为流量漏斗的上层，为MySQL承担起流量过滤作用，当然你可以有其他的更多的方案来做这件事，甚至组合起来使用。也可以增加分库分表的策略，来解决MySQL的性能瓶颈。在MySQL的存储成本是相对比较高的，我们可以对历史的数据做归档处理，只保留一部分的热数据，原则上保持单表的数据行数在500w~1000w之间，同时也可以有能支持一定量的历史数量查询。同时这个过程也需要考虑无锁处理问题和MySQL空间碎片的问题等等。

### 6.3 异常处理设计

第一步，明确导致发生异常的原因有哪些？一般可以归为几个分类，网路异常，数据格式错误，业务逻辑异常。第二步，针对特定类型的问题，我们做出相应处理方案。比如我们重试机制，控制重试频次，重试周期的衰减时间执行控制，处理数据处理的终态的异常数据的兜底处理机制等等方式。

### 6.4 返回结果唯一

- 我需要保证接口的返回的数据，再多次重复调用执行，依旧保证完全相同。我们可以基于状态机的流转控制，返回相同的状态码，也可以对一些核心业务参数做核对校验，如果不通过返回特定的异常码等等。
- 此外，平台侧系统的提供给基础能力接口的设计要求我们研发同学思考和考虑的更多，比如一致性延迟问题，状态机的设计，并发问题处理，接口不可用解决等等。

###   6.5 延迟问题的容忍度

- 能否在业务侧系统服务期望的时间点，完成数据一致性的校验核对工作？若有延迟，延迟是多少？尤其是极端场景下的延迟是多少？
- 案例：如果使用定时任务，做数据一致性校验核对工作。比如一个周期（假设1min），还有很多数据未完成核对工作，剩余多少，以及对业务侧系统的影响。解决思路：1. 评估和设计一个合理的周期大小；2. 选择全量核对和增量核对的选择；3. 增加核对的扫描的数据范围的策略；4. 增量核对确保不丢失未核对过的数据，等等。
- 案例：如果使用MQ消息，我们可能面临的问题是消息堆积，消息丢失等等场景MQ问题带来的数据不一致问题。
- 案例：如果使用同步等待方式，是可以将数据一致性的延迟降低为0，但是系统吞吐能力和可用性等等，都是无法保证，这也是选择权衡的结果。

###  6.6 基于状态机的设计

- 基于状态机的设计中，一定是有初始态和终态的，代表数据的核对工作，有始有终。至于中间态，可以有多个中间态，也可以是仅有一个中间态，这个和实际的需求和背景相关联的，可以灵活地控制。其中的终态，一般情况下都不会只有一种，而是有两大类，一种是成功的终态表示数据实现最终一致性，一种是失败的终态表示不因为不可抗拒的因素导致的数据不一致产生。失败的终态，也是可以设计出多种状态，根据实际需要来设计。比如多次重试从初始态到终态的耗时和处于失败态的数据核对检测工作的占比，一定程度上代表着业务侧系统对数据一致性延迟的容忍度。这应该是我们必须关注的核心指标信息。

### 6.7 并发问题

- 我们在创建一个初始化态的流水日志记录的时候，是一个MySQL的insert操作（假设你选择了MySQL作为存储），需要避免创建多条的业务操作唯一ID的记录。最简单粗暴的方式，依赖DB的联合唯一索引是可以实现的。但是需要考虑在并发比较多的时候，带来的性能和吞吐问题，甚至导致创建初始化态就失败的问题。
- 对于相同数据并发写的问题，我们成功执行一条insert语句，大多数情况可以满足我们业务侧系统的预期。我们可以采用加锁，排队等待，分组等待排队等等手段，限制类似场景的并发数来解决。这种方式，随着业务的发展扩张，可能会面临系统的吞吐量不足以支撑业务的问题。
- 解决上述的吞吐量下降的问题，我们可能又会想到采用MQ的方式来削峰填谷，因为实际生产实践中，并发写问题的往往都是一个特点 瞬时性发生的系统尖刺。采用MQ的方式，可以保证平台侧系统创建初始化态的流水日志的系统吞吐量。
- 在以上的基础之上，我们还是可以采用隔离拆分的方式，比如服务接口拆分层面的隔离，MQ的topic拆分的隔离等等，配合不同的限流熔断等等系统保护策略的方式以及不同的系统资源倾斜等等，解决平台侧系统的性能问题。

###   6.8 需要解决不可用

- 熔断限流，资源隔离，多元化的降级策略等等，这些是大家都非常熟悉的系统可用性保障的手段，这部分相关的内容，就不再展开叙述了。

### 6.9 需要提供可视化和可观测

完善告警机制，比如异常状态告警，超出阈值告警等等，让相关的业务侧系统和平台侧系统同学可以快速感知到问题并且介入解决问题。

建设监控大盘，比如 MySQL，Redis，MQ，以及数据核对工作的状态的监控等等，都是需要我们去一步一步建设起来的。

定位和排查问题的工具，拆分后的系统，其系统的复杂度是指数增长的，这个方面也是非常重要的。

## 7、总结

在本篇文章中，阐述了两种处理数据一致性问题的解决方案，从核心思想，设计原则，系统交互流程等等做了详细的阐述，比对两种方案，各有优劣和各自的适用场景。方案一，业务侧系统来保证数据的一致性，更适用于对数据的一致性有相对比较强的耦合依赖关系的业务场景，需要依赖业务操作的执行结果做出判断，执行不同后续业务逻辑分支的执行。 案例: 同一个商品在不同修改商品信息(变更不同的字段，变更不同表的字段)的入口触发异步更新C端缓存的单品维度的商品全量缓存数据构建，变更的事务是在成功完成提交以后，方可执行本次变更对应的后续缓存构建。方案二，平台侧系统来保证数据的一致性，更适用于业务侧系统，关注点是数据的最终执行结果的业务场景，案例: 不同业务场景入口的库存扣减和库存回滚执行结果。最后，提到在生产实践过程中一些经验和解决方案的总结分享，每个点都是值得继续深入探讨。


# 刨根问底：Kafka到底会不会丢数据？

王江华 2023-08-17 10:34:14

那么 Kafka 到底会不会丢数据呢？如果丢数据，究竟该怎么解决呢？

只有掌握了这些， 我们才能处理好 Kafka 生产级的一些故障，从而更稳定地服务业务。

认真读完这篇文章，我相信你会对Kafka 如何解决丢数据问题，有更加深刻的理解。

![图片](https://dbaplus.cn/uploadfile/2023/0817/20230817104002553.png)

一、总体概述

越来越多的互联网公司使用消息队列来支撑自己的核心业务。由于是核心业务，一般都会要求消息传递过程中最大限度做到不丢失，如果中间环节出现数据丢失，就会引来用户的投诉，年底绩效就要背锅了。

那么使用 Kafka 到底会不会丢数据呢？如果丢数据了该怎么解决呢？为了避免类似情况发生，除了要做好补偿措施，我们更应该在系统设计的时候充分考虑系统中的各种异常情况，从而设计出一个稳定可靠的消息系统。

大家都知道 Kafka 的整个架构非常简洁，是分布式的架构，主要由 Producer、Broker、Consumer 三部分组成，后面剖析丢失场景会从这三部分入手来剖析。

![图片](https://dbaplus.cn/uploadfile/2023/0817/20230817104014732.png)

二、消息传递语义剖析

在深度剖析消息丢失场景之前，我们先来聊聊「消息传递语义」到底是个什么玩意？

所谓的消息传递语义是 Kafka 提供的 Producer 和 Consumer 之间的消息传递过程中消息传递的保证性。主要分为三种， 如下图所示：

![图片](https://dbaplus.cn/uploadfile/2023/0817/20230817104027968.png)

- 首先当 Producer 向 Broker 发送数据后，会进行 commit，如果 commit 成功，由于 Replica 副本机制的存在，则意味着消息不会丢失，但是 Producer 发送数据给 Broker 后，遇到网络问题而造成通信中断，那么 Producer 就无法准确判断该消息是否已经被提交（commit），这就可能造成 at least once 语义。
    
- 在 Kafka 0.11.0.0 之前， 如果 Producer 没有收到消息 commit 的响应结果，它只能重新发送消息，确保消息已经被正确的传输到 Broker，重新发送的时候会将消息再次写入日志中；而在 0.11.0.0 版本之后， Producer 支持幂等传递选项，保证重新发送不会导致消息在日志出现重复。为了实现这个, Broker 为 Producer 分配了一个ID，并通过每条消息的序列号进行去重。也支持了类似事务语义来保证将消息发送到多个 Topic 分区中，保证所有消息要么都写入成功，要么都失败，这个主要用在 Topic 之间的 exactly once 语义。
    

其中启用幂等传递的方法配置：enable.idempotence = true。

启用事务支持的方法配置：设置属性 transcational.id = "指定值"。

- 从 Consumer 角度来剖析, 我们知道 Offset 是由 Consumer 自己来维护的, 如果 Consumer 收到消息后更新 Offset， 这时 Consumer 异常 crash 掉， 那么新的 Consumer 接管后再次重启消费，就会造成 at most once 语义（消息会丢，但不重复）。
    

- 如果 Consumer 消费消息完成后, 再更新 Offset， 如果这时 Consumer crash 掉，那么新的 Consumer 接管后重新用这个 Offset 拉取消息， 这时就会造成 at least once 语义（消息不丢，但被多次重复处理）。
    

总结：默认 Kafka 提供 「at least once」语义的消息传递，允许用户通过在处理消息之前保存 Offset 的方式提供 「at most once」 语义。如果我们可以自己实现消费幂等，理想情况下这个系统的消息传递就是严格的「exactly once」, 也就是保证不丢失、且只会被精确的处理一次，但是这样是很难做到的。

从 Kafka 整体架构图我们可以得出有三次消息传递的过程：

- Producer 端发送消息给 Kafka Broker 端。
    
- Kafka Broker 将消息进行同步并持久化数据。
    
- Consumer 端从 Kafka Broker 将消息拉取并进行消费。
    

在以上这三步中每一步都可能会出现丢失数据的情况， 那么 Kafka 到底在什么情况下才能保证消息不丢失呢？

通过上面三步，我们可以得出：Kafka 只对 「已提交」的消息做「最大限度的持久化保证不丢失」。

怎么理解上面这句话呢？

- 首先是 「已提交」的消息，当 Kafka 中 N 个 Broker 成功收到一条消息并写入到日志文件后，它们会告诉 Producer 端这条消息已成功提交了，那么这时该消息在 Kafka 中就变成 "已提交消息" 了。
    

这里的 N 个 Broker 我们怎么理解呢？这主要取决于对 "已提交" 的定义， 这里可以选择只要一个 Broker 成功保存该消息就算已提交，也可以是所有 Broker 都成功保存该消息才算是已提交。

- 其次是 「最大限度的持久化保证不丢失」，也就是说 Kafka 并不能保证在任何情况下都能做到数据不丢失。即 Kafka 不丢失数据是有前提条件的。假如这时你的消息保存在 N 个 Broker 上，那么前提条件就是这 N 个 Broker 中至少有1个是存活的，就可以保证你的消息不丢失。
    

也就是说 Kafka 是能做到不丢失数据的， 只不过这些消息必须是 「已提交」的消息，且还要满足一定的条件才可以。

了解了 Kafka 消息传递语义以及什么情况下可以保证不丢失数据，下面我们来详细剖析每个环节为什么会丢数据，以及如何最大限度避免丢失数据。

三、消息丢失场景剖析

1.Producer 端丢失场景剖析

在剖析 Producer 端数据丢失之前，我们先来了解下 Producer 端发送消息的流程，对于不了解 Producer 的读者们，可以查看 [聊聊 Kafka Producer 那点事](http://mp.weixin.qq.com/s?__biz=Mzg3MTcxMDgxNA==&mid=2247488849&idx=1&sn=febda095589f02553d9191528f271c07&chksm=cefb3c60f98cb576fd9c58d760b9a5e4ae32a0c001e2049b591297d904a0401646448999c78a&scene=21#wechat_redirect)。

消息发送流程如下：

![图片](https://dbaplus.cn/uploadfile/2023/0817/20230817104045264.png)

- 首先我们要知道一点就是 Producer 端是直接与 Broker 中的 Leader Partition 交互的，所以在 Producer 端初始化中就需要通过 Partitioner 分区器从 Kafka 集群中获取到相关 Topic 对应的 Leader Partition 的元数据 。
    

- 待获取到 Leader Partition 的元数据后直接将消息发送过去。
    

- Kafka Broker 对应的 Leader Partition 收到消息会先写入 Page Cache，定时刷盘进行持久化（顺序写入磁盘）。
    

- Follower Partition 拉取 Leader Partition 的消息并保持同 Leader Partition 数据一致，待消息拉取完毕后需要给 Leader Partition 回复 ACK 确认消息。
    
- 待 Kafka Leader 与 Follower Partition 同步完数据并收到所有 ISR 中的 Replica 副本的 ACK 后，Leader Partition 会给 Producer 回复 ACK 确认消息。
    

根据上图以及消息发送流程可以得出：Producer 端为了提升发送效率，减少IO操作，发送数据的时候是将多个请求合并成一个个 RecordBatch，并将其封装转换成 Request 请求「异步」将数据发送出去（也可以按时间间隔方式，达到时间间隔自动发送），所以 Producer 端消息丢失更多是因为消息根本就没有发送到 Kafka Broker 端。

导致 Producer 端消息没有发送成功有以下原因：

- 网络原因：由于网络抖动导致数据根本就没发送到 Broker 端。
    
- 数据原因：消息体太大超出 Broker 承受范围而导致 Broker 拒收消息。
    

另外 Kafka Producer 端也可以通过配置来确认消息是否生产成功：

![图片](https://dbaplus.cn/uploadfile/2023/0817/20230817104057913.png)

在 Kafka Producer 端的 acks 默认配置为1， 默认级别是 at least once 语义, 并不能保证 exactly once 语义。

既然 Producer 端发送数据有 ACK 机制, 那么这里就可能会丢数据的!!!

- acks = 0：由于发送后就自认为发送成功，这时如果发生网络抖动， Producer 端并不会校验 ACK 自然也就丢了，且无法重试。
    
- acks = 1：消息发送 Leader Parition 接收成功就表示发送成功，这时只要 Leader Partition 不 Crash 掉，就可以保证 Leader Partition 不丢数据，但是如果 Leader Partition 异常 Crash 掉了， Follower Partition 还未同步完数据且没有 ACK，这时就会丢数据。
    
- acks = -1 或者 all： 消息发送需要等待 ISR 中 Leader Partition 和 所有的 Follower Partition 都确认收到消息才算发送成功, 可靠性最高, 但也不能保证不丢数据,比如当 ISR 中只剩下 Leader Partition 了, 这样就变成 acks = 1 的情况了。
    

2.Broker 端丢失场景剖析

接下来我们来看看 Broker 端持久化存储丢失场景， 对于不了解 Broker 的读者们，可以先看看 [聊聊 Kafka Broker 那点事](http://mp.weixin.qq.com/s?__biz=Mzg3MTcxMDgxNA==&mid=2247488847&idx=1&sn=fe2dace4ebf39001062fa331711606ba&chksm=cefb3c7ef98cb5689c91b02edb345cc75751ae7e2daf27d8de9a47f9ecc3eedaf3551eead037&scene=21#wechat_redirect)，数据存储过程如下图所示：

![图片](https://dbaplus.cn/uploadfile/2023/0817/20230817104113242.png)

Kafka Broker 集群接收到数据后会将数据进行持久化存储到磁盘，为了提高吞吐量和性能，采用的是「异步批量刷盘的策略」，也就是说按照一定的消息量和间隔时间进行刷盘。首先会将数据存储到 「PageCache」 中，至于什么时候将 Cache 中的数据刷盘是由「操作系统」根据自己的策略决定或者调用 fsync 命令进行强制刷盘，如果此时 Broker 宕机 Crash 掉，且选举了一个落后 Leader Partition 很多的 Follower Partition 成为新的 Leader Partition，那么落后的消息数据就会丢失。

既然 Broker 端消息存储是通过异步批量刷盘的，那么这里就可能会丢数据的!!!

- 由于 Kafka 中并没有提供「同步刷盘」的方式，所以说从单个 Broker 来看还是很有可能丢失数据的。
    
- kafka 通过「多 Partition （分区）多 Replica（副本）机制」已经可以最大限度保证数据不丢失，如果数据已经写入 PageCache 中但是还没来得及刷写到磁盘，此时如果所在 Broker 突然宕机挂掉或者停电，极端情况还是会造成数据丢失。
    

3.Consumer 端丢失场景剖析

接下来我们来看看 Consumer 端消费数据丢失场景，对于不了解 Consumer 的读者们，可以先看看 聊聊 Kafka Consumer 那点事, 我们先来看看消费流程：

![图片](https://dbaplus.cn/uploadfile/2023/0817/20230817104126702.png)

![图片](https://dbaplus.cn/uploadfile/2023/0817/20230817104137738.png)

- Consumer 拉取数据之前跟 Producer 发送数据一样, 需要通过订阅关系获取到集群元数据, 找到相关 Topic 对应的 Leader Partition 的元数据。
    
- 然后 Consumer 通过 Pull 模式主动的去 Kafka 集群中拉取消息。
    
- 在这个过程中，有个消费者组的概念（不了解的可以看上面链接文章），多个 Consumer 可以组成一个消费者组即 Consumer Group，每个消费者组都有一个Group-Id。同一个 Consumer Group 中的 Consumer 可以消费同一个 Topic 下不同分区的数据，但是不会出现多个 Consumer 去消费同一个分区的数据。
    
- 拉取到消息后进行业务逻辑处理，待处理完成后，会进行 ACK 确认，即提交 Offset 消费位移进度记录。
    
- 最后 Offset 会被保存到 Kafka Broker 集群中的 __consumer_offsets 这个 Topic 中，且每个 Consumer 保存自己的 Offset 进度。 
    

根据上图以及消息消费流程可以得出消费主要分为两个阶段：

- 获取元数据并从 Kafka Broker 集群拉取数据。
    
- 处理消息，并标记消息已经被消费，提交 Offset 记录。
    

既然 Consumer 拉取后消息最终是要提交 Offset， 那么这里就可能会丢数据的!!!

- 可能使用的「自动提交 Offset 方式」
    
- 拉取消息后「先提交 Offset，后处理消息」，如果此时处理消息的时候异常宕机，由于 Offset 已经提交了,  待 Consumer 重启后，会从之前已提交的 Offset 下一个位置重新开始消费， 之前未处理完成的消息不会被再次处理，对于该 Consumer 来说消息就丢失了。
    
- 拉取消息后「先处理消息，再进行提交 Offset」， 如果此时在提交之前发生异常宕机，由于没有提交成功 Offset， 待下次 Consumer 重启后还会从上次的 Offset 重新拉取消息，不会出现消息丢失的情况， 但是会出现重复消费的情况，这里只能业务自己保证幂等性。        
    

四、消息丢失解决方案

上面带你从 Producer、Broker、Consumer 三端剖析了可能丢失数据的场景，下面我们就来看看如何解决才能最大限度保证消息不丢失。

1.Producer 端解决方案

在剖析 Producer 端丢失场景的时候， 我们得出其是通过「异步」方式进行发送的，所以如果此时是使用「发后即焚」的方式发送，即调用 Producer.send(msg) 会立即返回，由于没有回调，可能因网络原因导致 Broker 并没有收到消息，此时就丢失了。

因此我们可以从以下几方面进行解决 Producer 端消息丢失问题：

1）更换调用方式：

弃用调用发后即焚的方式，使用带回调通知函数的方法进行发送消息，即 Producer.send(msg, callback), 这样一旦发现发送失败， 就可以做针对性处理。

```
Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback);
```

- 网络抖动导致消息丢失，Producer 端可以进行重试。
    
- 消息大小不合格，可以进行适当调整，符合 Broker 承受范围再发送。
    

通过以上方式可以保证最大限度消息可以发送成功。

2）ACK 确认机制：

该参数代表了对"已提交"消息的定义。

需要将 request.required.acks 设置为 -1/ all，-1/all 表示有多少个副本 Broker 全部收到消息，才认为是消息提交成功的标识。

针对 acks = -1/ all , 这里有两种非常典型的情况：

- 数据发送到 Leader Partition， 且所有的 ISR 成员全部同步完数据， 此时，Leader Partition 异常 Crash 掉，那么会选举新的 Leader Partition，数据不会丢失， 如下图所示：
    

![图片](https://dbaplus.cn/uploadfile/2023/0817/20230817104156922.png)

- 数据发送到 Leader Partition，部分 ISR 成员同步完成，此时 Leader Partition 异常 Crash， 剩下的 Follower Partition 都可能被选举成新的 Leader Partition，会给 Producer 端发送失败标识， 后续会重新发送数据，数据可能会重复， 如下图所示：
    

![图片](https://dbaplus.cn/uploadfile/2023/0817/20230817104207718.png)

因此通过上面分析，我们还需要通过其他参数配置来进行保证：

replication.factor >= 2

min.insync.replicas > 1

这是 Broker 端的配置，下面会详细介绍。

3）重试次数 retries：

该参数表示 Producer 端发送消息的重试次数。

需要将 retries 设置为大于0的数， 在 Kafka 2.4 版本中默认设置为Integer.MAX_VALUE。另外如果需要保证发送消息的顺序性，配置如下：

```
retries = Integer.MAX_VALUE
```

这样 Producer 端就会一直进行重试直到 Broker 端返回 ACK 标识，同时只有一个连接向 Broker 发送数据保证了消息的顺序性。

4）重试时间 retry.backoff.ms：

该参数表示消息发送超时后两次重试之间的间隔时间，避免无效的频繁重试，默认值为100ms,  推荐设置为300ms。

2.Broker 端解决方案

在剖析 Broker 端丢失场景的时候， 我们得出其是通过「异步批量刷盘」的策略，先将数据存储到 「PageCache」，再进行异步刷盘， 由于没有提供 「同步刷盘」策略， 因此 Kafka 是通过「多分区多副本」的方式来最大限度保证数据不丢失。

我们可以通过以下参数配合来保证：

1）unclean.leader.election.enable：

该参数表示有哪些 Follower 可以有资格被选举为 Leader , 如果一个 Follower 的数据落后 Leader 太多，那么一旦它被选举为新的 Leader， 数据就会丢失，因此我们要将其设置为false，防止此类情况发生。

2）replication.factor：

该参数表示分区副本的个数。建议设置 replication.factor >=3, 这样如果 Leader 副本异常 Crash 掉，Follower 副本会被选举为新的 Leader 副本继续提供服务。

3）min.insync.replicas：

该参数表示消息至少要被写入成功到 ISR 多少个副本才算"已提交"，建议设置min.insync.replicas > 1, 这样才可以提升消息持久性，保证数据不丢失。

另外我们还需要确保一下 replication.factor > min.insync.replicas, 如果相等，只要有一个副本异常 Crash 掉，整个分区就无法正常工作了，因此推荐设置成： replication.factor = min.insync.replicas +1, 最大限度保证系统可用性。

3.Consumer 端解决方案

在剖析 Consumer 端丢失场景的时候，我们得出其拉取完消息后是需要提交 Offset 位移信息的，因此为了不丢数据，正确的做法是：拉取数据、业务逻辑处理、提交消费 Offset 位移信息。

我们还需要设置参数 enable.auto.commit = false, 采用手动提交位移的方式。

另外对于消费消息重复的情况，业务自己保证幂等性, 保证只成功消费一次即可。

五、总结

至此，我们一起来总结一下这篇文章的重点。

- 从 Kafka 整体架构上概述了可能发生数据丢失的环节。
    
- 带你剖析了「消息传递语义」的概念， 确定了 Kafka 只对「已提交」的消息做「最大限度持久化保证不丢失」。
    
- 带你剖析了 Producer、Broker、Consumer 三端可能导致数据丢失的场景以及具体的高可靠解决方案。
    

# 即拿即用：高并发下如何防重？

最近测试给我提了一个bug，说我之前提供的一个批量复制商品的接口，产生了重复的商品数据。

追查原因之后发现，这个事情没想象中简单，可以说一波多折。

**一、需求**

产品有个需求：用户选择一些品牌，点击确定按钮之后，系统需要基于一份默认品牌的商品数据，复制出一批新的商品。

拿到这个需求时觉得太简单了，三下五除二就搞定。

我提供了一个复制商品的基础接口，给商城系统调用。

当时的流程图如下：

![图片](https://dbaplus.cn/uploadfile/2023/0704/20230704102754692.png)

如果每次复制的商品数量不多，使用同步接口调用的方案问题也不大。

**二、性能优化**

但由于每次需要复制的商品数量比较多，可能有几千。

如果每次都是用同步接口的方式复制商品，可能会有性能问题。

因此，后来我把复制商品的逻辑改成使用mq异步处理。

改造之后的流程图：

![图片](https://dbaplus.cn/uploadfile/2023/0704/20230704102806847.png)

复制商品的结果还需要通知商城系统：

![图片](https://dbaplus.cn/uploadfile/2023/0704/20230704102817692.png)

这个方案看起来，挺不错的。

但后来出现问题了。

**三、出问题了**

测试给我们提了一个bug，说我之前提供的一个批量复制商品的接口，产生了重复的商品数据。

经过追查之后发现，商城系统为了性能考虑，也改成异步了。

他们没有在接口中直接调用基础系统的复制商品接口，而是在job中调用的。

站在他们的视角流程图是这样的：

![图片](https://dbaplus.cn/uploadfile/2023/0704/20230704102829451.png)

用户调用商城的接口，他们会往请求记录表中写入一条数据，然后在另外一个job中，异步调用基础系统的接口去复制商品。

但实际情况是这样的：商城系统内部出现了bug，在请求记录表中，同一条请求产生了重复的数据。这样导致的结果是，在job中调用基础系统复制商品接口时，发送了重复的请求。

刚好基础系统现在是使用RocketMQ异步处理的。由于商城的job一次会取一批数据（比如:20条记录），在极短的时间内（其实就是在一个for循环中）多次调用接口，可能存在相同的请求参数连续调用复制商品接口情况。于是，出现了并发插入重复数据的问题。

为什么会出现这个问题呢？

**四、多线程消费**

RocketMQ的消费者，为了性能考虑，默认是用多线程并发消费的，最大支持64个线程。

例如：

```
@RocketMQMessageListener(topic = "${com.susan.topic:PRODUCT_TOPIC}",
```

也就是说，如果在极短的时间内，连续发送重复的消息，就会被不同的线程消费。

即使在代码中有这样的判断：

```
Product oldProduct = query(hashCode);
```

在插入数据之前，先判断该数据是否已经存在，只有不存在才会插入。

但由于在并发情况下，不同的线程都判断商品数据不存在，于是同时进行了插入操作，所以就产生了重复数据。

如下图所示：

![图片](https://dbaplus.cn/uploadfile/2023/0704/20230704102847745.png)

**五、顺序消费**

为了解决上述并发消费重复消息的问题，我们从两方面着手：
- 商城系统修复产生重复记录的bug。
- 基础系统将消息改成单线程顺序消费。

我仔细思考了一下，如果只靠商城系统修复bug，以后很难避免不出现类似的重复商品问题，比如：如果用户在极短的时间内点击创建商品按钮多次，或者商城系统主动发起重试。

所以，基础系统还需进一步处理。

其实RocketMQ本身是支持顺序消费的，需要消息的生产者和消费者一起改。

生产者改为：

```
rocketMQTemplate.asyncSendOrderly(topic, message, hashKey, new SendCallback() {
```

重点是要调用rocketMQTemplate对象的asyncSendOrderly方法，发送顺序消息。

消费者改为：

```
@RocketMQMessageListener(topic = "${com.susan.topic:PRODUCT_TOPIC}",
```

接收消息的重点是RocketMQMessageListener注解中的consumeMode参数，要设置成ConsumeMode.ORDERLY，这样就能顺序消费消息了。

修改后关键流程图如下：

![图片](https://dbaplus.cn/uploadfile/2023/0704/20230704102909635.png)

两边都修改之后，复制商品这一块就没有再出现重复商品的问题了。

但是，修完bug之后，我又思考了良久。

复制商品只是创建商品的其中一个入口，如果有其他入口，跟复制商品功能同时创建新商品呢？

不也会出现重复商品问题？

虽说，这种概率非常非常小。

但如果一旦出现重复商品问题，后续涉及到要合并商品的数据，非常麻烦。

经过这一次的教训，一定要防微杜渐。

不管是用户，还是自己的内部系统，从不同的入口创建商品，都需要解决重复商品创建问题。

那么，如何解决这个问题呢？

**六、唯一索引**

解决重复商品数据问题，最快成本最低最有效的办法是：给表建唯一索引。

想法是好的，但我们这边有个规范就是：业务表必须都是逻辑删除。

而我们都知道，要删除表的某条记录的话，如果用delete语句操作的话。

例如：

```
delete from product where id=123;
```

这种delete操作是物理删除，即该记录被删除之后，后续通过sql语句基本查不出来。（不过通过其他技术手段可以找回，那是后话了）

还有另外一种是逻辑删除，主要是通过update语句操作的。

例如：

```
update product set delete_status=1,edit_time=now(3) 
```

逻辑删除需要在表中额外增加一个删除状态字段，用于记录数据是否被删除。在所有的业务查询的地方，都需要过滤掉已经删除的数据。

通过这种方式删除数据之后，数据仍然还在表中，只是从逻辑上过滤了删除状态的数据而已。

其实对于这种逻辑删除的表，是没法加唯一索引的。

为什么呢？

假设之前给商品表中的name和model加了唯一索引，如果用户把某条记录删除了，delete_status设置成1了。后来，该用户发现不对，又重新添加了一模一样的商品。

由于唯一索引的存在，该用户第二次添加商品会失败，即使该商品已经被删除了，也没法再添加了。

这个问题显然有点严重。

有人可能会说：把name、model和delete_status三个字段同时做成唯一索引不就行了？

答：这样做确实可以解决用户逻辑删除了某个商品，后来又重新添加相同的商品时，添加不了的问题。但如果第二次添加的商品，又被删除了。该用户第三次添加相同的商品，不也出现问题了？

由此可见，如果表中有逻辑删除功能，是不方便创建唯一索引的。

**七、分布式锁**

接下来，你想到的第二种解决数据重复问题的办法可能是：加分布式锁。

目前最常用的性能最高的分布式锁，可能是redis分布式锁了。

使用redis分布式锁的伪代码如下：

```
try{
```

不过需要在finally代码块中释放锁。

其中lockKey是由商品表中的name和model组合而成的，requestId是每次请求的唯一标识，以便于它每次都能正确地释放锁。还需要设置一个过期时间expireTime，防止释放锁失败，锁一直存在，导致后面的请求没法获取锁。

如果只是单个商品，或者少量的商品需要复制添加，则加分布式锁没啥问题。

主要流程如下：

![图片](https://dbaplus.cn/uploadfile/2023/0704/20230704102938374.png)

可以在复制添加商品之前，先尝试加锁。如果加锁成功，则再查询商品是否存在，如果不存在，则添加商品。此外，在该流程中如果加锁失败，或者查询商品时不存在，则直接返回。

加分布式锁的目的是：保证查询商品和添加商品的两个操作是原子性的操作。

但现在的问题是，我们这次需要复制添加的商品数量很多，如果每添加一个商品都要加分布式锁的话，会非常影响性能。

显然对于批量接口，加redis分布式锁，不是一个理想的方案。

**八、统一mq异步处理**

前面我们已经聊过，在批量复制商品的接口，我们是通过RocketMQ的顺序消息，单线程异步复制添加商品的，可以暂时解决商品重复的问题。

但那只改了一个添加商品的入口，还有其他添加商品的入口。

能不能把添加商品的底层逻辑统一一下，最终都调用同一段代码。然后通过RocketMQ的顺序消息，单线程异步添加商品。

主要流程如下图所示：

![图片](https://dbaplus.cn/uploadfile/2023/0704/20230704103009836.png)

这样确实能够解决重复商品的问题。

但同时也带来了另外两个问题：

- 现在所有的添加商品功能都改成异步了，之前同步添加商品的接口如何返回数据呢？这就需要修改前端交互，否则会影响用户体验。
    

- 之前不同的添加商品入口，是多线程添加商品的，现在改成只能由一个线程添加商品，这样修改的结果导致添加商品的整体效率降低了。
    

由此，综合考虑了一下各方面因素，这个方案最终被否定了。

**九、insert on duplicate key update**

其实，在mysql中存在这样的语法，即：insert on duplicate key update。

在添加数据时，mysql发现数据不存在，则直接insert。如果发现数据已经存在了，则做update操作。

不过要求表中存在唯一索引或PRIMARY KEY，这样当这两个值相同时，才会触发更新操作，否则是插入。

现在的问题是PRIMARY KEY是商品表的主键，是根据雪花算法提前生成的，不可能产生重复的数据。

但由于商品表有逻辑删除功能，导致唯一索引在商品表中创建不了。

由此，insert on duplicate key update这套方案，暂时也没法用。

此外，insert on duplicate key update在高并发的情况下，可能会产生死锁问题，需要特别注意一下。

**十、insert ignore**

在mysql中还存在这样的语法，即：insert ... ignore。

在insert语句执行的过程中：mysql发现如果数据重复了，就忽略，否则就会插入。

它主要是用来忽略，插入重复数据产生的Duplicate entry 'XXX' for key 'XXXX'异常的。

不过也要求表中存在唯一索引或PRIMARY KEY。

但由于商品表有逻辑删除功能，导致唯一索引在商品表中创建不了。

由此可见，这个方案也不行。

温馨提醒一下，使用insert ... ignore也有可能会导致死锁。

**十一、防重表**

之前聊过，因为有逻辑删除功能，给商品表加唯一索引，行不通。

后面又说了加分布式锁，或者通过mq单线程异步添加商品，影响创建商品的性能。

那么，如何解决问题呢？

我们能否换一种思路，加一张防重表，在防重表中增加商品表的name和model字段作为唯一索引。

例如：

```
CREATE TABLE `product_unique` (
```

其中表中的id可以用商品表的id，表中的name和model就是商品表的name和model，不过在这张防重表中增加了这两个字段的唯一索引。

视野一下子被打开了。

在添加商品数据之前，先添加防重表。如果添加成功，则说明可以正常添加商品，如果添加失败，则说明有重复数据。

防重表添加失败，后续的业务处理，要根据实际业务需求而定。

如果业务上允许添加一批商品时，发现有重复的，直接抛异常，则可以提示用户：系统检测到重复的商品，请刷新页面重试。

例如：

```
try {
```

在批量插入数据时，如果出现了重复数据，捕获DuplicateKeyException异常，转换成BusinessException这样运行时的业务异常。

还有一种业务场景，要求即使出现了重复的商品，也不抛异常，让业务流程也能够正常走下去。

例如：

```
try {
```

在插入数据时，如果出现了重复数据，则捕获DuplicateKeyException，在catch代码块中再查询一次商品数据，将数据库已有的商品直接返回。

如果调用了同步添加商品的接口，这里非常关键的一点，是要返回已有数据的id，业务系统做后续操作，要拿这个id操作。

当然在执行execute之前，还是需要先查一下商品数据是否存在，如果已经存在，则直接返回已有数据，如果不存在，才执行execute方法。这一步千万不能少。

例如：

```
Product oldProduct = productMapper.query(product);
```

千万注意：防重表和添加商品的操作必须要在同一个事务中，否则会出问题。

顺便说一下，还需要对商品的删除功能做特殊处理一下，在逻辑删除商品表的同时，要物理删除防重表。用商品表id作为查询条件即可。

说实话，解决重复数据问题的方案挺多的，没有最好的方案，只有最适合业务场景的，最优的方案。



# 购物车
![[Pasted image 20230821145745.png]]
![[Pasted image 20230821145805.png]]

![[Pasted image 20230821145906.png]]
![[Pasted image 20230821150101.png]]
![[Pasted image 20230821150219.png]]
![[Pasted image 20230821150327.png]]
![[Pasted image 20230821150417.png]]

# 一种简易但设计全面的ID生成器思考

分布式系统中，全局唯一 ID 的生成是一个老生常谈但是非常重要的话题。随着技术的不断成熟，大家的分布式全局唯一 ID 设计与生成方案趋向于趋势递增的 ID，这篇文章将结合我们系统中的 ID 针对实际业务场景以及性能存储和可读性的考量以及优缺点取舍，进行深入分析。本文并不是为了分析出最好的 ID 生成器，而是分析设计 ID 生成器的时候需要考虑哪些，如何设计出最适合自己业务的 ID 生成器。

> 项目地址：https://github.com/JoJoTec/id-generator

![image](https://ask.qcloudimg.com/http-save/yehe-8494643/44bea33de2fded6d062a6dbce9e05461.jpeg)

首先，先放出我们的全局唯一 ID 结构：

![image](https://ask.qcloudimg.com/http-save/yehe-8494643/92ac3ec8c77f0017e2b4f66713e91e6e.png)

这个唯一 ID 生成器是放在每个微服务进程里面的插件这种架构，不是有那种唯一 ID 生成中心的架构：

- 开头是时间戳格式化之后的字符串，可以直接看出年月日时分秒以及毫秒。由于分散在不同进程里面，需要考虑不同微服务时间戳不同是否会产生相同 ID 的问题。
- 中间业务字段，最多 4 个字符。
- 最后是自增序列。这个自增序列通过 [Redis](https://cloud.tencent.com/product/crs?from_column=20065&from=20065) 获取，同时做了分散压力优化以及集群 fallback 优化，后面会详细分析。

![image](https://ask.qcloudimg.com/http-save/yehe-8494643/d08eeb39a942dad1b8846205ba5e9de3.jpeg)

序列号的开头是时间戳格式化之后的字符串，由于分散在不同进程里面，不同进程当前时间可能会有差异，这个差异可能是毫秒或者秒级别的。所以，**要考虑 ID 中剩下的部分是否会产生相同的序列**。

自增序列由两部分组成，第一部分是 Bucket，后面是从 Redis 中获取的对应 Bucket 自增序列，获取自增序列的伪代码是：

```javascript
1. 获取当前线程 ThreadLocal 的 position，position 初始值为一个随机数。
2. position += 1，之后对最大 Bucket 大小（即 2^8）取余，即对 2^8 - 1 取与运算，获取当前 Bucket。
   如果当前 Bucket 没有被断路，则执行做下一步，否则重复 2。
   如果所有 Bucket 都失败，则抛异常退出
3. redis 执行： incr sequence_num_key:当前Bucket值，拿到返回值 sequence
4. 如果 sequence 大于最大 Sequence 值，即 2^18， 对这个 Bucket 加锁（sequence_num_lock:当前Bucket值），
   更新 sequence_num_key:当前Bucket值 为 0，之后重复第 3 步。否则，返回这个 sequence
   
-- 如果 3，4 出现 Redis 相关异常，则将当前 Bucket 加入断路器，重复步骤 2
```

复制

在这种算法下，即使每个实例时间戳可能有差异，只要在**最大差异时间内，同一业务不生成超过 Sequence 界限数量的实体，即可保证不会产生重复 ID**。

同时，我们设计了 Bucket，**这样在使用 Redis 集群的情况下，即使某些节点的 Redis 不可用，也不会影响我们生成 ID**。

![image](https://ask.qcloudimg.com/http-save/yehe-8494643/90fc5f3fe548b5743aa809ba944f5a1d.jpeg)

当前 OLTP 业务离不开传统[数据库](https://cloud.tencent.com/solution/database?from_column=20065&from=20065)，目前最流行的数据库是 [MySQL](https://cloud.tencent.com/product/cdb?from_column=20065&from=20065)，MySQL 中最流行的 OLTP 存储引擎是 InnoDB。考虑业务扩展与分布式数据库设计，InnoDB 的主键 ID 一般不采用自增 ID，而是通过全局 ID 生成器生成。这个 ID 对于 MySQL InnoDB 有哪些性能影响呢？我们通过将 BigInt 类型主键和我们这个字符串类型的主键进行对比分析。

首先，由于 B+ 树的索引特性，主键越是严格递增，插入性能越好。越是混乱无序，插入性能越差。这个原因，主要是 B+ 树设计中，如果值无序程度很高，数据被离散存储，造成 innodb 频繁的页分裂操作，严重降低插入性能。可以通过下面两个图的对比看出：

**插入有序**：

![image](https://ask.qcloudimg.com/http-save/yehe-8494643/c85cc059600bc57c93a78e2bd2e7a347.gif)

**插入无序**：

![image](https://ask.qcloudimg.com/http-save/yehe-8494643/f4e29c56cf5bd4678c2d4bebd82cdb8a.gif)

如果插入的主键 ID 是离散无序的，那么每次插入都有可能对于之前的 B+ 树子节点进行裂变修改，**那么在任一一段时间内，整个 B+ 树的每一个子分支都有可能被读取并修改，导致内存效率低下**。**如果主键是有序的（即新插入的 id 比之前的 id 要大），那么只有最新分支的子分支以及节点会被读取修改，这样从整体上提升了插入效率**。

我们设计的 ID，由于是当前时间戳开头的，从**趋势上是整体递增**的。**基本上能满足将插入要修改的 B+ 树节点控制在最新的 B+ 树分支上，防止树整体扫描以及修改**。

![image](https://ask.qcloudimg.com/http-save/yehe-8494643/4060cee50ac6ed424675b08870f21805.jpeg)

和 SnowFlake 算法生成的 long 类型数字，在数据库中即 bigint 对比：bigint，在 InnoDB 引擎行记录存储中，无论是哪种行格式，都占用 **8 字节**。我们的 ID，char类型，字符编码采用 **latin1**（**因为只有字母和数字**），占用 27 字节，大概是 bigint 的 3 倍多。

- MySQL 的主键 B+ 树，如果主键越大，那么单行占用空间越多，即 B+ 树的分支以及叶子节点都会占用更多空间，造成的后果是：MySQL 是按页加载文件到内存的，也是按页处理的。这样一页内，可以读取与操作的数据将会变少。**如果数据表字段只有一个主键，那么 MySQL 单页（不考虑各种头部，例如页头，行头，表头等等）能加载处理的行数， bigint 类型是我们这个主键的 3 倍多**。但是数据表一般不会只有主键字段，还会有很多其他字段，**其他字段占用空间越多，这个影响越小**。
- MySQL 的二级索引，叶子节点的值是主键，那么同样的，单页加载的叶子节点数量，bigint 类型是我们这个主键的 3 倍多。但是目前一般 MySQL 的配置，都是内存资源很大的，造成其实二级索引搜索主要的性能瓶颈并不在于此处，**这个 3 倍影响对于大部分查询可能就是小于毫秒级别的优化提升。相对于我们设计的这个主键带来的可读性以及便利性来说，是微不足道的**。

![image](https://ask.qcloudimg.com/http-save/yehe-8494643/86fcb3b9b170a9797b7cc950ba1d6d18.jpeg)

业务上，其实有很多需要按创建时间排序的场景。比如说查询一个用户今天的订单，并且按照创建时间倒序，那么 SQL 一般是：

```javascript
## 查询数量，为了分页
select count(1) from t_order where user_id = "userid" and create_time > date(now());
## 之后查询具体信息
select * from t_order where user_id = "userid" and create_time > date(now()) order by create_time limit 0, 10;
```

复制

订单表肯定会有 user_id 索引，但是随着业务增长，下单量越来越多导致这两个 SQL 越来越慢，这时我们就可以有两种选择：

1. 创建 user_id 和 create_time 的联合索引来减少扫描，但是大表额外增加索引会导致占用更多空间并且和现有索引重合有时候会导致 SQL 优化有误。
2. 直接使用我们的主键索引进行筛选:

```javascript
select count(1) from t_order where user_id = "userid" and id > "210821";
select * from t_order where user_id = "userid" and id > "210821" order by id desc limit 0, 10;
```

复制

但是需要注意的是，第二个 SQL 执行会比创建 user_id 和 create_time 的联合索引执行原来的 SQL 多一步 `Creating sort index` 即将命中的数据在内存中排序，如果命中量比较小，即大部分用户在当天的订单量都是几十几百这个级别的，那么基本没问题，这一步不会消耗很大。否则还是需要创建 user_id 和 create_time 的联合索引来减少扫描。

如果不涉及排序，**仅仅筛选的话**，这样做基本是没问题的。

![image](https://ask.qcloudimg.com/http-save/yehe-8494643/56e6bf8b11b8905952ad12f164bbac37.jpeg)

我们不希望用户通过 ID 得知我们的业务体量，例如我现在下一单拿到 ID，之后再过一段时间再下一单拿到 ID，对比这两个 ID 就能得出这段时间内有多少单。

我们设计的这个 ID 完全没有这个问题，因为最后的序列号：

1. 所有业务共用同一套序列号，每种业务有 ID 产生的时候，就会造成 Bucket 里面的序列递增。
2. 序列号同一时刻可能不同线程使用的不同的 Bucket，并且结果是位操作，很难看出来那部分是序列号，那部分是 Bucket。

![image](https://ask.qcloudimg.com/http-save/yehe-8494643/9ef4465321be8d061ff8cdb7da8eb974.jpeg)

从我们设计的 ID 上，可以直观的看出这个业务的实体，是在什么时刻创建出来的：

- 一般客服受理问题的时候，拿到 ID 就能看出来时间，直接去后台系统对应时间段调取用户相关操作记录即可。简化操作。
- 一般的业务有报警系统，一般报警信息中会包含 ID，从我们设计的 ID 上就能看出来创建时间，以及属于哪个业务。
- 日志一般会被采集到一起，所有微服务系统的日志都会汇入例如 ELK 这样的系统中，从搜索引擎中搜索出来的信息，从 ID 就能直观看出业务以及创建时间。

![image](https://ask.qcloudimg.com/http-save/yehe-8494643/d1fc12c0514cf6e834bfa81e075c27ec.jpeg)

在给出的项目源码地址中的单元测试中，我们测试了通过 embedded-redis 启动一个本地 redis 的单线程，200 线程获取 ID 的性能，并且对比了只操作 redis，只获取序列以及获取 ID 的性能，我的破电脑结果如下：

```javascript
单线程
BaseLine(only redis): 200000 in: 28018ms
Sequence generate: 200000 in: 28459ms
ID generate: 200000 in: 29055ms

200线程
BaseLine(only redis): 200000 in: 3450ms
Sequence generate: 200000 in: 3562ms
ID generate: 200000 in: 3610ms
```

# 拆解雪花算法生成规则 | 京东物流技术团队
## 1 介绍

雪花算法（Snowflake）是一种生成分布式全局唯一ID的算法，生成的ID称为Snowflake IDs或snowflakes。这种算法由Twitter创建，并用于推文的ID。目前仓储平台生成ID是用的雪花算法修改后的版本。

雪花算法几个特性

- 生成的ID分布式唯一和按照时间递增有序，毫秒数在高位，自增序列在低位，整个ID都是趋势递增的。
- 不依赖数据库等三方系统，稳定性更高，性能非常高的。
- 可以根据自身业务特性分配bit位，非常灵活。

## 2 其他分布式唯一ID生成方案

### 2.1 数据库生成

以MySQL为例，单库单表，给字段设置auto_increment来生成全局唯一ID

优点:

- 非常简单，维护成本比较低
- ID唯一，单调递增，可以设置固定步长

缺点:

- 可用性难以保证，每次生成ID都需要访问数据库，瓶颈在于单台MySQL读写性能上，如果数据库挂掉会造成服务不可用，这是一个致命的问题

### 2.2 UUID

UUID是由一组32位数的16进制数字所构成，故UUID理论上的总数为16^32=2^128，约等于3.4 x 10^38。也就是说若每纳秒产生1兆个UUID，要花100亿年才会将所有UUID用完。UUID的标准型式包含32个16进制数字，以连字号分为五段，形式为8-4-4-4-12的32个字符。示例：550e8400-e29b-41d4-a716-446655440000

优点:

- 本地生成ID，不需要进行远程调用，没有网络耗时
- 基本没有性能上限

缺点:

- 可读性差
- 长度过长，16字节128位，生成的UUID通常是36位(包含-)，有些场景可能不适用。如果用作数据库主键，在MySQL的InnoDB引擎下长度过长，二级索引(非主键索引)会占用很大的空间。
- 无法保证趋势递增，在MySQL的InnoDB引擎下，新插入数据会根据主键来寻找合适位置，会导致频繁的移动、分页增加了很多开销。

## 3 snowflake算法实现细节

### 3.1 拆解64bit位

snowflake生成的id通常是一个64bit数字，java中用long类型。  
![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aeca9f0e8eff43dc963ec35c42850a5a~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)  
图1：snowflake算法中的64-bit划分方式

- 1-bit不用于生成ID(符号位) long 范围[-2^(64-1), 2^(64-1) ] , (64-1)中的1代表的就是符号位
- 41-bit时间戳(毫秒)可以表示1 x 2^41 / (1000 x 3600 x 24 x 365) = 69年的时间
- 10-bit可以分别表示1 x 2^10 = 1024台机器,范围[0,1023]
- 12-bit表示1ms内自动递增的序列号，1 x 2^12 = 4096个 范围[0,4095]。单机1ms可以生成4096个不重复的ID

通过上述方式进行生成ID，可以保证1024台机器在任意69年的时间段里不会出现重复的ID，而且单台机器支持一秒能够生成409.6万个ID。

这种方式可以支撑大部分业务，如果不满足，可以根据自身业务特点来调整不同命名空间占用的bit数。如果我们有划分IDC的需求，可以将10-bit分5-bit给IDC，分5-bit给工作机器。这样就可以表示32个IDC，每个IDC下可以有32台机器。如果我们的机器位比较特殊，数值相对较大，但是对并发要求不高，还可以将时间位调整为秒级，时间位节省出10-bit留给机器位。

- 1-bit符号位
- 31-bit时间戳(秒)1 x 2^31/ (3600 x 24 x 365) = 68年
- 22-bit机器位 运维平台给提供的数值 范围 [0,2^22-1]
- 10-bit序列号 范围[0, 2^10 - 1]共1024个

通过上述方式进行生成ID，可以保证4194303台机器在任意68年的时间段里不会出现重复的ID，而且单台机器支持一秒能够生成1024个ID。
### 3.3 一些疑问

#### 3.3.1 为什么bit位置只利用了63位？

因为long在java中占8字节，每字节8bit，一共64bit，其中有1个bit位是符号位不能用做生成ID，如果符号位也用来做ID中的1个bit为会导致ID出现负数，影响趋势递增特性。

#### 3.3.2 计算最大机器ID

见代码中注释 序号1  
maxInstanceId = -1L ^ (-1L<<instanceIdBits)  
等价于 maxInstanceId = -1 ^( -1<<10)  
① -1二进制

> 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111

② -1左移10位 -1<<10 二进制

> 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1100 0000 0000

①与②进行异或运算 异或运算:同为假，异为真,所以最终结果应该为

> 0000 0000 0000 0000 0000 0000 0000 00000000 0000 0000 0000 0000 0011 1111 1111

最后:maxInstanceId = 2^10 - 1 = 1023  
sequenceMask 计算方法相同，结果为2^12 - 1 = 4095

#### 3.3.3 计算序列号位

见代码中注释 序号4

ini

复制代码

`if (lastTimestamp == timestamp) {     sequence = (sequence + 1) & sequenceMask;     if (sequence == 0) {         timestamp = tilNextSecs(lastTimestamp);     } } else {     sequence = 0L; }`

其中这段代码的是计算序列号的代码主要逻辑是，如果上个生成ID的时间位与当前ID的时间位冲突，则会生成一个序列号进行区分，如果序列号用尽，则等待下一个时间点再生成。如果上个生成ID的时间位与当前ID的时间位不冲突，则将序列号设置成0。

sequence = (sequence + 1) & sequenceMask，序列号最大值sequenceMask为4095，等价于如下这种写法。

ini

复制代码

`sequence = (sequence + 1); if(sequence == 4095){     sequence = 0; }`

其实这两种写法的结果是一致的，就是对(sequence + 1)进行取余。  
这里有个位运算知识点 k % m = k & (m - 1),m需要满足 m = 2^n,sequenceMask = 2^12 - 1。所以刚好可以用与运算进行取余操作，效率杠杠滴。

#### 3.3.4 生成ID

见代码中注释 序号5:  
此时我们拿到了时间位(timestamp - from)、机器位(instanceId )、序列号位(sequence),所以就可以计算最终的ID了。

scss

`((timestamp - from) << timestampLeftShift)  // (当前时间 - 起始时间) 向左移位 | (instanceId << instanceIdShift)  // 机器位 向左移位 | sequence; // 序列位`

①((timestamp - from) << timestampLeftShift) 计算时间位  
from是固定的1422720000000, timestampLeftShift = 12 + 10.我们假设timestamp = 1422720000001。也就是from刚刚过去1毫秒。1毫秒也是我们时间位倒数第二小的值，因为0是最小值。时间位取值范围[0, 2^41 - 1],从这也可以看出上边描述时间位时为什么把时间段特意标注了，因为时间位存的不是具体时间，而是以from为起始来算的过去了多少时间。

来看下 1<<22 结果

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4ecd7a06c129485d9281ccbc9eefddff~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

图2: 时间位移位结果

图2可以看出，时间位向左移位22，位置正好到第一个时间位。

②(instanceId << instanceIdShift) 计算机器位  
为了方便计算，这里我们假设instanceId 等于1,机器位取值范围[0,-1]。  
那么机器位就是 1 << 12

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/409d90edd13041d8ab2ea8ee4dab2745~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

图3: 机器位移位结果

图3可以看出，机器位左移12位，位置正好到第一个机器位。

③按照 ① | ② | sequence 进行或运算进行生成ID

现在我们有了时间位的值，机器位的值，就只差序列号位的值，序列号是上面3描述代码生成的，范围是[0, 2^12-1]。为了方便计算，我们假设sequence = 1

那么 ID = ① | ② | 1。进行或运算

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0d224e49bda643afb4446d15b7cf3f14~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

图4: ID = ① | ② | 1

下图是按照上面逻辑生成的ID

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ca1dd0904a2e4faea8392244e3bfce45~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

图5: 程序生成结果

#### 3.3.5 注意:雪花算法需要用单例方式生成ID

因为雪花算法会依赖上一次生成的ID的时间来判断是否需要对序列号进行增加的操作，如果不是单例，两个业务用两个对象同时获取ID，则可能会生成相同的ID

## 4 关于雪花算法的一些思考

### 机器位怎么取值

- 主机唯一标识 如果运维平台有机器唯一标识，可以在运维平台取。不过需要考虑机器位能否容纳下唯一标识，可能会过长，也需要考虑运维平台的唯一标识未来变化。
- 可根据ip进行计算 如果能保证不同机房的机器ip不重复，可以利用ip来计算机器位，IP最大 255.255.255.255。而（255+255+255+255) < 1024，因此采用IP段数值相加即可生成机器位，不受IP位限制。不过这种方式也不是绝对ok，要根据自身情况在选择，比如10.0.5.2 与 10.0.2.5 计算出来也是相同的。使用这种IP生成机器位的方法,必须保证IP段相加不能重复
- 通过数据库/redis/zk等进行协调，在应用启动的时候给每个机器分配不会重复的机器位id。

### 时钟回拨问题

雪花算法强依赖时间，如果时间发生回拨，有可能会生成重复的ID，在我们上面的nextId中我们用当前时间和上一次的时间进行判断，如果当前时间小于上一次的时间那么肯定是发生了回拨，雪花算法的做法是简单的抛出了一个异常。

arduino

复制代码

`if (timestamp < lastTimestamp) {    throw new RuntimeException(String.format("Clock moved backwards.  Refusing to generate id for %d milliseconds", lastTimestamp - timestamp)); }`

如果业务的异常容忍度低，这里我们可以对其进行优化,如果时间回拨时间较短，比如配置5ms以内，那么可以直接等待一定的时间，让机器的时间追上来。也可以利用扩展位，将64-bit的机器位或者序列号位预留出2-bit的防止时钟回滚的扩展位。

## 5 ID逆运算

如果线上出现ID重复，如何进行问题定位？对ID进行逆运算拿到ID的时间位、机器位、序号位。就可以进行下一步分析了。以上述生成的4198401为例

### 5.1时间

时间位 = ID / 2^(机器位 + 序列号位) + from  
时间位 = 4198401 / 2^(12 + 10) + 1422720000000 = 1422720000001  
与上述生成ID时用时间位相符  
注意:ID / 2^(机器位 + 序列号位) 是整数

### 5.2机器

机器位 = (ID / 2^序列号位 ) % 2^(机器位)  
机器位 = (4198401 / 2^12) % 2^10= (1025) % 1024 = 1  
与上述生成ID时用机器位数值相符

### 5.3序列号

ID % 2^序列号位  
序列号 = 4198401 % = 4198401 % 1024 = 1  
与上述生成ID时用的序列号数值相符

# 有赞订单管理的三生三世与“十面埋伏”

![](https://tech.youzan.com/trade_manage/)

有赞订单管理主要承接有赞所有订单搜索及详情展示功能，系统随着业务的不断发展经历了多次飞升之路。下面简单介绍下有赞订单管理系统的三生三世与“十面埋伏”。

# 第一世：凡人飞升小仙之路-分库分表

随着业务发展，单库单表所能承载的数据量局限性越发严重。  
**历劫**：单库单表数据量承载局限  
**渡劫**：分库分表  
分库分表的维度针对系统买卖家查询的需求，分片键为买家id和店铺id，其余订单扩展信息表属于数据组装信息，均以店铺id为分片键。  
**结果**：分库分表后，数据业务量的承载质的提升。

# 第二世：小仙飞升上仙之路-引入ES搜索引擎

随着业务搜索维度的不断添加，使得跨表查询需求越来越多，系统的慢查不断报出，为此引入了ES搜索引擎。  
**历劫**：跨表查询越来越多，系统慢查频频报出![跨表搜索越来越多](https://tech.youzan.com/content/images/2017/04/---------1.png)**渡劫**：引入ES搜索引擎  
ElasticSearch是一个基于Lucene的搜索服务器,这里主要是通过将订单主表及辅表的索引字段同步到ES里，这些每张表单独的索引字段，汇总成一个联合索引，来实现多个表的跨表搜索。  
**结果**：目前运行良好，统计的检索需求响应时间均值20ms以内，对于命中缓存的在1ms以内返回。由于多表联查的流量都进了ES,所以系统慢查被清0。

_两个问题需要注意下_：  
1. **单Type与多Type的选择**  
ES里使用文档存储，一个Index可以理解为一个库，一个Type可以理解为一张表，那么订单及其扩展信息面临使用单Type还是多Type的抉择。  
_多Type优点_： 可以做到索引字段与表结构一一对应, 数据同步隔离单一，直观简单。  
_多Type缺点_：获取数据时候需要一次数据聚合，比如一次跨5张表索引联查，需要先分别取出5张表的数据，然后做一次交集。性能会有影响。类似于DB的跨表联查，而且当数据做冷热隔离，数据拆分时候，多Type的拆分和维护成本反而更高。  
_单Type优点_：可以做到数据一次请求即可将目标信息全量返回，一个Type的数据拆分冷热隔离维护成本可控。  
_单Type缺点_：数据同步成本高一些，要做好数据聚合一致性问题。 结合业务需求场景，这里采用的单Type方案。如下图所示。

![](https://tech.youzan.com/content/images/2017/04/----.png)

2.**索引字段数量控制**  
由于订单及其扩展信息字段较多，将这些信息全量同步到ES会导致索引字段过多，索引文件也会随之过大，检索效率会受到影响。所以这里采用了将订单及其扩展信息中强搜索需求的索引字段同步了进来，并没有做全量所有字段同步。

# 第三世：上仙飞升上神之路-引入Hbase

随着业务的不断发展，对系统性能的要求的不断提高，我们发现虽然数据检索的效率很高，但是数据组装的效率令人担忧，由于需要从ES中获取的订单号列表到各个扩展表去获取具体信息，也就是一次完整的订单列表拉取时间=数据检索时间+数据组装时间，而数据组装就算是批量获取，也要去取N（假如有N张订单扩展表）次，即使并行去取也不够高效，上文讨论过没有将订单的所有信息全部同步到ES的原因，这样一次完整的订单拉取时间=数据检索时间。  
**历劫**：数据组装效率低下![](https://tech.youzan.com/content/images/2017/04/trade_detail.png)**渡劫**：引入Hbase来为详情数据组装 Hbase是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统。可以通过MapReduce来处理HBase中的海量数据。  
这里将订单基本信息及其强依赖的扩展信息全量导入Hbase，其中历史数据采用BulkLoad导入，增量数据采用消息同步写入，以订单号为rowKey为订单号，这样通过一次请求，将该订单的基本信息及扩展信息一次性取出。![](https://tech.youzan.com/content/images/2017/04/hbase.png)**结果**：订单管理架构被抽象成了DB+ES+HBASE的三层架构(如下图所示)，DB作为数据写入源头，ES负责搜索请求的parser解析及基本数据返回（即Es返回字段即可满足需求的场景），而Hbase作为订单详情详细信息的组装载体，两者配合提升整个订单列表搜索和详情组装的性能。同时Hbase的数据源也可以被复用到订单导出，数据统计等离线需求上。

![](https://tech.youzan.com/content/images/2017/04/------.png)

写到这里可能很多朋友都看出，实现这一切还有一个非常重要的劫需要去渡。那就是数据同步的实时性和一致性。感兴趣的话后续文章可以重点写写数据同步以及踩过的坑和破局之道，这里简单抛砖引玉下。

**历劫**：数据同步的实时性与一致性  
**渡劫**：链路白盒+幂等性保障  
1. 链路白盒：整个同步链路是先监听binlog然后同步到消息队列中，业务消费处理同步到Es和Hbase，可以将整个同步链路监控起来，比如一个消息binlogTime->addMqTime->processTime->addEsOrHbaseTime,这个差值其实就是实时性的一个指标。一旦整个同步链路的白盒搭建好，那么对应的报警监控，失败重试补偿就都可以跟进配套完成。保证数据的完整性与实时性。同步链路及同步监控如下图所示。  
![](https://tech.youzan.com/content/images/2017/04/-------1.png)![](https://tech.youzan.com/content/images/2017/04/-----1.png)

2.幂等性保障：如果不能保证业务消费的幂等性，那么消息的乱序，数据的重放监控补偿等等就会很被动。这里简单提供几种幂等思路：

- 业务乐观锁字段：比如订单状态的流转，应该是一个正向更新，即后一次更新的state一定大于等于前一次。
- version字段：表设计时候留一个version字段，每次数据库更新都会将该字段加1，作为乐观锁依据。
- sonwflake算法：可以根据业务需要定制自己的snowflake算法，比如毫秒级时间戳+binlog变更自增号
- 消息有序：对于一些强依赖消息有序或者业务乐观锁不好设定时候，消息端的有序变得尤为重要，可以给根据业务唯一键（如订单号）进行机器取模，保证同一笔订单的变更数据会被推送到同一台机器上消费。 其中业务乐观锁使用简单高效，无需额外存储乐观锁字段，而消息强有序每次需要使用取模计算，性能多少会有些影响，不过经过压测数据显示性能差别不大，这边采用业务乐观锁+消息有序共用的方案。目前线上运行良好。


  # Reference
  https://zhuanlan.zhihu.com/p/371301650
  https://toutiao.io/posts/b16j0ld/preview
https://www.bilibili.com/video/BV1dV41137qg/?spm_id_from=333.999.0.0&vd_source=cddcb4c3c25d4c9f77e78fb09f61a3c0

https://dbaplus.cn/news-73-5316-1.html
https://dbaplus.cn/news-160-5213-1.html
https://dbaplus.cn/news-141-4876-1.html
https://juejin.cn/post/7106059934239834143

https://www.51cto.com/article/756994.html
https://cloud.tencent.com/developer/article/1927251
https://juejin.cn/post/7259411780375576634#heading-4

https://tech.youzan.com/trade_manage/
https://mp.weixin.qq.com/s/dURbeBAJtWICNjBWStxp5g
https://mp.weixin.qq.com/s/QOSzP7f2Wc8tozr3u7t-Fg

