# 压测确定系统指标
目前实际情况是：
1、根据商品id返回商品详情 ab -n5000 -c300 qps500，tp99 2000ms.
2、根据skuIds获取sku信息 ab -n5000 -c300 qps480，tp99只有1678ms.
3、根据productIds获取商品信息 ab -n5000 -c300 qps828，tp99只有1050ms.  
4、根据属性值获取商品列表接口 qps128,tp99:7596
5、查询单个订单详情 -n5000 -c500  qps1240 tp99 456ms
6、批量新增购物车 -n5000 -c500 qps355 tp99 4740
7、根据用户id查询用户购物车列表 qps1403 tp99 423
8、 批量查询订单商品扩展信息 qps1492 tp99 364
9、批量删除购物车 qps1393 tp99 396
10、根据payOrderId查询订单 qps1287 tp99 449
11、用户订单列表 qps 563 tp99 990
12、下单接口 -n1000 -c100 tps 70 tp99 2189
-n5000 -c500 tps72 tp99 11581
通过这些压测情况，我们可以清晰知道系统的限流目标和 超时tp99目标。

订单下单tps600,tp99 700ms, 商品查询 tps1000,tp99 500ms
# 注册中心

🔍 服务注册与发现机制的基本模型是怎样的？
![[Pasted image 20240221145303.png]]
1. 生产者对外暴露服务，即将定位信息和元数据信息注册到注册中心中；
2. 消费者订阅注册中心的服务信息；
3. 在生产者服务上下线后会更新注册中心的信息，此时注册中心将变化信息主动通知消费者；
4. 消费者根据注册信息向生产者发起调用。

我们使用k8s 的service作为注册中心，所以cap中是按照etcd raft协议的话是cp模型。

🔍 服务上线与服务下线的步骤是什么？
服务上线应该是 完整启动好项目，然后先注册中心注册，注册中心推送给消费者。
服务下线应该是先取消注册中心的数据，然后等待请求处理完或者超时，然后再关闭服务。另外取消服务之后，要通知消费者。

🔍 注册中心选型需要考虑哪些因素？
cap。

🔍 你为什么使用 Zookeeper/Nacos/etcd 作为你的注册中心？
一致性协议出色。

🔍 在服务注册与发现里面你觉得应该用 AP 还是 CP？
看业务，一般是ap比较好。但是其实普通业务区别不大的。ap就算不一致了也可以重试。兼容性比较好。

🔍 如何保证服务注册与发现的高可用？
集群方案，心跳方案，raft方案解决脑裂或者数据不一致（最少同步到大多数节点，并且跟从节点的延迟不小于某个值。）

🔍 服务器崩溃，如何检测？
心跳
🔍 客户端容错的措施有哪些？
重试，及时跟注册中心校验数据，合理负载均衡。

🔍 注册中心崩溃了怎么办？
使用缓存，集群方案
🔍 注册中心怎么判断服务端已经崩溃了？
心跳

# 负载均衡：调用结果、缓存机制怎么影响负载均衡的？

⚖️ 静态负载均衡算法和动态负载均衡算法的核心区别是什么？
二、动态算法
动态算法有:最少连接数,最快响应速度，观察方法，预测法，动态性能分配，动态服务器补充，服务质量，服务类型，规则模式

最少连接数（Least Connection）：传递新的连接给那些进行最少连接处理的服务器。当其中某个服务器发生第二到第7 层的故障，BIG-IP 就把其从服务器队列中拿出，不参加下一次的用户请求的分配, 直到其恢复正常。
最快响应速度（Fastest）：传递连接给那些响应最快的服务器。当其中某个服务器发生第二到第7 层的故障，BIG-IP 就把其从服务器队列中拿出，不参加下一次的用户请求的分配，直到其恢复正常。
观察方法(模式)（Observed）：连接数目和响应时间以这两项的最佳平衡为依据为新的请求选择服务器。当其中某个服务器发生第二到第7 层的故障，BIG-IP就把其从服务器队列中拿出，不参加下一次的用户请求的分配，直到其恢复正常。
预测模式（Predictive）：BIG-IP利用收集到的服务器当前的性能指标，进行预测分析，选择一台服务器在下一个时间片内，其性能将达到最佳的服务器相应用户的请求。(被BIG-IP 进行检测) 。
动态性能分配(Dynamic Ratio-APM):BIG-IP 收集到的应用程序和应用服务器的各项性能参数，动态调整流量分配。
动态服务器补充(Dynamic Server Act.):当主服务器群中因故障导致数量减少时，动态地将备份服务器补充至主服务器群。
服务质量(QoS）:按不同的优先级对数据流进行分配。
服务类型(ToS): 按不同的服务类型（在Type of Field中标识）负载均衡对数据流进行分配。
规则模式：针对不同的数据流设置导向规则，用户可自行。
三、静态算法
静态算法有:轮询，比率，优先权 (加权)，随机

轮询（Round Robin）：顺序循环将请求一次顺序循环地连接每个服务器。当其中某个服务器发生第二到第7 层的故障，BIG-IP 就把其从顺序循环队列中拿出，不参加下一次的轮询，直到其恢复正常。
比率（Ratio）：给每个服务器分配一个加权值为比例，根椐这个比例，把用户的请求分配到每个服务器。当其中某个服务器发生第二到第7 层的故障，BIG-IP 就把其从服务器队列中拿出，不参加下一次的用户请求的分配, 直到其恢复正常。
优先权（Priority）：给所有服务器分组,给每个组定义优先权，BIG-IP 用户的请求，分配给优先级最高的服务器组（在同一组内，采用轮询或比率算法，分配用户的请求）；当最高优先级中所有服务器出现故障，BIG-IP 才将请求送给次优先级的服务器组。这种方式，实际为用户提供一种热备份的方式。
随机(random)：随机分配服务器处理，简单，但不可控。
 
⚖️ 轮询与随机负载均衡算法有什么区别？
不加权重的话没什么差比，只是结果没那么容易预测而已。
但是加权重轮询的话，某个节点权重过大可能会饿死其他节点。权重随机的话相对饿死几率小些。

⚖️ 你了解平滑的加权轮询算法吗？
就是如果某台服务器的权重很大的时候，那么在一段时间内就会一直击中它，服务器压力也很大，我们希望将请求尽可能的打散会好一点，所以就提出了平滑加权轮询算法。

![[Pasted image 20240221154854.png]]


⚖️ 如何根据调用结果来调整负载均衡效果？
每个被调用方上传自己的数据到同一个地方，然后每次发起调用前先通过这个地方来决策出需要调用的ip

⚖️ 为什么有些算法要动态调整节点的权重？权重究竟代表了什么？

⚖️ 你们公司的算法有没有调整过权重？为什么？

⚖️ 最快响应时间负载均衡算法有什么缺点？
耗费性能，而且很难确定合适的采样率以及采样周期。

⚖️ 如果我现在有一个应用，对内存和 CPU 都非常敏感，你可以针对这个特性设计一个负载均衡算法吗？
请求合并
⚖️ 为什么使用轮询、加权轮询、随机之类的负载均衡算法，系统始终会出现偶发性的流量不均衡，以至于某台服务器出故障的情况？怎么解决这一类问题？
限流，熔断。请求合并，动态调整权重（慎重）
# 单元测试以及代码分析

## 单元的mock
sqlMock可以模拟数据库，在这个过程中你可以传入sql然后对sql分析后返回特定的结果。
gomock工具可以对接口生成代码后进行代理，可以对特定入参生成特定回参。

## 单测覆盖率
首先我们使用 go test 生成覆盖率输出文件 cover.out ，并通过 gocov 工具来将生成的覆盖率文件 cover.out 转换成可以被 sonar 识别的 Cobertura 格式  
的 xml 文件。  
将生成的单测覆盖率报告发送到 sonar 平台上来展示。

一般来说我们要求单测覆盖率不低于80%

## 静态代码分析
在需要进行静态代码扫描的目录下执行 golangci-lint run ，此命令和 golangci-lint run ./… 命令等效，表示扫描整个项目文件代码，并进行监测，也  
可以通过指定 go 文件或者文件目录名来对特定的代码文件或者目录进行代码扫描，例如 golangci-lint run dir1 dir2/... dir3/file1.go 。

# 限流
🏁 限流算法都包括哪些？
漏桶，令牌，bbr，滑动窗口。
漏桶的缺点是无法最大化利用系统性能，优点是恒定的速率
并且漏桶中所有的请求都需要等待，而且请求太多超出漏桶就会直接拒绝。
令牌的优点是令牌用完的时候是恒定的速率，没用完的时候可以加快系统的qps。
令牌桶中获取不到令牌的也是需要等待一段时间。

滑动窗口的确定是如果在窗口初期就大量请求，后续一段时间都系统不可用了。
bbr就是可以通过系统的反馈来调整流量，比较不受到固定参数的影响，没那么死板需要总是调整。

🏁 不同的限流算法怎么选？
互相补充，比如对于商品系统的某些接口，可以先全局限流采用令牌桶的做法使用压测的结果值，防止后续的bbr公式预测不准，因为有可能cpu没有达到80%但是数据库已经扛不住了。
bbr用于单个服务cpu的控制，以及应对突发流量。

🏁 限流的对象应该如何选择？

🏁 怎么确定流量的阈值？

🏁 如何应对突发流量？

🏁 被限流的请求会被怎么处理？
系统繁忙，稍后重试。
🏁 为什么使用了限流，系统还是有可能崩溃？

🏁 我们有一个功能，对于普通用户来说，一些接口需要限制在每分钟不超过 10 次，整天不能超过 1000 次；VIP 用户不限制。你怎么解决这个问题？
通过redis设置两个zset用来限流，但是vip用户不设置redis就行。
## kong 根据ip限流

## 关键路径全局限流
根据redis全局限流，限流的标准是压测的结果，例如订单是600tps，商品列表是1000qps
## 普通路径本地限流
bbr和自适应熔断的解决的痛点都是频繁开关，以及秒的维度太过粗放，搞得像个阀门一样不行。
缺点是公式的判断容易误判，导致无法及时解决下游的问题。


bbr和自适应都是根据多少秒内有多少个窗口这种参数来弄滑动窗口进行 公式数据的生成。
一般来说我们是5s,50个窗口，也就是200ms一个桶。
至于为什么是5s,50个窗口，主要是就是为了维护200ms一个桶，200ms是很多db和redis和es要返回的一个范围，也是一个最小的原子单位。
bbr和熔断都是根据滑动窗口的滚动来进行公式的计算来进行自适应的

漏斗桶/令牌桶确实能够保护系统不被拖垮, 但不管漏斗桶还是令牌桶, 其防护思路都是设定一个指标, 当超过该指标后就阻止或减少流量的继续进入，当系统负载降低到某一水平后则恢复流量的进入。但其通常都是被动的，其实际效果取决于限流阈值设置是否合理，但往往设置合理不是一件容易的事情.

项目日常维护中, 经常能够看到某某同学在群里说:xx系统429了, 然后经过一番查找后发现是一波突然的活动流量, 只能申请再新增几台机器. 过了几天 OP 发现该集群的流量达不到预期又下掉了几台机器, 然后又开始一轮新的循环.

这里先不讨论集群自动伸缩的问题. 这里提出一些问题

1. 集群增加机器或者减少机器限流阈值是否要重新设置?
2. 设置限流阈值的依据是什么?
3. 人力运维成本是否过高?
4. 当调用方反馈429时, 这个时候重新设置限流, 其实流量高峰已经过了重新评估限流是否有意义?

这些其实都是采用漏斗桶/令牌桶的缺点, 总体来说就是太被动, 不能快速适应流量变化

TCP BBR 拥塞控制算法 的思想给了我们一个很大的启发。我们应该根据系统能够处理的请求，和允许进来的请求，来做平衡，而不是根据一个间接的指标（系统 load）来做限流。最终我们追求的目标是 在系统不被拖垮的情况下，提高系统的吞吐率，而不是 load 一定要到低于某个阈值。如果我们还是按照固有的思维，超过特定的 load 就禁止流量进入，系统 load 恢复就放开流量，这样做的结果是无论我们怎么调参数，调比例，都是按照果来调节因，都无法取得良好的效果。

kratos 自适应限流分析  
限流公式  
(cpu > 800 OR (Now - PrevDrop) < 1s) AND (MaxPass * MinRt * windows / 1000) < InFlight  
为true代表要限流
采集cpu用的shirou/gopsutil/cpu 这个库来采集cgroup中的cpu
cpu > 800  表示 CPU 负载大于 80% 进入限流  
(Now - PrevDrop) < 1s  这个表示只要触发过 1 次限流，那么 1s 内都会去做限流的判定，这是为了避免反复出现限流恢复导致请求时间和系统负载产生大量毛刺  
(MaxPass * MinRt * windows / 1000) < InFlight  判断当前负载是否大于最大负载  
InFlight  表示当前系统中有多少请求  
(MaxPass * MinRt * windows / 1000)  表示过去一段时间的最大负载  
MaxPass  表示最近 5s 内，单个采样窗口中最大的请求数  
MinRt  表示最近 5s 内，单个采样窗口中最小的响应时间  
windows  表示一秒内采样窗口的数量，默认配置中是 5s 50 个采样，那么 windows 的值为 10。

一句话描述就是，只有在cpu大于800的时候才会去判断是否限流，如果没有达到，再判断下是否1s内限流过。如果两者都没有达到，就不限流。但是如果有一者为true，那么就获取所有桶中最大的通过数和最小rt，相乘后，乘以一秒内拥有的桶数，再除以1000ms，获取ms的理想请求数是否小于当前请求数，如果是，说明不理想要限流。
[![Go可用性 限流-分布式限流-kratos 限流策略.png](https://img.lailin.xyz/image/1618158541909-f94f46a3-fad8-47d3-a389-395a57116c55.png)
### kratos 中间件实现

```golang
func (b *RateLimiter) Limit() HandlerFunc {
	return func(c *Context) {
		uri := fmt.Sprintf("%s://%s%s", c.Request.URL.Scheme, c.Request.Host, c.Request.URL.Path)
		limiter := b.group.Get(uri)
		done, err := limiter.Allow(c)
		if err != nil {
			_metricServerBBR.Inc(uri, c.Request.Method)
			c.JSON(nil, err)
			c.Abort()
			return
		}
		defer func() {
			done(limit.DoneInfo{Op: limit.Success})
			b.printStats(uri, limiter)
		}()
		c.Next()
	}
}
```

使用方式

```golang
e := bm.DefaultServer(nil)
limiter := bm.NewRateLimiter(nil)
e.Use(limiter.Limit())
e.GET("/api", myHandler)
```

### 源码实现

#### Allow

```golang
func (l *BBR) Allow(ctx context.Context, opts ...limit.AllowOption) (func(info limit.DoneInfo), error) {
	allowOpts := limit.DefaultAllowOpts()
	for _, opt := range opts {
		opt.Apply(&allowOpts)
	}
	if l.shouldDrop() { // 判断是否触发限流
		return nil, ecode.LimitExceed
	}
	atomic.AddInt64(&l.inFlight, 1) // 增加正在处理请求数
	stime := time.Since(initTime) // 记录请求到来的时间
	return func(do limit.DoneInfo) {
		rt := int64((time.Since(initTime) - stime) / time.Millisecond) // 请求处理成功的响应时长
		l.rtStat.Add(rt) // 增加rtStat响应耗时的统计
		atomic.AddInt64(&l.inFlight, -1) // 请求处理成功后, 减少正在处理的请求数
		switch do.Op {
		case limit.Success:
			l.passStat.Add(1) // 处理成功后增加成功处理请求数的统计
			return
		default:
			return
		}
	}, nil
}
```

#### shouldDrop

```golang
func (l *BBR) shouldDrop() bool {
	// 判断目前cpu的使用率是否达到设置的CPU的限制, 默认值800
	if l.cpu() < l.conf.CPUThreshold { 
		// 如果上一次舍弃请求的时间是0, 那么说明没有限流的需求, 直接返回
		prevDrop, _ := l.prevDrop.Load().(time.Duration)
		if prevDrop == 0 {
			return false
		}
		// 如果上一次请求的时间与当前的请求时间小于1s, 那么说明有限流的需求
		if time.Since(initTime)-prevDrop <= time.Second {
			if atomic.LoadInt32(&l.prevDropHit) == 0 {
				atomic.StoreInt32(&l.prevDropHit, 1)
			}
			// 增加正在处理的请求的数量
			inFlight := atomic.LoadInt64(&l.inFlight)
			// 判断正在处理的请求数是否达到系统的最大的请求数量
			return inFlight > 1 && inFlight > l.maxFlight()
		}
		// 清空当前的prevDrop
		l.prevDrop.Store(time.Duration(0))
		return false
	}
	// 增加正在处理的请求的数量
	inFlight := atomic.LoadInt64(&l.inFlight)
	// 判断正在处理的请求数是否达到系统的最大的请求数量
	drop := inFlight > 1 && inFlight > l.maxFlight()
	if drop {
		prevDrop, _ := l.prevDrop.Load().(time.Duration)
		// 如果判断达到了最大请求数量, 并且当前有限流需求
		if prevDrop != 0 {
			return drop
		}
		l.prevDrop.Store(time.Since(initTime))
	}
	return drop
}
```

#### maxFlight

该函数是核心函数. 其计算公式: MaxPass * MinRt * windows / 1000. maxPASS/minRT都是基于`metric.RollingCounter`来实现的, 限于篇幅原因这里就不再具体看其实现(想看的可以去看rolling_counter_test.go还是蛮容易理解的)

```golang
func (l *BBR) maxFlight() int64 {
	return int64(math.Floor(float64(l.maxPASS()*l.minRT()*l.winBucketPerSec)/1000.0 + 0.5))
}
```

- winBucketPerSec: 每秒内的采样数量,其计算方式:int64(time.Second)/(int64(conf.Window)/int64(conf.WinBucket)), conf.Window默认值10s, conf.WinBucket默认值100. 简化下公式: 1/(10/100) = 10, 所以每秒内的采样数就是10

```golang
// 单个采样窗口在一个采样周期中的最大的请求数, 默认的采样窗口是10s, 采样bucket数量100
func (l *BBR) maxPASS() int64 {
	rawMaxPass := atomic.LoadInt64(&l.rawMaxPASS)
	if rawMaxPass > 0 && l.passStat.Timespan() < 1 {
		return rawMaxPass
	}
	// 遍历100个采样bucket, 找到采样bucket中最大的请求数
	rawMaxPass = int64(l.passStat.Reduce(func(iterator metric.Iterator) float64 {
		var result = 1.0
		for i := 1; iterator.Next() && i < l.conf.WinBucket; i++ {
			bucket := iterator.Bucket()
			count := 0.0
			for _, p := range bucket.Points {
				count += p
			}
			result = math.Max(result, count)
		}
		return result
	}))
	if rawMaxPass == 0 {
		rawMaxPass = 1
	}
	atomic.StoreInt64(&l.rawMaxPASS, rawMaxPass)
	return rawMaxPass
}

// 单个采样窗口中最小的响应时间
func (l *BBR) minRT() int64 {
	rawMinRT := atomic.LoadInt64(&l.rawMinRt)
	if rawMinRT > 0 && l.rtStat.Timespan() < 1 {
		return rawMinRT
	}
	// 遍历100个采样bucket, 找到采样bucket中最小的响应时间
	rawMinRT = int64(math.Ceil(l.rtStat.Reduce(func(iterator metric.Iterator) float64 {
		var result = math.MaxFloat64
		for i := 1; iterator.Next() && i < l.conf.WinBucket; i++ {
			bucket := iterator.Bucket()
			if len(bucket.Points) == 0 {
				continue
			}
			total := 0.0
			for _, p := range bucket.Points {
				total += p
			}
			avg := total / float64(bucket.Count)
			result = math.Min(result, avg)
		}
		return result
	})))
	if rawMinRT <= 0 {
		rawMinRT = 1
	}
	atomic.StoreInt64(&l.rawMinRt, rawMinRT)
	return rawMinRT
}
```
# 熔断

🔥 为什么说熔断可以提高系统的可用性？

🔥 如何判断节点的健康状态，需要看哪些指标？
响应时间，异常状态持续时间

🔥 触发熔断之后，该熔断多久？

🔥 响应时间超过多少应该触发熔断？

🔥 响应时间超过阈值就一定要触发熔断吗？
异常状态持续一段时间
🔥 怎么避免偶发性超过阈值的情况？
异常状态持续一段时间
🔥 服务熔断后如何恢复？
半打开
🔥 产生抖动的原因，以及如何解决抖动问题？
选择的熔断标准不正常，例如设置的响应时间过小，导致系统请求稍微一上来就出问题。
打开和关闭的频率过高。第一个原因是上面的标准过严，第二个原因是太过容易就关闭熔断了，例如打开熔断后只要半打开过程中只要有一个请求成功就关闭熔断，但是后续的请求又是一大片有问题的又立刻打开熔断了。
## 简单的熔断
之前的文章，分析了 Hystrix-Go 熔断算法的实现，其算法核心是：当请求失败比率达到一定阈值之后，熔断器开启，并休眠一段时间（由配置决定），这段休眠期过后，熔断器将处于半开状态，在此状态下将试探性的放过一部分流量，如果这部分流量调用成功后，再次将熔断器关闭，否则熔断器继续保持开启并进入下一轮休眠周期。  
  
但这个熔断算法有一个问题，过于一刀切。是否可以做到在熔断器开启状态下（但是后端未 Shutdown）仍然可以放行少部分流量呢？当然，这里有个前提，需要看后端此时还能够接受多少流量。下一步我们来看看 Google 的策略实现。

什么时候判定服务需要触发熔断，为什么选用这个指标。  
  
假如说你准备用响应时间来作为指标，那么你可以这么回答，关键词是持续超过阈值。  
  
为了保障微服务的可用性，我在我的核心服务里面接入了熔断。针对不同的服务，我设计了不同的微服务熔断策略。  
  
比如说最简单的熔断策略就是根据响应时间来进行。当响应时间超过阈值一段时间之后就会触发熔断。我一般会根据业务情况来选择这个阈值，例如，如果产品经理要求响应时间是1s，那么我会把阈值设定在1.2s。如果响应时间超过1.2s，并且持续三十秒，就会触发熔断。在触发熔断的情况下，新请求会被拒绝，而已有的请求还是会被继续处理，直到服务恢复正常。

关闭(closed): 关闭状态下没有触发断路保护，所有的请求都正常通行  
打开(open): 当错误阈值触发之后，就进入开启状态，这个时候所有的流量都会被节流，不运行通行  
半打开(half-open): 处于打开状态一段时间之后，会尝试尝试放行一个流量来探测当前 server 端是否可以接收新流量，如果这个没有问题就会进入关闭状态，如果有问题又会回到打开状态
## google sreBreaker

我们打算使用的方法：
```
max(0, (requests-k*accepts)/(requests+1) )
```

算法如上所示，这个公式计算的是请求被丢弃的概率[3]  
  
requests: 一段时间的请求数量  
accepts: 成功的请求数量  
K: 倍率，K 越小表示越激进，越小表示越容易被丢弃请求  



例如k=1.1 就说明十个请求中只要有一个请求出问题，就出现概率熔断,也就是十个请求中成功的请求小于9就出问题。

k=1.6,就说明十个请求中，成功的请求小于6个，就出现概率熔断

  

这里面要配合滑动窗口，比如 三秒内分十个窗口就行，当熔断器开启，每过一个窗口就有概率通过请求，知道成功数上升。每个窗口都负责存储count总数，accepts总数。所以最后的总数和成功数就是所有窗口的总数相加，acceppts总数相加。

# 超时控制


🕙 为什么要做超时控制？
1、确保客户端能在预期时间内拿到响应  
2、及时释放资源  
🕙 为什么缺乏超时控制有可能引起连接泄露、线程泄露？

🕙 什么是链路超时控制？

🕙 如何确定超时时间？
确定超时时间的方法：1、产品经理   
2、tp99,根据tp99 冗余一点点  
3、区分重试和难以重试的接口
🕙 怎么在链路中传递超时时间？
context
🕙 超时时间传递的是什么？

🕙 如何计算网络传输时间？

🕙 什么是时钟同步问题？

🕙 客户端和服务端谁来监听超时？

🕙 超时之后能不能中断业务？怎么中断？

# 降级
🤚 什么时候会用到降级，请举例说明？

🤚 降级有什么好处？

🤚 跨服务降级常见的做法是什么？

🤚 你怎么评估业务服务的重要性？或者说，你怎么知道 A 服务比 B 服务更加重要？

🤚 请说一说服务内部常见的降级思路。

🤚 怎么判断哪些服务需要降级？

🤚 触发降级之后，应该保持在降级状态多久？

🤚 服务降级之后如何恢复，如何保证恢复过程中不发生抖动？

🤚 你们公司的产品首页是如何保证高可用的？


## 通过监听nsq消息进行降级

## 通过配置文件降级
# 自适应扩缩容
## k8s hpa扩容


# 链路追踪
## 背景
2020 年，我们不断收到业务研发的反馈：能不能全量采集 trace？这促使我们开始重新思考如何改进调用链追踪系统。我们做了一个简单的容量预估：目前 Jaeger 每天写入 ES 的数据量接近 100GB/天，如果要全量采集 trace 数据，保守假设平均每个 HTTP API 服务的总 QPS 为 100，那么完整存下全量数据需要 10TB/天；乐观假设 100 名服务器研发每人每天查看 1 条 trace，每条 trace 的平均大小为 1KB，则整体信噪比千万分之一。可以看出，这件事情本身的 ROI 很低，考虑到未来业务会持续增长，存储这些数据的价值也会继续降低，因此全量采集的方案被放弃。退一步想：全量采集真的是本质需求吗？实际上并非如此，我们想要的其实是「有意思」的 trace 全采，「没意思」的 trace 不采。

## 需求

随着服务数量的日益增加，承接全量的 Tracing 数据所需的资源越来越多。如果全量采集，链路中产生的span 所占的存储成本将会很高。目前生产环境采用0.1% 的采集率，存储成本较低。这种策略虽然很节省资源，但其缺点在一次次线上问题排查中逐渐暴露：  
  
一个进程中包含多个接口：不论按固定概率采样还是限流采样，都会导致小流量接口一直采集不到调用链数据  
线上服务出错是小概率事件，导致出错的请求被采中的概率更小，就导致采到的调用链信息量不大，引发问题的调用链却丢失的问题
## 做法
Tail Sampling Processor  
为了解决这些问题，我们可以采取一种方法，即将所有生成的数据发送到一个中间平台，该平台会先进行暂存和清洗，然后再决定是否要保留或丢弃整个跟踪信息。具体操作如下图所示：当一个请求到达时，每个服务会将其生成的跟踪信息（Span）发送到一个名为OpenTelemetry Collector的中间组件。在Collector上有一个尾部采样组件，该组件会在接收到第一个Span后，等待一段时间（例如5秒），以便继续收集来自其他服务、具有相同Trace ID的Span。等待时间结束后，大量的Span会按照它们的Trace ID进行分类汇总，然后对属于同一个Trace ID的Span进行遍历，以检查是否包含错误信息，或者累计耗时是否超过了预设的阈值等。基于这些信息，可以有依据地筛选出高价值的跟踪信息，将它们纳入后续的处理流程。这种方法有助于更有效地处理跟踪数据，识别潜在问题，以及提高整体性能。  
  
这种采样的模式称为尾部采样，它由 Collector 依据完整 Trace 的信息进行决策，决策的依据比头部采样丰富很多。但是由于需要承载大量临时数据，所以对基础设施要求很高。它的效果在于：  
持久化的数据有依据、有规律、有价值；  
减少了展示有价值数据所需的成本，例如存储资源，并且对提高查询速度也有帮助。  

需要注意的是，在实际部署中，整个架构要做到高可用，往往会存在多个 Collector 节点，而同一个 Trace 的不同 Span 是由不同服务产生的，这些服务位于不同地方，尾部采样要求他们都落入相同的 Collector 节点，那么必然需要一层负载均衡架设在 Collector 之前，依照 Trace ID 进行转发。让 otel-agent 按照 traceID 做负载均衡，使用exporters 中的loadbalancing 组件。
## 架构
架构图如下：  
![[Pasted image 20240202120446.png]]
![[Pasted image 20240202122208.png]]
pod 通过sdk 的方式上报到otel-agent，agent 的receivers使用otel grpc的方式接收trace；processors 使用 batch 来批量处理；exporters 使用 loadbalancing 方式根据traceID 负载到下游同一个collector。  
  
otel-collector 由grpc 接收相同traceID的span，本地批量处理，基于这些信息根据 tail_sampling 配置规则决定是否上报到阿里云analysis平台。同时可配置不同的processor 处理不同数据导出到不同的平台。

最终的标准的是：
在调用链上error 级别日志  
整个调用链请求耗时超过 1s

常规链路每天只对每个接口采用一次，部分接口一天采样多次常规链路。
## 最终规模
最终规模：
otel-agent的每个pod使用内存在3g，cpu使用在1.2。生产环境启动8个pod，每分钟接受30万左右的span。

otel-collector的每个pod使用automaxproc内存是2g，cpu使用1c，生产环境启动8个pod。

## 结果
ES 索引体积下降超过 80%，目前每天索引体积为 10-20 GB；链路追踪整体的cpu消耗降低了50%。只存储一个月的数据。

这个过程体现的是 利用一个新框架的能力，以及获取资料的能力。

# 监控

# 灰度
## 灰度的用途
灰度环境的目的是控制影响的范围，只有部分客户充当金丝雀。重点是控制问题影响的范围

所以灰度可以作为预生产环境的补充，用来减小对上线的压力。毕竟上线之后可以把使用范围指定于某一个用户之类的。
数据库出错时灰度需要负责的吗？
不是，那是质量把控的问题，灰度不灰度都会出问题
## 目前系统流量的背景
### 管理系统
根据域名从ingress进去，然后多个同域名的ingress通过ingress的path来判断，然后打到gateway这个deployment，这个deployment是一个kong项目
### 业务系统
根据域名从ingress进去，然后多个同域名的ingress通过ingress的path来判断，然后打到gateway这个deployment，这个deployment是一个kong项目，最后通过xhost判断
## 考虑的做法（理论上的做法）

通过salesChannelId，用户id，流量来源，ip地址去一个流程引擎项目中判断使用的k8s service，并且在请求头中设置是否开启灰度。所以开启这个插件就是代表要进行灰度了，关闭这个插件就没有灰度了

每个pod在进行远程调用的时候，如果判断到请求头中有开启灰度的标志，就代表需要去salesChannelId，用户id，流量来源，ip地址 发到流程引擎项目中获取下游地址service地址，这个动作由client包默认进行

# 异步/解耦

# 数据一致性检测和修复

## 微服务架构中数据一致性通用做法
### 调用方系统保证最终一致性
#### 核心思想
通过业务侧系统的服务保证数据的最终一致性，其核心思想就是业务侧系统记录下来每一次具体业务操作的执行流水日志信息，并且对没有全部成功的变更结  
果，触发执行数据一致性的校验核对工作。
#### 设计原则  
平台侧系统服务，提供支持执行业务操作的基础服务能力的接口。

特别强调一点，这里是需要根据业务操作标识和业务操作唯一ID来实现接口的幂等设计。
为什么我们有了唯一ID，同时还是需要有业务操作标识？
因为在实际的生产实践中，在各种内因和外因的背景下，需要兼顾系统的稳定性和业务迭代的灵活性，很难做到绝对的全局性唯一ID的生成。更多时候，只需要在某个业务侧系统的内部，保证全局唯一性即可，这也是符合实际情况的系统设计。

类似的解决问题的思路，在其他的系统设计场景，也是有非常高的借鉴价值的。  
平台侧系统服务，提供执行业务操作后的结果查询接口，支持根据业务操作标识和业务操作的唯一性ID查询能力。  
业务操作记录表，支持记录和识别业务操作的标识和每次执行的唯一ID。  
业务侧系统服务，触发对业务操作记录表的数据一致性的检查核对工作，执行核对的方式，比如实时的同步检查核对、准实时的异步检查核对、定时任务的异步检查核对等等，为了保证自己和平台侧系统的数据最终一致性。

#### 状态存储设计
在一般情况下，建议把MySQL存储当做我们首选的存储，MySQL提供非常完善的数据一致性保证能力，最简单的方式是基于数据库的联合唯一索引设计，多次  
层Tag + 唯一ID的业务唯一键。但是也是有缺陷的，比如MySQL自身的性能瓶颈和昂贵的存储成本。性能上的瓶颈，可以通过访问MySQL的幂等校验之前，增  
加访问Redis的幂等校验，校验不通过抛出异常，在MySQL幂等校验通过以后，异步刷数据到Redis中，这样保证Redis校验通过的同时MySQL校验一定是通过  
的。
我们可以接受Redis的幂等校验的不准确性，仅仅是期望它成为流量漏斗的上层，为MySQL承担起流量过滤作用，当然你可以有其他的更多的方案来做这件事，甚至组合起来使用。也可以增加分库分表的策略，来解决MySQL的性能瓶颈。在MySQL的存储成本是相对比较高的，我们可以对历史的数据做归档处理，只保留一部分的热数据，原则上保持单表的数据行数在500w~1000w之间，同时也可以有能支持一定量的历史数量查询。同时这个过程也需要考虑无锁处理问题和MySQL空间碎片的问题等等。


# other
常见问题  
  
问题：SLA应该按照哪个维度去定义？接口、应用、业务？  
  
答：都可以，只要讲清楚是接口SLA，还是应用SLA，还是业务SLA就可以。但注意：提到应用SLA，应该等于核心接口的最差SLA；提到业务SLA应该等于黄金链路的最差SLA。  
  
问题：SLA时间计算周期应该多少？  
  
答：都可以，主要讲清楚计算周期就可以，一般以年为单位更具代表性。