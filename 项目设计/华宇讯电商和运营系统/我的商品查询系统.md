# 整体系统参数
商品数实际774782  ，七十多万
sku数实际1639630，一百六十多万
但是有一千多个站点，所有要 一百六十多万 乘 1000 =  十六亿。当然最终肯定没有这么夸张的数据量。

4库32个表的话也就是每个表一千多万最多。所以这里面还是没有说出来到底现在有多少数据量。

所以我用的redis还是单机的吗还是集群的？目前是永不过期放到16g单机redis中只占16，但是我预估的这个数据量太大了。

# 商品系统表设计
![[Pasted image 20240219103545.png]]

# CQRS设计



# 消息解耦和异步
## 消息推送架构

### nsq的producer推送

推荐拓扑，每个producer的pod都要内置一个nsqd，然后每个producer发到自己的nsqd，每个nsqd都去nsqlookup注册

用nsq balancer开源工具去轮询，给他每个地址以及权重，这个工具自己去发送。

为什么不用这个推荐拓扑，因为我们是交给运维去管理nsq集群的，而且跟服务放到一个pod中，容易加大pod的不稳定，我们习惯业务和组件分离，不需要关心这些东西。

# 调用具体流程
![[Pasted image 20240204144703.png]]
所有的分页获取都必须是 带上过滤条件和偏移量做一个fromto，然后pageNum永远是1。只需要一开始获取总数就行，或者不获取总数，是一个瀑布流
![[Pasted image 20240204171811.png]]
  
以前使用的是dns+headless+grpc自己的负载均衡。（grpc连接池用于断开）

为什么不用dns了，因为每个用这个的人老是忘记加某个配置定时断开链接重新获取。
最开始我们打算使用的是不去理会的方案，因为一开始我们的qps不高，而且服务提供者比客户端多，所以一定程度上倾斜不严重。

后来客户端多了，我们就采用了kuberesolver解析器来监听apiServer的pod列表，用自定义的一致性hash picker选择器从ctx中获取key来进行连接。

在最终方案前，我们考虑了dns+headless+grpc自定义负载均衡+连接自动断开的策略，但是最终不予采用是因为不想提醒每个人使用断开的操作。

grpc根据请求来hash来路由到同一个，如果带了page的话就预先load下一个请求
使用singleFilght去批量更新。

grpc自定义平衡器，然后这个平衡器使用分布式一致性hash来保证路由到同一个pod,然后这个pod本地缓存这个请求的数据，并针对page预先load下一个请求。

# 商品es实践中遇到的问题
## 楼层模板商品的痛点

痛点在于，楼层模板绑定了商品id之后，是不知道每个商品现在的上下架状态的，那么每次都是要分页拉取的时候是有可能这一页是没有数据的，所以前端要一直拉，知道6003。那么现在的痛点是如果第一页数据不够怎么办？而且这种多次拉取其实效率也不好。  

前提是每个楼层不能超过两千个商品，商品不能露出的条件有上下架，黑名单，是否可售（渠道或者平台是否可售，毕竟商品是我们的商品库），是否开放

全部通过es来查询判断出来就行，不走redis位图了。（然后每次顺势加载后面几页到redis缓存起来。）

## 运营商挑选的问题
每个商品都会给一千多个站点挑选，甚至都要给每个站点中多个楼层挑选。

这里面有个非常强的1：n的关系。n>1000。

以前的方式是 一个商品文档下面通过nest的方式挂载多个站点和模板，但是后面发现，每个商品下面挂在一千多个站点信息导致单个doc太过庞大，单个文档最小15k，而且每次某个站点更新都要更新整个doc数据太过浪费了，而且非常耗费cpu,写入的数量以及查询的数量加起来并发量上百的话cpu就爆震，因为查询一查就是要遍历整个1000站点，随便改一点就要修改整个15k的数据。父子文档的方式更加耗费性能。所以现在把es文档以站点的角度摊开，文档数量急剧增加，算是空间换时间，毕竟磁盘比较便宜。

## opensearch方案
从openSearch获取到推荐商品或者搜索商品之后，一次性拉取到几百个商品id后，要立即根据商品id过滤出是否这个运营商的，是否这个站点的，是否对这个站点开放了。

