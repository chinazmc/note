将一条条数据转为批量，做法是使用时间窗口的business-data-clean

还有就是各种对es本来的优化


还有就是对kafka做了一个自定义的分配器逻辑，让数据可以分配到更多的地方处理


背景
现公司主营个性化定制商城
订单量已达一亿
因之前 ES 的配置问题，导致现在 ES 集群不够稳定。3 台 node 只用到 2 个

迁移
分了几个步骤
1. 新建索引 修改 mapping ，对其不需要查询的字段取消索引，将 text 类型改成 keyword 类型
2. 将老索引使用 alias 使用新的别名
3. 在代码处使用全匹配的地方，将 must 改成为 filter，二次查询的时候使用缓存，使用 新的别名索引
4. 先将代码发布，替换带老的真实别名
5. 停止写入 ES 使用 reindex 迁移节点
6. 分片数根据 每个分片大约占 20～30G 的内存
7. 然后使用 reindex 迁移数据
8. 迁移之后 ，在将副本分片从 0 改为 1
9. 后期使用 GO 服务 监听 binlog 致使结偶系统

结果

最直观 就是储存从 150G 降低到了 50G。

————————————————
原文作者：CrazyZard
转自链接：https://learnku.com/articles/59420
版权声明：著作权归作者所有。商业转载请联系作者获得授权，非商业转载请保留以上作者信息和原文链接。