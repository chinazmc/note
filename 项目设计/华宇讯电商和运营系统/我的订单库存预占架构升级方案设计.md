# 摘要
订单系统目前数据量一个亿多。
一千个请求，100的并发，tps到了70，tp99 只有2189ms

最终取得的结果是tps是600，tp99从2189ms到700ms+,取得了三倍多的缩小

 # 订单系统背景
日平均订单最少四万
一个运营商一天要有200单最少。我们有几十个运营商，有好几个商城，那么就是一千多个站点，例如1000 \*200。但是最终还是说一天平均四万。

我通过计算登陆表，
```sql
SELECT COUNT(*) FROM `user_login_history` where created_at BETWEEN "2023-09-25 01:00:00" and "2023-10-13 23:59:59"
```
感觉总共有554937条记录，总共有19天，所以日活应该是3万最少，我可以宣称日活五万最少

每一个分库分表的表都需要一个hash field，这个字段专门用于分库分表逻辑

最终决定 订单下单的tps是600的时候tp99是700ms+。
预测的瓶颈是 数据库锁竞争开始激烈，通过压测过程中的show innodb看到lock wait的上升 以及阿里云的**Innodb_row_lock_waits**和**Innodb_row_lock_time**监控项的指标升高监控判断。
但是暂时还不需要解决，如果需要解决，可以通过后面的改进方案来做。

# 购物车流程
现在购物车也有两千万的数据了。没有分库分表
## 购物车表结构
```sql
CREATE TABLE `shopping_cart` (  
  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT 'id',  
  `user_id` bigint(20) unsigned NOT NULL DEFAULT '0' COMMENT '客户_id',  
  `sku_id` int(10) unsigned NOT NULL DEFAULT '0' COMMENT 'sku _id',  
  `product_name` varchar(255) NOT NULL DEFAULT '' COMMENT '商品名字',  
  `image` varchar(255) NOT NULL DEFAULT '' COMMENT 'sku缩略图',  
  `sku_name` varchar(255) NOT NULL DEFAULT '' COMMENT 'sku名字',  
  `sku_count` int(10) unsigned NOT NULL DEFAULT '0' COMMENT 'sku 数量',  
  `price` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '商品加入购物车的价格',  
  `sales_channel_id` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '销售渠道',  
  `select_flag` tinyint(2) unsigned NOT NULL DEFAULT '0' COMMENT '商品勾选状态：0-不勾选,1-勾选',  
  `business_type` tinyint(2) unsigned NOT NULL DEFAULT '0' COMMENT '业务类型',  
  `business_code` varchar(100) NOT NULL DEFAULT '' COMMENT '商品业务编码',  
  `score` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '商品加入购物车的积分价格',  
  `relation_id` bigint(20) unsigned NOT NULL DEFAULT '0' COMMENT '关联商品_id/换购或赠送等',  
  `promotion_id` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '被采用的关联商品的优惠规则_id',  
  `created_at` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00' COMMENT '创建时间',  
  `updated_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间',  
  `status` tinyint(3) unsigned NOT NULL DEFAULT '0' COMMENT '状态',  
  PRIMARY KEY (`id`),  
  KEY `idx_user_id` (`user_id`),  
  KEY `idx_user_union_id` (`user_id`,`sales_channel_id`)  
) ENGINE=InnoDB AUTO_INCREMENT=27775011 DEFAULT CHARSET=utf8 COMMENT='购物车';
```

## 加入购物车  /api/cart/batch-add
第一步，构造购物车请求
```json
{  
    "addType": 0,  
    "skuList": [  
        {  
            "skuId": 162,  
            "count": 1  
        }  
    ],  
    "businessType": null  
}
```
第二步，加分布式锁来限制频率以及确保操作原子性。通过用户id以及一个traceId来保证正确加锁和解锁。用同一个userId作为key,traceId作为value，解锁的时候就通过lua判断下value是否同一个来解锁
第三步，根据操作类型进行不同操作，比如追加操作就是检查重复skuid。比如merge操作，就是对merge过来的skuid进行检查（无登录态的购物车添加，主要是前端把sku发过来判断和校验下，主要还是前端在存储）
第四步，检查添加的skuIds的长度不能大于某个值，比如50
第五步，检查商品有效性
第六步，检查购买限制
第七步，查询商品价格信息
第八步，查询用户购物车中已经存在的商品
第九步，购物车全部的数量不能超过100


## 查询购物车  /api/cart/cartlist/
1、获取用户购物车信息
2、根据购物车状态获取具体的购物车商品具体信息
3、根据查询过来的商品具体信息组装运费规则，购买限制，供应商信息
4、保留符合条件的商品
5、根据规则对商品进行分组

# 订单下单流程

## 订单系统的分布式id如何确定
在redis cluster中，可以根据key来选择hash slot进行incr操作。但目前是单节点redis 32g  
如果返回值大于2的18次方，就加分布式锁将这个key的值置为0。
中间的四位业务id的话是跟着订单主表走的，其他表或许是不同的业务id。前面的时间精确到毫秒。  
其他优点：
1、数据库主键插入比较顺序
2、数据库索引性能比雪花算法的bigint 8字节大，为27字节
3、数据库查询方便，通过id>'230801'就可以过滤出来大于23年八月一号的数据！！！！！！！！！
4、业务敏感，不会让人看出来多了多少订单。
5、可读性

## 为什么要用分布式事务
其实不用分布式事务，不代表没有分布式事务。所有的东西都是追求最终一致性的。只是这个最终一致性到底要在多大的时间窗口内达成。你用一个宽松的分布式事务做法，比如saga，对性能影响也是不大的。

为什么需要分布式事务？因为你不确定下游什么时候恢复正常，不确定调用者是不是挂了，不确定网络什么时候恢复

很多人都不用分布式事务，那么他们怎么做的？

我这边也可以说以前有个异常表，有错误类型，错误入参，错误接口，错误时间，是否已修复这些schema。然后有个risk或者timer的项目去定时轮询确认状态。现在有个dtm了。现在对于dtm后出现的问题，我们有个risk程序来定时比对

为什么你们不继续用异常表了呢，因为一开始简单的时候还好，后面订单的各种判断越做越复杂之后，异常情况越来越多，改的时候不注意的话很容易影响到其他异常修复逻辑。最好是单独把这个补偿的动作抽离出来不受逻辑的影响。（实在没办法在主流程写完的时候，还要写一份逻辑在补偿程序中，改错了把程序弄乱还容易影响到其他逻辑）

痛点就是，你扫表和补偿逻辑写多了，一方面是慢，一方面是代码越来越乱，而且互相影响，毕竟整个补偿项目不是你一个在上面修改。 最重要的是，当时有问题立即回滚就方便简单，如果要等到补偿程序的话就要判断多种条件才知道怎么补偿了。

## 如何解决时间回拨问题
![[Pasted image 20240219104725.png]]
但是目前我的分布式id是时间+业务码+redis自增，redis自增的这几位是不怕时钟回拨的。
![[Pasted image 20240219153038.png]]
 
## 点击结算按钮
1、mall.order.verify.v2  获取到preOrderId
![[Pasted image 20240219160436.png]]

2、mall.common.set-cache
3、mall.user.address.list
4、mall.common.get-cache
5、mall.order.pre-order.info
6、mall.order.sku-extend.v2  通过preOrderId获取商品标签属性
7、mall.product.sku-list  获取sku数据
8、mall.order.pay-limit.v2
9、mall.order.card.match-total.v2
10、mall.order.logistics-check
11、mall.order.optimal-pay.v2

订单的其他关联表，比如order_product_attr也是有分库分表的，但是这些表的分片key在表中也是单独的一列，叫做 partition_key，存储用户id。

所以都是要先去order_info表找到order_id，再通过order_id找到其他属性。

其他表分别是：
order_address_history
order_balance_info
order_delivery_label
order_delivery_result_info
orderexchange_info
order_package_info
order_product_attr
order_product_ext_info
order_relation_info
order_sku
order_sku_epay
## 点击提交订单按钮

# 订单查询流程

![[Pasted image 20240219104153.png]]

订单中心的标准接口在上层做了单据收口，同时我们在数据层也做了统一的收口。  
  
将业务架构与数据解耦，分布式数据库、缓存、一致性等高可用、高性能设计从业务架构范畴剥离，使业务架构聚焦在业务自身。  
  
持久化系统：用于支撑接单、订单修改、订单取消、订单删除等数据持久化。  
  
搜索系统：提供订单详情查询、订单列表查询、订单状态流水查询、判断是否百川订单等服务。  
  
中继系统：数据枢纽，通过消费消息队列将订单数据写入Elasticsearch、HBase、MySQL。  
  
数据对账系统：用于对比多套存储中间件的数据是否一致，以保障数据最终一致性。  
  
数据同步系统：将订单列表查询所需的查询条件和列表展示字段从老系统同步至订单中心，用于解决因切量过程中订单数据存在于新老系统中而分页困难的问题。
![[Pasted image 20240219105214.png]]
【读写分离架构】采用读写分离架构模式（CQRS），将订单读写流量分离，以提高查询性能和可扩展性，同时达到读、写解耦。  
【缓存】使用分布式缓存Redis缓存热门订单数据以及与订单相关的信息提高并发和响应速度减少对HBase的访问，同时，通过主、备、临时3套高性能缓  
存以提升系统容灾能力。  
【消息队列】使用消息队列JMQ实现异步处理订单提升系统吞吐量，同时流量削峰减轻直接请求ES、HBase、数据库的压力。将不同业务场景(如下单、回  
传)使用不同的Topic进行隔离，可以更好地管理和维护；将不同业务使用不同的Topic隔离，可以实现消息的并行处理和水平扩展，提高系统的吞吐量和性  
能。  
【复杂查询】使用搜索引擎Elasticsearch解决订单复杂查询，先通过Elasticsearch获取订单号，然后根据订单号查询分布式缓存Redis+列式数据库 HBase。  
【低成本持久化存储】采用HBase列式数据库以支持海量数据规模的存储和极强的扩展能力。  
【数据一致性】通过强事务、最终一致、幂等、补偿、分布式锁、版本号等实现  
【多租户架构】系统中采用多租户数据模型，将租户的数据分离存储，以确保数据的隔离性和安全性。根据不同租户的需求动态扩展系统的容量和资源，可以支持系统的水平扩展。通过共享基础设施和资源，多租户架构实现了更高的资源利用率和降低成本。

![[Pasted image 20240219103038.png]]

这里应该是不需要对es进行冷热分离的，es的承载量远大于db，先对es进行冷热分离只会增加业务复杂度
### es写入性能差查询不到订单的问题
es的数据是通过异步写入的，所以有可能还没写入成功。
所以订单的数据的每一步都需要写入redis，做一个短暂的缓存，比如十分钟。
每次都是一起同时获取redis和es的数据，如果es为空或者跟redis数据不一致，就以redis为准。
如果es和redis都没有数据，就去热库尝试寻找下。 

## 分库分表前如何查询大数据量表
数据量大了，经常遇到走错索引了，要使用force index来指定索引，因为数据量越大，mysql统计就越有问题。

而且还需要经常重新建表，减少内存空洞。


## 为什么一开始想对es进行routing操作，后面放弃了？

因为针对order_id，product_id来设置routing，但是实际上有了id之后是不会去es查数据的，都是去数据库查了。  
针对operatorId或者类目之类的，太容易倾斜了。
## 为什么不去hbase中获取数据？
redis已经够快了，如果要实时的话，肯定要落库的，商品不比订单，订单完成后是基本不会改的。

## 为什么要使用grpc？

首先，grpc是比较符合服务器对服务器架构的远程调用方式，可以方便地扩展和压缩之类地。其次，proto文件可以很方便地减少协商交互（最主要原因，太多团队来调用了）。

为什么自定义grpc resolver和picker，而且要使用一致性hash？

grpc在k8s环境下无法自动识别pod的缩容扩容，缩容也许可以通过失效链接判断。

定时断开重新解析dns的方案不够合理，侵入性强。所以通过自定义resolver来监听api server是比较合理的。

然后自定义picker可以充分利用到数据层的本地缓存，只要用到本地缓存也就不会有倾斜问题就不会带来性能问题，而且还可以对分页数据进行预加载(分页预加载要求不对pageNum，pageSize做md5)

只是某些接口有本地缓存的采用自定义picker，比如团队间的拉取操作

grpc是基于http2之上的，http2被设计为一个长期存在的tcp链接，所有都通过该连接进行多路复用。
这样虽然减少了管理连接的开销，但是在负载均衡上又引出了新的问题，就是所有的http请求会走第一个tcp连接。但是grpc的dns解析以及headless以及rounb又无法识别pod的扩容和缩容。
所以还是kubesolver方案比较靠谱。

## 如何确定分库分表的规则
虽然使用用户id作为分片键，但是分片的规则还是非常复杂的。一方面考虑是否均衡，一方面考虑是否易于扩容。  
用户id的hash取库的时候，用的是最后两位，取表的时候用的是最后五位，所以只要最后两位确定了，依靠前面三位数来选表就不够随机了。如果分开选择位数的话，userid hash出来的值又不一定够用（对于现在或者未来）。从扩容的角度上来说的话，一致性hash可以保证一些数据可以留在原表，可以不会受到位数的限制

一致性hash选表，就是通过"库号：表号"的方式再加以hash加入数组，然后将userId哈希值在数组中找到第一个比它大的hash值，在map中通过hash值获取到字符串，用：分割，就获取到库和表了。

(一致性hash扩容方便，增加一个节点只有新增位置以及这个位置之前的节点的范围数据会受到影响。)

不同 分库分表规则的优劣？
基因法：
比如用户下单，通过用户id的md5取最后几位来决定库和表，然后把最后这几位 放到订单id的末尾。这个样子订单id进行分库分表的时候，自然就是用户id的最后几位起到决定性作用。

优点：根据基因法生成的所有id都有相同的库和表
缺点：本质上还是hash取模，不利于扩容

一致性hash法，容易数据倾斜。（反正一致性hash扩容方便，到时候哪里倾斜了，就算出一个位置多加一个表就行。不要害怕倾斜，有倾斜了再来解决，只要设计上扩容方便就行。）

时间range法
查表法



## 如果没有自定义resolver和picker之前，你们是怎么做的？

我们通过定义grpc dns解析以及轮询的方式来做，其实一般情况下是没有什么问题的，因为grpc server一般都是起一般情况更多的pod，所以也极少会对这个情况扩容了，所以一些服务就还是这个老的情况。

但是有些服务主要是面对团队间的调用的，这些服务被拉取的数据量大，有些时候要求的响应速度还比较高，所以我们就引入本地缓存提前预读，而且用一致性hash来命中，就使用了picker和resolver
## 为什么订单数据要分冷热库，以及如何迁移
未完成的订单需要加快查询速度，而且简单化查询逻辑不需要关心分库分表逻辑。（而且下单流程不需要改动，因为冷热库分离中，下单流程全部都是在热库的，只有查询需要关心冷热库。）


有一个服务，当订单状态已完成的时候，先将数据写到订单历史数据库，再开始删除未完成订单数据库，如果这个过程中pod挂了，启动的时候立即去扫库。而且还有个定时任务每半小时扫库去更新。  
一开始使用分布式锁来让单一线程来扫表防止数据竞争，后期考虑分区的方式。
## 冷热迁移为什么要在代码写死？  
通过binlog太麻烦，实时性不够就容易有空洞（突然订单就消失了）。

冷热库双写这个方案的话，可是如果订单状态没有完成我们是没必要去写冷库的。  
通过一些数据迁移工具定时迁移的话太麻烦，暂时不需要引入。

## 为什么要分库分表？
因为innode的一个数据页也就16k，所以三层树高的b+树也就能容纳差不多2000w的数据，再多的话就会多一层树高，也就多一次磁盘io。  
从上述推论中可知，树高取决于三个因素  
1. page的大小  
2. 索引字段 + 指针的大小  
3. 完整数据记录的大小
### **分表是为了什么？**  
  
分表是要对数据表进行一个水平拆分，通过降低单表数据量的方式，来降低CURD时所耗费的时间。  
  
当然上面也讲过，三层树能存储的数据量同样与单条记录的大小有关。  
  
当单表字段过多时，可以先进行一个表的垂直拆分，将一些**非强关联**字段拆分到另外一个表中，通过id进行关联。以这种方式提升能够存储数据的上限。  
  
### **分库是为了什么？**  
  
每个MySQL单实例能够承受的并发量都是有限的，如果在进行数据拆分的同时，我们也想去提高能够接收请求的并发量，这时就同时需要用到分库的手段了。  
  
> 如果并发量不是瓶颈问题，可以仅做分表，后续有需要再进行分库

## kubesolver的原理
通过kubernetes的List-watch机制。


# 订单风控
风控项目中，对于十五分钟内没有改变状态的订单，要置为风险，
# 订单超时取消
我们通过nsq发布延时消息来取消超时订单
那么如何解决 取消超时订单的同时支付成功呢？
首先，支付宝和微信这些支付接口中都有订单取消时间，这个参数传入订单的支付截止时间就行。
第二，支付成功后，要先检查一下订单状态是否合理后才向用户返回支付成功，如果订单状态有问题，就发起退款，通知用户订单过期。（暂时不采用 过期订单支付成功还可以去重新锁定库存的方案）

需要注意的地方  
  
订单生成环节存在超时未支付自动取消的过程，库存的占用会在订单取消后释放。(目前主要是通过nsq发送延时消息来进行超时的处理，并且在背后还有定时逻辑才扫表处理订单状态。)  
  
电商系统要考虑 7 天无理由退货的情景，即订单状态完成后申请退货。此时主要涉及的是金额上的计算以及一些财务程序（如发票等）问题的处理。（只要支付成功后，后面的逻辑就跟我们没有多少关系了，都归售后部门管，我们的退费规则太过复杂，有门店，供应商，电影各种场景。）


# 分布式事务框架

## 老的最终一致性方案
通过xxx校验表和xxxx

## dtm事务
参考https://cloud.tencent.com/developer/article/2230471
https://zhuanlan.zhihu.com/p/562174105

![[Pasted image 20240204174132.png]]

```sql
CREATE TABLE `trans_global` (  
  `id` int(11) NOT NULL AUTO_INCREMENT,  
  `gid` varchar(128) NOT NULL COMMENT '事务全局id',  
  `trans_type` varchar(45) NOT NULL COMMENT 'transaction type: saga | xa | tcc | msg',  
  `status` varchar(45) NOT NULL COMMENT '全局事务的状态 prepared | submitted | finished | rollbacked',  
  `query_prepared` varchar(128) NOT NULL COMMENT 'prepared状态事务的查询api',  
  `create_time` datetime DEFAULT NULL,  
  `update_time` datetime DEFAULT NULL,  
  `commit_time` datetime DEFAULT NULL,  
  `finish_time` datetime DEFAULT NULL,  
  `rollback_time` datetime DEFAULT NULL,  
  `next_cron_interval` int(11) DEFAULT NULL COMMENT '下次定时处理的间隔',  
  `next_cron_time` datetime DEFAULT NULL COMMENT '下次定时处理的时间',  
  `owner` varchar(128) NOT NULL DEFAULT '' COMMENT '正在处理全局事务的锁定者',  
  `protocol` varchar(45) NOT NULL,  
  `options` varchar(1024) DEFAULT NULL COMMENT 'options for transaction like: TimeoutToFail, RequestTimeout',  
  `custom_data` varchar(1024) DEFAULT NULL COMMENT 'custom data for transaction',  
  `ext_data` text COMMENT 'extra data for this trans. currently used in workflow pattern',  
  `result` varchar(1024) DEFAULT NULL COMMENT 'result for transaction',  
  `rollback_reason` varchar(1024) DEFAULT NULL COMMENT 'rollback reason for transaction',  
  PRIMARY KEY (`id`),  
  UNIQUE KEY `gid` (`gid`),  
  KEY `owner` (`owner`),  
  KEY `status_next_cron_time` (`status`,`next_cron_time`) USING BTREE COMMENT 'cron job will use this index to query trans'  
) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4;
```

```sql
CREATE TABLE `trans_branch_op` (  
  `id` bigint(22) NOT NULL AUTO_INCREMENT,  
  `gid` varchar(128) NOT NULL COMMENT 'global transaction id',  
  `url` varchar(1024) NOT NULL COMMENT 'the url of this op',  
  `data` text COMMENT 'request body, depreceated',  
  `bin_data` blob COMMENT 'request body',  
  `branch_id` varchar(128) NOT NULL COMMENT 'transaction branch ID',  
  `op` varchar(45) NOT NULL COMMENT 'transaction operation type like: action | compensate | try | confirm | cancel',  
  `status` varchar(45) NOT NULL COMMENT 'transaction op status: prepared | succeed | failed',  
  `finish_time` datetime DEFAULT NULL,  
  `rollback_time` datetime DEFAULT NULL,  
  `create_time` datetime DEFAULT NULL,  
  `update_time` datetime DEFAULT NULL,  
  PRIMARY KEY (`id`),  
  UNIQUE KEY `gid_uniq` (`gid`,`branch_id`,`op`)  
) ENGINE=InnoDB AUTO_INCREMENT=9 DEFAULT CHARSET=utf8mb4;
```

## 分布式事务的最终兜底方案
business-order-risk预警和对账系统



# 订单系统压测情况
下单接口 -n1000 -c100 tps 70 tp99 2189
一千个请求，100的并发，tps到了70，tp99 只有2189ms

## 库存情况
▪以QPS+10递增加压至60时，TP99在2分钟左右快速增长至7000ms，“库存预占”主应用TPS≤60，预判系统能力达到瓶颈，停止加压库存
▪当TPS上探至1300时，TP99出现明显波动（毛刺≈420ms），且“缓存层交互”应用CPU占用率飙升至90%+，核心链路稳定性劣化，停止加压。  
# 对订单系统的调用链路分析
通看分析下单调用链路发现，存在多个接口重复调用，接口全是串行调用的情况，下单接口耗时太长，已经严重影响系统性能及用户体验
从链路上可有看出下单是IO密集型应用，CPU 利用率低，代码串行执行的话同步等待时间较长，为止我们重新梳理下单业务逻辑，对下单流程进行责任链模式改造

同时我们对系统做了以下优化
1. 对没有依赖的服务进行并发调用（商品/店铺/活动/用户信息等一起并发调用），如图2-3所示。通过md5获取摘要，并且放到redis中缓存几秒钟，每一层获取到商品后就对请求进行md5校验，如果获取不到redis中的盐就重新校验。（每一层不信任上一层的校验）
2. 优化调用链减少冗余调用，推动下游服务接口改造及合并，保证一次请求下来，每个基础接口只会被调用一次，如图2-3所示
3. 设置合理的超时时间和及连接重试（200ms, 部分接口99分位上浮100%,connect连接重试）
4. 排除事务内的外部调用（服务依赖，mq，缓存）    
5. 对弱依赖接口进行mq或异步调用（设置关注/缓存手机号/回滚库存优惠券等）

最终取得的结果是tp99从2189ms到700ms+,取得了三倍的缩小。但是还是没有打到我们500ms的要求。
（最终取得的结果是tps是600，tp99从2189ms到600ms+,取得了三倍的缩小）
我们最终决定对库存系统做一个异步调整，不先对订单系统整体做一个异步是因为我们想小步快走，后期有需要再改造。
一开始考虑做一个批量扣减，但是后面觉得不需要了，mq削峰排队后对锁的竞争已经小了，而且概率上也很难聚合到一起，反而搞得复杂了。
# 后续有可能的库存改进背景介绍

我公司内部有多条业务线，有面向c端的商城，也有面向b端的saas，还有其他我不太了解的业务线团队，公司内部都是共用我这边的订单系统来进行快速接入和开发的。

伴随物流行业的迅猛发展，一体化供应链模式的落地，对系统吞吐、系统稳定发出巨大挑战，库存作为供应链的重中之重表现更为明显。近三年数据可以看出：

![](https://image.3001.net/images/20230823/1692755088_64e564902099a0ae37abc.png!small)

接入商家同比增长37.64%、货品种类同比增长53.66%

货品数量同比增长46.43%、仓库数量同比增长18.87%

通过分析过往大促流量，分钟级流量增长率为75%，大促仓内反馈三方订单下传不及时，库存预占吞吐量和性能是导致订单积压因素之一。目前库存使用mysql数据库作为接单预占的扛量手段，随着一体化供应链建设以及重点KA商家不断接入，现有库存架构在业务支撑上存在风险和缺陷。

此外未来3到5年业务增长、流量增长预计增长5-10倍。为避免系统性能和技术架构缺陷导致业务损失，轻量级库存架构势在必行。

// 名词解释：

库存预占：是指消费者拍下商品订单后，库存先为该订单短暂预留，预留的库存即为预占库存。

当大促仓配单量进入爆发期，热点SKU预占请求快速增长，且库存预占请求直达数据库，系统TP99会出现跳点甚至持续升高，严重情况下造成接单超时。

业务背景解释：
我们做库存预占系统中，核心困难是作为福利电商，一般都是用于企业发放福利，用户上来购买物品本就是由很强的目的性，比如节假日，比如便于套现的商品，比如奶茶券等专用的福利券之类的，而且随着用户接入数越来越多，库存系统的tp99越来越大。

那么主要的困难原因是：
1、流量集中
2、商品集中
3、数据量大

# 压测方案对比
线下压测方案中的不足  
在线下压测试过程中，为了减少与真实环境物理资源上的差异，公司采取的是针对CPU与内存进行等比缩放的策略，如果是计算密集型应用，线上环境总的CPU核数为80，线下压测环境总CPU核数为8核，则为10:1的线上线下等比缩放策略，如果是IO（目前只看内存）密集型应用，线上内存总量如果为80G，线下压测环境内存总量为8G，也同样是10:1的线上线下等比缩放策略。  
很明显这种等比缩放策略在高TPS目标的压测场景下会失真，TPS目标越高，则失真越严重，因为我们并不能对网络、中间件、数据库等一系列的因子也同样做出等比缩放。  
  
线上压测方案中的不足  
在线上压测时，多以读接口为主，只有相当少量的写接口会做线上压测，这少量的写接口通常也需要被压应用的开发人员进行代码改造，以避免大量的压测数据对业务数据造成污染。  
所以这种线上压测方式无法大范围应用，同时这种对代码硬改造的方式，也增加了压测成本与风险，导致大家通常不愿意面对线上压测，更不用提联合上下游一起进行线上全链路压测了，而这种不敢在线上大量压测的思维，也导致更多压测被放在线下以等比缩放的方式进行，其后果则是压测结果的失真，在2012年，某厂正是因为测试环境等比缩放压测，导致网络流量数据失真，引发了线上故障，才促使其下决心走上了线上全链路压测的道路。

因为是分模块测试的，比如商品模块，订单模块，所以我们都是根据线上配置启动了一套docker 集群，每次都只是部署一个模块的内容，用完就删除。我觉得这个方案比较合理，强制征用公司所有的本地资源和网络资源来做这个事情。我们用的测试数据都是人工（链路的复杂度这个方面，首先，我们在压测之前，首先要做的是分析整个模块的调用链路，在这个基础之上，先进行对链路的合并和废弃，或者进行mq调用，然后对一些日常开发中以前没有注意到可以并发调用的地方进行并发调用）

(可能还是需要线上流量回放，我们通过获取线上某个时间段的数据库的快照放到我们的压测环境数据库，然后进行流量录制两天的流量，然后处理一下数据之后把流量重放而已)

流量回放工具选择了 GoReplay，使用 Minio 对象存储对文件管理，可以做到对流量文件和 Gor 执行器 HTTP 上传下载。  
同时在压测过程中，我们考虑将 GoReplay 数据做实时采集，然后再由 Grafana 来可视化展现。  
最后，我们考虑结合 Shell 脚本，使用分布式任务调度中心对命令执行做统一任务调度。  
在压测方案中，我们规划的硬件资源是8台服务器，但我们最后用到的硬件不止8台，中间我们又多加了四台8C16G的机器，所以总共是12台。

# 首次压测和分析
## 首测过程中主要问题
◦压测目标：“库存预占主应用”下的“预占接口”，在数据库承载热点SKU预占请求模式下，探索目标TP99（≤3000ms）可承载的峰值流量，并验证调优后的峰值承载能力（目标 TP99≤500ms）。  
◦压测方案：单个热点SKU持续发压预占，发压起始QPS=10，并以QPS+10递增，探索可承载请求的性能上限。  
◦压测过程及结论  
▪在QPS=50时，系统可稳定支撑库存预占业务（TP99≈100ms）。  
▪“库存预占”主应用：CPU使用率≤15%，内存使用率≤35%  
▪“库存扣减逻辑管控及数据库层交互”应用：CPU使用率≤18%，内存使用率≤65%  
▪数据库：CPU使用率≤7.8%（无慢SQL）  
▪基于当前的系统性能体现，具备持续加压的条件。  
▪以QPS+10递增加压至60时，TP99在2分钟左右快速增长至7000ms，“库存预占”主应用TPS≤60，预判系统能力达到瓶颈，停止加压

瓶颈预判：首先没有看到慢查询，而且单据维度的库存预占是以先查（可用库存）后写（预占库存）的方式进行，在对热点SKU高频次下单过程中，数据库会对该行记录长时间持续读写，数据库层面会通过行锁机制保证单笔交易的原子性，行级锁引发的锁竞争大概率会导致系统处理能力达到瓶颈，制约系统的执行效率。同时从应用层到存储层，未出现硬件资源瓶颈，排除硬件资源不足的影响。


---------------------
库存数据落地到数据库实现其实是一行存储（MySQL），因此会有大量线程来竞争 InnoDB 行锁。但并发越高，等待线程就会越多，TPS 下降，RT 上升，吞吐量会受到严重影响
以下两种方式是常见的
1. 先开启事务，然后select for update得到stock库存数量，然后通过代码计算最终库存，使用SQL `update goods_sku set stock = ? where id = 1`，数量递增递减，这个交给MySQL去操作就好了，因为MySQL的update是自带排它锁的。
2. 使用MySQL的递减语句 `update goods_sku set stock = stock - 2 where id = 1 and stock - 2 > 0;`，但是不检查执行结果，因为很有可能返回的影响行数是0，即没有更新成功，代码中却认为减库存成功了，也需要多留意，需要在业务中判断影响条数，不判断的就是大傻逼！！！！

我们可以在阿里云上看到
- - **Rows Processed**：查看CPU使用率和系统处理行数的关系，判断CPU使用率变化时是否存在突增的行数。        
- **Queries**：查看CPU使用率变化时，主要执行的SQL语句类型。     
- **Thread Running**：高并发会导致CPU使用率变高；MDL堆积或者行锁会导致连接数堆积，进而影响CPU使用率。
- **update_ps** ：平均每秒UPDATE语句执行次数。update_ps 下降。

行锁冲突表现为**Innodb_row_lock_waits**和**Innodb_row_lock_time**监控项的指标升高。
    
- 解决方案
    
    您可以通过`show engine innodb status;`命令查看是否有大量会话处于`Lock wait`状态，如果有，说明行锁冲突比较严重，需要通过优化热点更新、降低事务大小、及时提交事务等方法避免行锁冲突。

我们通过以上指标判断出行锁竞争激烈。

-----------------------------------
## 首测过程中其他问题
通过代码分析和CAT调用堆栈分析，梳理Web层接口对下游服务的调用顺序和调用次数，汇总成对服务层各应用的调用倍数。  
比如order主要链路中，对data-product调用了10次，那么每启动一个order pod，就要有十个data-product，需要获取这样的信息来便于发现启动数据  

### 调用量调整（限流）
问题现象：压测过程qps出现阶段性下降，响应时间阶段性上升  
  
排查手段：首先排查压测平台发出的请求是否正常，通过运维监控工具分析服务运行、硬件资源等是否正常，被调用方访问量是否有设置限流和流量请求是否正常  
例如：商品服务调用scf服务，scf服务设定调用量是8万/分钟，压测300秒，当商品qps达1400/s左右，scf服务出现4次限流，被限流请求走原逻辑（es查询），可以计算出实际调用量为8.4万/s，要确保商品服务请求量在达到峰值不出现被限流的情况，需要把scf调整量提高到10万/分钟。当商品qps需要达到4000/s，那scf服务调用量至少需要调整到25万/分钟。当然调用量不是越大越好，需要考虑被调用方服务实际的承载能力和机器容量情况，如达不到25万调用量承载能力，应当设定限流閥值。  

### 应用降级  
  
问题现象：压测关键业务场景时，发现存在一些边缘业务的接口  
排查手段：梳理边缘业务是否会影响关键业务，跟业务，技术负责人确认，把这些边缘业务接口和功能列出来，在入口处做降级处理  

例如：app首页，会调用多个接口，其中有4个接口不是关键功能接口（活动弹窗，订单提示，回收提示，客服），在配置中心增加配置项开关，在压测过程开启和关闭，对比压测，当关闭降级开关，可以明确看到接口会占用主入口流量，除了关键接口数据，4个接口非关键功能接口同样请求到服务会把数据返回  

开启降级开关，关键接口调用量明显提升，4个接口只是返回空数据，不会请求到服务  
降级的作用是在大促高峰时段，保障应用服务能正常运行，且不影响主功能流程，所采用的应急手段  
### 容量问题 
  
优化前：A服务功能并发压测时，A服务返回响应时间非常慢，当时nginx日志已发出被调用服务的告警信息，查看被调用依赖服务的监控时，发现被调用服务部署（k8s）的实例cpu占用100%，业务功能打开已经有部分出现异常响应。  

优化后：k8s扩容3个实例，重新压测 A服务功能，没有出现响应时间很慢的情况，被调用服务的cpu保持在40-50%之间，没有出现100%的情况  
  
### 应用逻辑抽离（代码改造）  
  
P服务接口，每次请求qps比较低，与研发一同分析代码逻辑，接口每次查询都会查询商列数据，经讨论计划把代码进行改造，把查询商列数据的逻辑业务抽离，再进行对比压测。  
  
老版本接口，逻辑会查询所有商列数据  
新版本接口，抽离业务逻辑，商列数据不查询  
老版本走老逻辑全部逻辑查询在一个接口实现，新版本抽离部分查询逻辑，新增另外的接口查询，降低接口查询压力  
  
# 方案取舍

> **架构：**是⾯向问题，解决问题的手段。 库存系统的问题:非功能性：1.高并发 2.系统稳定性(容灾) 3.数据一致性 功能性: 1.业务复杂 2.数据一致性

## 通过阿里云rds的aliSql
社区不活跃，好几年不发包了，不敢用。

## 异步队列

## redis+任务库+批量扣减库存



# 系统设计

## 设计思路

1.当前库存系统瓶颈在哪里？
抗写流量，数据库成为瓶颈点。

2.如何解决系统瓶颈？
由高并发组件Redis替代数据库。

3.利用Redis需要解决哪些问题？
防超卖，异步写数据库保证最终一致性。
并且作为预占的一部分，不需要实际落到商品库修改库存。

![](https://image.3001.net/images/20230823/1692755088_64e56490aa9eaedeff150.png!small)

### 正向库存扣减流程

对于库存扣减，目前常见有两种库存扣减方案：

**（1）下单时扣库存。**
- **优点是：**实时扣库存，避免付款时因库存不足而阻断影响用户体验。
- **缺点是：**库存有限的情况下，恶意下单占库存影响其他正常用户下单。比如说有 100 台手机，如果没有限制下单数量，这 100 个库存可能被一个用户恶意占用，导致其他用户无法购买。

**（2）支付时扣库存。**
- **优点是：**不受恶意下单影响。
- **缺点是：**当支付订单数大于实际库存，会阻断部分用户支付，影响购物体验。比如说只有 100 台手机，但可能下了 1000 个订单，但有 900 个订单在支付时无法购买。


从用户体验考虑，我们采用的是支付时扣库存 + 回退这种方案。  
下单时记录下来扣减库存的任务流水，但只保留一段时间（比如 30 分钟），保留时间段内未支付则将任务流水置为超时未支付失败，并回滚redis预占库存，避免长时间占用库存。
### 逆向库存回退流程

库存回退基于库存变更日志逐个回退。
库存回退基本流程：订单出库前用户申请退款，回退可售库存、回退预占库存、软删除扣减日志、增加回退日志；一旦商品出库，用户申请退货走处理机流程，可售库存和实物库存均不回退。

![](https://static001.geekbang.org/infoq/cb/cbb0f5e83523cd287ffba989dcd64cda.jpeg?x-oss-process=image%2Fresize%2Cp_80%2Fauto-orient%2C1)


## 总体设计
---

•**扛量部分：**库存性能瓶颈在预占，传统架构主要依靠数据库事务保持数据一致以及数据读写；新版架构设计将数据扛量部分移植到Redis，利用Redis高性能吞吐解决高并发场景下数据读写。

•**数据回写：**Redis进行扛量削峰，后续数据仅用于记账，最终牺牲数据的短暂一致性达到削峰的目的。

•**差异部分：**老版本库存预占设计仅依靠数据进行数据处理，新版设计依靠切量配置建数据切换到Redis，利用Redis高读写进行削峰操作。

![[Pasted image 20240129110027.png]]
## 详细设计

•主流程： 这里的任务库事务插入和redis预占逻辑都需要分布式事务来保证。因为redis预占完可能立即就崩溃了
![[Pasted image 20240129115119.png]]
![[Pasted image 20240129115140.png]]

•**库存初始化：**竞态条件利用Redis map中的锁标记位来实现锁等待，解决并发场景数据不一致问题。
•**LUA执行器：**将原子操作指令/复用指令封装到LUA脚本中以减少网络开销。
•**补偿机制：i>** 执行流程中所有业务异常发生时会同步发起反向操作请求；**ii>** 反向操作执行异常后会提交异步反向操作任务；**iii>**异步任务执行异常后，依赖监控系统扫描异常单据或异常库存并修改异常库存量

•**回溯回写：**任务落库后，支付成功或者支付超时回滚库存等操作调用数据回写服务，数据回写服务操作库存数量；同时回写redis数据，释放预占量库存数据；更新任务库数据状态
![[Pasted image 20240218151443.png]]

![[Pasted image 20240129115536.png]]
## 数据结构

缓存结构：
库存预占关键key：{skuId}\_业务id
库存预占关键key的value:
- is_sell_out  是否售罄
- is_infinite   是否无限库存
- dangerNum   危险警戒库存量
- stock_num 库存数量
-  used_stock_num 预占库存量
- lock_time  锁定截止时间
- lock_user_id 锁定人（用来标记是否处于锁定状态，只有在锁定截止时间前，正确的锁定人才可以修改库存key）

{skuid}\_订单号 预占防重码
{skuId}\_rollback\_订单号  回滚预占防重码  
比如如果扣减Redis过程中，命令编排是先设置防重码，再执行扣减命令失败；如果执行过程网络抖动可能放重码成功，而扣减失败，重试的时候就会认为已经成功，造成超卖，所以上面的命令顺序是错误的，正确写法应该是：

如果是扣减库存，顺序为：1.扣减库存 2.写入放重码。

如果是回滚库存，顺序为：1.写入放重码 2.扣减库存。

单向保证  
在很多场景中，因为没有使用事务，你很难做到不超卖，并且不少卖，所以在极端情况下，可以选择不超卖，但有可能少卖。当然还是应该尽量保证数据准确，  
不超卖，也不少卖；不能完全保证的前提下，选择不超卖单向保证，也要通过手段来尽可能减少少卖的概率。  
比如如果扣减Redis过程中，命令编排是先设置防重码，再执行扣减命令失败；如果执行过程网络抖动可能放重码成功，而扣减失败，重试的时候就会认为已经  
成功，造成超卖，所以上面的命令顺序是错误的，正确写法应该是：  
如果是扣减库存，顺序为：1.扣减库存 2.写入放重码。  
如果是回滚库存，顺序为：1.写入放重码 2.扣减库存

|  |  |  |  |  |
| ---- | ---- | ---- | ---- | ---- |
| key | 预占 | 缺量预占 | 回滚 | 回写 |
| **库存数量** | - | - | + | 不变 |
| **预占库存量** | + | + | - | - |
| **预占防重key** | + | + | - | 不变 |
| **回滚防重** | 不变 | 不变 | + | 不变 |

# 复压结论  
▪完成数据架构升级及热点SKU缓存预热后，初始QPS=1100并以100递增，TPS上探至1200时，TP99≈130ms，系统可稳定支撑批次库存预占业务。  
▪当TPS上探至1300时，TP99出现明显波动（毛刺≈420ms），且“缓存层交互”应用CPU占用率飙升至90%+，核心链路稳定性劣化，停止加压。  
▪相较数据库承载模式，缓存化升级后，TP99满足预期（≤500ms），TPS承载能力大幅提升2300%=(1200-50)/50。

# 系统健壮性思考

## 对于防重的思考

redis的预占防重和回滚防重 用于对同一个订单扣减sku进行防重
mysql中任务流水表中判断订单号+sku来进行防重

mysql中如何扣减库存表防重呢？ 通过分布式事务来做dtm，具体后面在说。

## 热点防刷：

但Redis也是有瓶颈的，如果出现过热SKU就会打向Redis单片，会造成单片性能抖动。库存防刷有个前提是不能卡单的。可以定制设计JVM内毫秒级时间窗的限  
流，限流的目的是保护Redis,尽可能的不限流。限流的极端情况就是商品本来应该在一秒内卖完，但实际花了两秒，正常并不会发生延迟销售，之所以选择JVM  
是因为如果采用远端集中缓存限流，还未来得及收集数据就已经把Redis打死。  
实现方案可以通过guava之类的框架，每10ms一个时间窗，每个时间窗进行计数，单台服务器超过计数进行限流。比如10ms超过2个就限流，那么一秒一台服  
务器就是200个，50台服务器一秒就可以卖出1万个货，自己根据实际情况调整阈值就可以
## 缓存预热和保温
◦全量缓存的弊端：供应链模式中的不同行业，SKU品类生命周期存在较大差异（如服饰行业≈3个月），全量缓存模式会导致Redis中存在大量无效品类，资源消耗膨胀不可控，增加资源成本，有必要设计更有效的缓存方案。  
◦缓存预热及保温的必要性：缓存命中率，与预热机制和保温策略紧密相关。  
▪必要性：常规大促节奏，起售期会触发首次缓存初始化，促销品类与日常销售品类的重合度，决定了首次缓存击穿的概率。目前的Key有效期=7天，大促起售期→开门红→高峰期间隔均大于7天，缺少必要的保温策略，会增加下个促销节点前缓存失效的可能性。

### 系统优化思路  
▪静态方案：  每个节庆期间的上下十五天都是要延长key有效期，覆盖大促关键环节。
▪动态方案：增加热点SKU缓存效期延时策略，Key到期T-1天，日均预占请求量大于1的SKU，自动延长Key有效期。
## Redis&DB对比

---

•首先进行redis&从库数据比对，若存在差异则对主库进行校验

•比对过程中，DB中sku明细行进行锁定(for update)，比对逻辑为DB可用库存量==(Redis可用库存量+Redis预占量)

•有差异，报警且触发SDK可用量过期，同时矫正预占量

![](https://image.3001.net/images/20230823/1692755092_64e564946f3e80d681c51.png!small)

## 容灾方案

![](https://image.3001.net/images/20230823/1692755093_64e564951cede32aba946.png!small)
redis集群不允许主从同步，如果master宕机，从节点走缓存初始化就行。

初始化redis或者对比redis需要先锁住库存表来更新或者对比redis，最后再锁住流水表对比redis预占量，然后再释放。

# 无效调用量分析、识别及调优实战
在性能测试的流量分析阶段，结合业务场景调研，前置识别性能瓶颈疑点。  
推动排查及调整核心链路调用逻辑后，在标定的业务窗口期，核心接口调用总量降低60%↓。  
深入细分业务场景，推演潜在的调优空间。

# 项目最终结论
总体思路：

通过性能测试，推测SKU库存预占场景，在不同存储模式下的性能瓶颈及风险。  
数据架构升级后，SKU库存预占效率（TPS）提升2300%↑。  
测试驱动，结合系统实现，论证缓存预热的必要性，并借助大数据分析，探索科学的缓存预热及保温策略。  
结合新业务模式，思考更加科学的测试数据构建思路和测试过程提效方案


# 项目最后的感受

## 对瓶颈的看法
其实瓶颈这个事情是没有一个具体的定义的，是要看具体场景的。
因为瓶颈其实可以从很多方面说明，例如  
- 系统负载的瓶颈  
- 系统横向纵向扩容的瓶颈  
- 系统开发上人效的瓶颈（团队协作，个人效率方向）

而且瓶颈这个事情是要看 具体的条件和目标的？ 比如具体的硬件设备，所用的cpu和内存，tp99和tps

瓶颈绝对不是硬件问题，硬件问题是钱的问题。


## 稳定性测试应该要注意的地方
### 不要使用过大的测试负载
这里有一点需要注意，千万不要使用过大的测试负载。因为测试负载过大的话，系统资源也会成为性能瓶颈，一定会使响应时间变长。但这时，响应时间变长主要是由资源瓶颈造成的，而不是你开始要找的那个原因。

所以我觉得最后问题的答案绝对不能是 mysql等硬件的问题，硬件问题就已经是钱的问题的了，除非是有一个大的大促之类的活动目标在，不然硬件问题一般是无法解决的，无法作为日常压测了解系统的结果。

不允许过度使用cpu和内存，

### 稳定性测试成功的标志
稳定性测试成功完成的标志，主要有以下三项：  
  
①系统资源的所有监控指标不存在“不可逆转”的上升趋势；  
②事务的响应时间不存在逐渐变慢的趋势；  
③事务的错误率不超过 1%。

### 数据准备原则
（1）真实性和可用性：可以从生产环境获取完整数据（历史数据），作为压测的基础数据，通过分析历史数据增长趋势，作为预估判断基准  
（2）数据脱敏：生产环境的全链路压测，要考虑到不能对生产环境造成影响和不能影响用户体验等，因此在数据准备时，需要进行数据脱敏  
（3）数据隔离：不要污染正常数据，梳理数据处理的每一个环节，提前准备一批压测用户（白名单）  
 准备优惠券数据（支持多个用户不限制领券次数；判断条件只给特定压测用户领取）  
 选用特定商品（过滤正式商户商品）  
 订单数据根据压测用户进行过滤，生成订单数据取消或者删除  
  
（4）在链路的基础上，也有很多的分支业务，而这些分支业务有的需要压测，有的不需要压测，如以下业务可考虑不压测：  
给用户下放 push 消息 ，这是定时执行批量任务，可以不用压测，本身push数据量是通过异步队列处理，不是业务接口发起的短信 

支付，支付会涉及到调用第三方，可能有人想通过mock数据实现，这也是一种压测方式，但是压测出来的结果跟实际对比肯定是有差异的，涉及生产环境，支付是不做压测的


# 后续改进
考虑使用异步下单的环节

## 异步下单的第一种方式
![[Pasted image 20240204175056.png]]

第一种订单队列有两个问题：

1、1和2中任何一处失败怎么办
2、第三方付款成功并且回调通知了我们，当时持久化还没有做好怎办？


优点：无须持久化逻辑就可以直接付款，速度快。
持久化逻辑顺序消费，削峰填谷。
缺点：实现较为复杂。

> 如果订单存在第三方支付情况，① 和 ② 的一致性如何保证？  
首先我们看下，不一致性的时候，会产生什么结果：  
1. ① 失败，用户因为网络原因或返回其它页面，不能获取结果。而 ② 成功，那么最终该订单的状态是待支付。用户进入到个人订单中心完成订单支付即可；  
2. ① 和 ② 都失败，那么下单失败；  
3. ① 成功，② 失败，此时用户在`响应页面`完成了支付动作，用户查看订单信息为空白。  
上述的情况，明显地，只有 3 是需要恢复订单信息的，应对的方案有：  
- 当服务端支付回调接口被第三方支付平台访问时，无法找到对应的订单信息。那么先将这类支付了却没订单信息的数据存储起来先，比如存储到`表A`。同时启动一个`定时任务B`专门遍历表A，然后去订单列表寻找是否已经有了对应的订单信息，有则更新，没则继续，或跟随制定的检测策略走。  
- 当 ② 是由于服务端的`非崩溃性原因而导致失败时：  
    - 失败的时候同时将原始订单数据重新插入到`队列头部`，等待下一次的重新持久化处理。  
- 当 ② 因服务端的`崩溃性原因而导致失败时：  
    - `定时任务B`在进行了多次检测无果后，那么根据第三方支付平台在回调时候传递过来的`订单附属信息`对订单进行恢复。  
- 整个过程订单恢复的过程，用户查看订单信息为空白。  
- `定时任务B` 所在服务`最好`和回调链接 `notifyUrl` 所在的接口服务一致，这样能保证当 B 挂掉的时候，回调服务也跟随挂掉，然后第三方支付平台在调用回调失败的情况下，他们会有`重试逻辑`，依赖这个，在回调服务重启时，可以完成订单信息恢复。

![[Pasted image 20240204175319.png]]

## 异步下单的第二种方式
![[Pasted image 20240204175908.png]]
优点：  
  
1. 持久化与响应的强一致性。  
2. 持久化处理，采用排队的先来先处理，不会像上面谈到的高并发请求一起冲击数据库层面的情况。  
3. 实现简单  
  
缺点：  
  
1. 多订单入队时，持久化单元处理速度跟不上，造成客户端同步等待响应。

对比上面两种常见的订单模型，如果从`用户体验的角度`去优先考虑，第一种不需要用户等待`持久化处理`结果的是明显优于第二种的。如果技术团队完善，且技术过硬，也应该考虑第一种的实现方式。  
  
如果仅仅想要达到`宁愿用户等待到超时`也不愿意存储层服务被冲垮，那么有限考虑第二种。

### b站就采用类似这种方案
按照消息队列的排队方案，我们把整个下单流程调整为异步批量下单链路（图2-7所示） ，在合法校验过后生成订单号提交到databus消息队列 （图2-8所示），再监听databus批量拉取订单进行合并下单（图2-9所示），目前设置的是最多20个一消费，下单结果会在数据库及redis中保存。
![[Pasted image 20240204180037.png]]
![[Pasted image 20240204180053.png]]

在进入队列后，前端会提示活动火爆，正在努力下单中 ，同时在0~2秒内随机调用下单结果查询接口，轮询30秒（必须设置最大时间兜底，防止无限查询）

对于合并的订单进行批量冻结库存，并行冻结优惠券，批量合并sql插入数据库，最大限度上减少性能消耗
![[Pasted image 20240204180118.png]]
其他优化细节

1. 下单限频/限流
2. 其中对于一些弱依赖的操作直接进行降级，比如设置商铺关注，缓存手机号，记录操作日志等
3. 批量操作异常时（接口超时则fail fast），会分解为单个订单重新进行调用(库存操作会试探单库存扣减 单库存扣减成功 并发请求剩余订单，单库存扣减失败 剩余订单全部置为失败)
4. 下单结果查询走redis，异常情况降级为数据库
5. databus异常时，直接降级为同步下单（库存服务也会做限流）    
6. databus消费者会做幂等及超时判断（订单投递时间跟当前时间差值），超过一定时间会自动抛弃，下单失败 

经过改造，压测下单支持4000+tps，最终也顺利利用异步下单支撑了早期的拜年祭手办抢购

# Reference
https://mp.weixin.qq.com/s/H6XBE0utoJGz3XwTINX5VA
https://mp.weixin.qq.com/s/H6XBE0utoJGz3XwTINX5VA