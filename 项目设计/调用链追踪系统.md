
# 调用链追踪系统的设计维度

 发表于 2020-12-20  更新于 2023-11-05  分类于 [系统设计](https://zhenghe-md.github.io/blog/categories/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/)  Disqus： [0 Comments](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/#disqus_thread "disqus")

本文将调用链追踪系统的设计维度归结于以下 5 个：调用链数据模型、元数据结构、因果关系、采样策略以及数据可视化。我们可以把这 5 个维度当作一个分析框架，用它帮助我们在理论上解构市面上任意一个调用链追踪系统，在实践中根据使用场景进行技术选型和系统设计。如果你对调研相关系统很感兴趣，也欢迎参与到 [Database of Tracing Systems](https://github.com/ZhengHe-MD/database-of-tracing-systems) 项目中，一起调研市面上的项目，建立起调用链追踪系统的数据库。

# 引言

阅读本文并不要求读者具备任何调用链追踪系统相关的理论知识或实践经验，读者具备一定的微服务架构的概念或实践经验即可。期望读者看完这篇文章以后，能掌握调用链追踪系统的核心设计维度，理解其中的设计权衡，并能使用这些维度来分析市面上的新老调用链追踪系统实现，甚至帮助到自己在生产实践中根据使用场景进行技术选型和系统设计。

# 解决的问题

## 微服务的可观测性

> Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure.
> 
> — Melvin E. Conway

如果有一门学科叫软件社会学，那么康威定律 (Conway's law) 必定是其中的基本定律之一。如果把互联网公司内部的全体信息系统看作是一整个系统，那么这个系统模块结构会向公司的组织架构收敛。从组织架构层面看，公司结构从扁平向多层级演变，信息传递的环节增加，沟通效率随之下降，进而影响公司的行动效率。不论从组员之间的熟悉程度还是从部门目标一致性来看，部门内部的沟通效率要远远高于部门间的沟通效率。因此，如果系统模块结构与组织架构约趋近，公司的沟通效率就能接近极大值。团队的分化通常伴随着服务的拆分，这也是许多公司业务增长以后进行微服务化的动机。微服务化后，公司信息系统就被迫成为了分布式系统。尽管分布式系统带来了种种好处，如持续集成、增量部署、横向扩展、故障隔离，但系统可观测性比起单机系统下降了很多，甚至几乎没有人能够对公司信息系统有全局性的了解。

任意一个分布式系统的终极理想是：“给开发者以分布式的能力，单机的感受”。而调用链追踪系统就是实现终极理想不可或缺的一部分。调用链追踪系统通过收集调用链数据，帮助开发者在观测分布式系统行为时，从以机器为中心 (machine-centric) 走向以请求为中心 (workflow-centric)。调用链 (traces)、日志 (logs)、监控指标 (metrics)，三者合称 Telemetry，有了它们，微服务开发者既能通盘考虑，又能深入局部分析，在系统规模扩大的同时仍然能够掌控全局。

## 使用场景

当系统的调用链信息被串联起来以后，开发者就能基于此展开各种形式的系统行为分析。常见的使用场景可以分为以下几类：

### 异常检测

异常检测指的是定位和排查一些引发系统异常行为的请求。通常这些请求的出现频率通常很低，其出现属于小概率事件。尽管异常事件被采样的概率很低，但它的信息熵大，能给到开发者更多细节信息。这些细节可能体现在：慢请求、慢查询、循环调用未设上限、存在错误级别日志、未覆盖测试的问题逻辑分支等等。如果调用链追踪系统能主动为开发者发现异常问题，将使得风险隐患提前暴露，并被扼杀在摇篮中。

### 稳态分析

稳态分析指的是分析微服务在正常流量下的各方面状态，分析粒度可能包括单个接口、单个服务、多个服务等等；分析范围可能是单个请求或多个请求；分析角度可能包括埋点指标、依赖关系、流量大小等等。稳态分析通常反映的是系统主要流程的健康状态，一些配置的改动，如存储节点修改、客户端日志上报频率，都可能反馈到系统稳态。稳态分析还可以有很多细分场景，如：

- 稳态性能分析：定位和排查系统稳态中的性能问题，这些问题的起因通常与异常检测类似，只是其影响尚不足以触发报警。
- 服务依赖分析：构建接口级别的依赖关系图，节点通常为接口或服务，边通常为接口或服务的调用关系，边的权重则为流量。构建方式可以分为离线构建和在线构建，对应的就是静态关系图和动态关系图。这些信息可以以基础 API 的方式提供给上游应用使用，如 APM。

### 分布式侧写 (profiling)

许多编程语言都提供侧写工具，如 go tool pprof，能通过采集不同资源的使用负载，如 CPU、内存、协程等，分析进程内部不同模块的资源使用模式，最后通过调用树或火焰图等可视化方式呈现给开发者。分布式侧写就是这类侧写工具的分布式版本，开发者通过打开侧写开关，采样分析一段时间，得到微服务之间的资源占用比例，如时延，然后通过类似单机的数据可视化方式分析接口或服务整体的性能瓶颈。

### 资源归因

资源归因解答的主要问题是：“谁该为我的服务成本买单？” 它需要将资源消耗或占用与请求方关联，资源归因也是成本分析的基础。

### 负载建模

负载建模主要指分析和推测系统的行为表现，该场景解答的问题通常可以表述为 “如果出现 XX 变化，系统整体或关键链路状态会发生什么改变？” 常见应用如容量预估、全链路压测、混沌测试等等。

# 基本实现方案

## 如何追踪调用链

在微服务架构下，每个调用链的信息散落在请求经过的各个微服务中，这些信息需要通过某种技术手段收集并串联起来，重建出完整调用链。存在两种基本思路来解决问题，一种是无代码入侵的黑盒法 (blackbox)；另一种是有代码入侵的元数据传播法 (metadata propagation)。

### 黑盒法

黑盒法，顾名思义，就是将整个微服务集合看作一个黑盒，把特定格式的日志收集到存储中心后，利用统计方法推断、重建调用链：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/blackbox.png)

这种方式的优势就是无代码入侵，但劣势也很明显：推断结果不准确，具体表现在：

- 难以推测异步计算任务关系
- 统计分析需要一定量的数据以及计算资源，耗时长
- 统计分析需要处理数据集不均衡的情况，如请求量很少的接口
- ...

黑盒法在生产实践中并未真正被使用，目前仅作为理论思路，提供参考。

### 元数据传播法

元数据传播法则是将调用链中必要的元数据注入到微服务之间的通信消息中，然后每个服务负责将自己记录的一部分调用链信息上报，这些信息中包含调用链标识、上游服务等信息，最后由后端系统利用这些信息重建调用链。示意图如下：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/metadata-propagation.png)

元数据传播法与黑盒法正好相反，优势在于调用链重建结果准确，劣势在于有代码入侵。但这些代码埋在统一的微服务治理框架中，避免暴露给一线业务开发。几乎所有生产实践中的调用链追踪系统采用的都是元数据传播法。

## 调用链追踪系统基本架构

尽管市面上存在各式各样的调用链追踪系统，但它们的基本架构相对一致：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/basic-architecture.png)

### 埋点

每个微服务会在跨进程的连接处埋点 (instrumentation)，如：

- 发送 HTTP/RPC 请求，接收 HTTP/RPC 响应
- 数据库查询
- 缓存读写
- 消息中间件的生产及消费
- ...

每个点上会记录跨进程操作的名称、开始时间、结束时间以及一些必要的标签键值对，这些信息是整个调用链拼图中的一片。

### 采样

实践中无论从计算和存储资源成本消耗上分析，还是从具体使用场景出发，都不一定需要收集所有埋点数据。因此许多调用链追踪系统会要求按照一定的策略上报数据，目的是取得成本与收益之间的平衡，提高投入产出比。

### 上报

数据可以从服务实例中直接发送到处理中心，或经由同一宿主机上的 agent 代理上报。使用 agent 上报的好处之一在于一些计算操作可以在 agent 中统一处理，一些逻辑如压缩、过滤、配置变更等可以集中到 agent 中实现，服务只需要实现很薄的一层埋点、采样逻辑即可，这也能使得调用链追踪方案对业务服务本身的影响降到最低；使用 agent 上报的另一好处是数据处理服务的发现机制对服务本身透明。因此在每台宿主机上部署 agent 是许多调用链追踪系统的推荐部署方案。

### 处理

调用链数据上报到处理中心，通常称后者为收集器 (collector)，由收集器完成必要的后处理，如数据过滤、数据标记、尾部采样、数据建模等等，最后批量写到不同的存储服务中，并建立必要的索引。

### 存储/索引

调用链追踪数据主要有两个特点：体量大、价值随时间的推移而降低。因此存储服务的选型除了数据模型之外，还需要考虑可扩展性以及数据保留策略 (retention policy) 的支持。另外为了便于查询，我们还需要为数据的存储建立合适的索引。

### 可视化

可视化是高效利用调用链数据的最重要一环，高质量的交互体验能帮助研发快速获取所需信息。通常我们可以将可视化分为两种粒度：单个调用链查看、多个调用链聚合分析，在每个粒度上都有许多可视化方案选择。

### 可扩展性

如果不做任何采样，调用链追踪系统需要处理的数据与全站的请求总量正相关。假如全站所有请求平均要经过 20 个服务处理，那么调用链追踪系统将需要承担全站请求总量 20 倍压力，因此其架构设计上的每一层都需要具备可扩展性。

如果采用服务 SDK 直接上报，那么上报层的横向扩容就自动地通过实例的增加实现；如果采用 agent 代理的上报形式，那么横向扩容就可以通过增加宿主机来实现。数据处理层理论上应该是无状态的，支持横向扩容。由于许多调用链数据的处理逻辑需要获取同一调用链的所有数据，那么通过 TraceID 做负载均衡是天然的选择；数据的存储可扩展性会由所使用的存储服务保证。

### 过载控制

瞬时高峰是常见的流量负载模式，因此调用链追踪系统的各个组件也需要考虑过载控制逻辑。既要防止在峰值流量下埋点及上报对在线服务的影响，也需要考虑调用链追踪后端各模块的承载能力。

在数据上报和处理的过程中，agent 或 collector 可以通过维持本地队列来削峰，但如果超出局部队列的容量限制，就要考虑数据丢失与时效性之间的权衡。如果可以容忍数据丢失，就可以像路由器丢包似的直接丢掉无法处理的数据；如果可以放弃峰值时效性，则可以通过高吞吐、存储容量高的消息中间件，如 Kafka，来代替局部队列。

# 设计维度

在 2014 年的一篇[论文](https://www.pdl.cmu.edu/PDL-FTP/SelfStar/CMU-PDL-14-102.pdf)中，研究团队在分析多个当时的调用链追踪系统后，总结出 4 个设计维度：

- Which causal relationships should be reserved?
- How should causal relationships be tracked?
- How should sampling be used to reduce overhead?
- How should traces be visualized?

本节，我们以这篇论文为起点，介绍调用链追踪系统的 5 个设计维度：调用链数据模型、元数据结构、因果关系、采样策略、数据可视化。

## 1. 调用链数据模型

每个调用链追踪系统实现都需要为调用链数据合理建模，它们对数据模型的选择可能影响到埋点、收集、处理、查询等各个环节。其中最常见的数据模型就是 Span Model 和 Event Model。如果你对这个话题感兴趣可以阅读这篇[文章](http://cs.brown.edu/~rfonseca/pubs/leavitt.pdf)。

### Span Model

Span Model 最早由 Google 在 [Dapper](https://research.google/pubs/pub36356/) 中提出，它将一次计算 (computation) 任务，如处理用户请求，表达成一组 spans 集合，其中每个 span 表示计算任务的一部分 (segment)，记录着开始时间和结束时间。其中每个 span 还记录着触发它的 span，即 parent span，标志着系统中的因果关系。假设 `SpanA` 触发了 `SpanB`，那么 `SpanA` 就是 `SpanB` 的父节点。由于父子关系意味着因果关系，那么 spans 之间组成的关系不会形成环，否则就会出现因果循环。因此通常同一个 trace 的 spans 关系可以使用一棵树表示，举例如下：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/span-model.png)

需要注意的是：在 Span Model 中，每个 span 都只存在一个父节点，即导致某段计算发生的原因只有一个。使用 Span Model 的追踪系统在埋点时需要主动停止 span，停止之后该段计算的信息会被上报到处理中心。从逻辑上看，子节点完成上报后，父节点才会上报；从上报通路上看，二者都由本地线程上报，并无关系。

Span Model 单因多果的关系与调用栈在概念上十分契合，很容易被工程师理解和掌握。然而它并不足以表达所有类型的计算依赖关系，如多因一果：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/diamond.png)

### Event Model

[X-Trace](http://www.icsi.berkeley.edu/pubs/networking/xtrace07.pdf) 是最早使用 Event Model 的项目。在 X-Trace 中，一个事件 (event) 是计算任务中的一个时刻，计算任务中的因果关系由事件之间的边 (edges) 表示，任意两个事件都可以用一条边连接。值得注意的是，这里的 edge 表示的实际上是 [Lamport (1978)](https://lamport.azurewebsites.net/pubs/time-clocks.pdf) 中提到的 "happens-before" 关系，假设有一条边从 EventA 连到 EventB，那么 "happens-before" 表示 EventA 可能对 EventB 产生影响。但在简单场景下，我们可以直接认为边指代激活关系 (activation relationship) 或依赖关系 (dependency relationships)，二者都是 "happens-before" 关系的子集。与 Span Model 不同的是，Event Model 中每个事件可以有多条入边 (incoming edges)，这让 Event Model 可以轻松表达复杂关系，如 fork/join 或 fan-ins/fan-outs 关系。Event Model 支持更精细化的调用链数据展示，举例如下：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/event-model.png)

其中虚线框表示某个执行线程；圆点表示事件；箭头表示边。为了便于理解和对比，图中也用实线方框表示 span。

Event Model 的优势在于表达力强，但缺点是相比 Span Model 更加复杂，对工程师来说更不易接受和上手，同时 Span Model 的类似调用栈的可视化也更加简洁。

## 2. 元数据结构

首先，为了防止歧义，这里特别指出：**元数据指的是在进程间传递的调用链追踪相关的数据**。几乎所有调用链追踪系统都采用元数据传播的方式来追踪跨进程的调用链。那么我们应该如何设计进程间传递的元数据结构？从元数据结构的内容可变性和长度限制两个维度可以将元数据结构划分为三种：静态定长、动态定长和动态变长。

### 静态定长

静态定长元数据，即数据的长度固定且在传播过程中不发生变化。静态定长元数据中只包含一个请求级别的唯一固定 ID，即 TraceID。调用链追踪系统可以通过 TraceID 获取所有与同一个请求相关的信息，然后再建立因果关系。由于元数据中只有 TraceID，系统只能借助一些外部信息，如 threadID、宿主机的墙上时钟，来推测因果关系。

### 动态定长

动态定长元数据，即数据的长度固定但在传播过程中可能发生变化。动态定长元数据中除包含 TraceID 之外，还会传递请求来源标识，如 SpanID 或 EventID，其中来源标识可以建立两两节点之间的上下游关系。

### 动态变长

动态变长元数据，即数据的长度和内容都会在传播过程中发生变化。动态变长元数据中通常包含上游所有节点的全量或部分信息，当前节点处理完毕后，会将当前节点信息及上游信息一同往下游传递。每个节点都能获取调用链到当前节点为止的所有信息，因此无需通过额外的组件重建调用链。

## 3. 因果关系

同一请求 (intra-request) 的计算任务之间可能存在因果关系，如：

- 进程 P1 通过 HTTP 或 RPC 调用进程 P2
- 进程 P1 或 P2 写入数据到存储服务中，或从存储服务中读取数据
- 进程 P1 生产消息到 MQ 中，进程 P2 消费到消息并处理
- ...

不同请求 (inter-request) 的计算任务之间也可能存在因果关系，如：

- 请求 R1 和 R2 同时获取某分布式锁，R1 成功，R2 失败
- 请求 R1 写入数据到本地缓存后请求 R2 也写入数据，同时触发批处理
- 请求 R1 写入数据到存储系统后请求 R2 读出对应数据进行处理
- ...

在实践中，开发者习惯以单个请求的视角分析问题，因此调用链追踪系统通常不会关注不同请求之间的因果关系，但会在数据模型上保持对应的表达能力。对于同一请求的计算任务之间的因果关系，通常 SDK 提供方会尽可能地帮助开发者在所有跨进程的连接点上埋点，以此达到追踪目的，如 HTTP/RPC 调用、数据库访问、消息生产和消费等。但有时候源自于 A 请求的计算任务会被 B 请求触发，如下图中的例子所示：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/submitter-and-trigger.png)

Request one 将数据 d1 提交到局部写回缓存 (write-back cache)，Request two 将数据 d2 提交到同一个缓存中，触发 d1 被写出到持久化存储中。这时如何归属 d1 的写出操作就决定了调用链追踪系统是选择提交者角度 (submitter-preserving) 还是触发者角度 (trigger-preserving)。

### 提交者角度

提交者角度意味着，当聚合或批处理操作被另一个请求触发时，该操作将被归属于提交者。如上方左图所示：Request one 留存在写回缓存中的数据因为 Request two 写入数据而最终被清出，此时清出数据的操作归属于 Request one。

### 触发者角度

触发者角度意味着，当聚合或批处理操作被另一个请求触发时，该操作将被归属于触发者。如上方右图所示：Request one 留存在写回缓存中的数据因为 Request two 写入数据而最终被清出，此时清出数据的操作归属于 Request two。

## 4. 采样策略

调用链数据总体体量与业务体量正相关，全量采集调用链数据将会给公司系统整体带来两方面压力：

- 因数据上报造成的每个业务服务的网络 I/O 压力
- 因数据采集、分析造成的调用链追踪服务的计算和存储压力

为了降低这两方面压力，采样是大多数调用链追踪系统的必备模块。实践中常用的采用策略可以分为三类：

- 头部连贯采样：Head-based coherent sampling
- 尾部连贯采样：Tail-based coherent sampling
- 单元采样：Unitary sampling

它们的示意图如下所示：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/sampling-methods.png)

### 头部连贯采样

头部连贯采样指的是请求进入系统时就立即决定是否采样，并且这个决定会随着元数据被传递到下游服务，保证采样的连贯性。由于采样决定做得早，对系统整体带来的压力较小。但也正因为决定做得早，采样的准确度也最低，很难保证采集到的调用链有价值。头部连贯采样还有一种变体，即头部连贯采样配合异常回溯上报：在头部连贯采样的同时，于每个服务节点缓存最近的若干 spans 信息，一旦下游调用出现异常，则可在微服务框架中感知同时回溯到上游节点，保证出现异常的调用链数据能被上报。

### 尾部连贯采样

尾部连贯采样指的是在请求完成时才决定是否采样。在决定之前，系统需要将数据缓存起来，以保证采样的连贯性。由于采样决定做得晚，数据需要全量上报并临时存储一段时间，这将加重上文提到的两方面压力。但也正因为决定做得晚，获取的信息更全，尾部连贯采样能利用一些经验性的规则保证重要的调用链被采集。

### 单元采样

单元采样并不要求连贯性，系统中的每个组件自行决定是否采样，因此这种方案通常无法建立单个请求的调用链信息。

## 5. 数据可视化

调用链数据的可视化通常与使用场景一一对应，高效的可视化形式能更好地赋能工程师，缩短故障排查时间，提升研发生活质量。

### 甘特图 (Gantt charts)

甘特图常被用于展示单个请求的调用链数据，以下是调用链追踪系统最常用的甘特图变体：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/gantt.png)

图的左边通常组织为树状结构，通常父节点表示调用方，子节点表示被调方，兄弟节点之间为并发关系，且从上至下时间单调递增；图的右边展示的是与标准甘特图类似的条状结构。

### 泳道图 (Swimlane)

泳道图可以被用于展示单个请求的调用链数据，相比甘特图更加精细，常用于 [Event Model](https://github.com/ZhengHe-MD/database-of-tracing-systems/blob/main/dimensions/design/tracing-model/README.md#event-model) 展示更复杂的计算关系，举例如下：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/swimlane.png)

其中泳道，即虚线框，用于表示计算执行单元；圆点展示某时刻发生的事件；箭头表示事件之间的关系。

### 流程图 (Flow graphs)

流程图常被用于展示多个相似请求调用链数据的聚合信息，这些请求的调用链结构应该完全一致。举例如下：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/flow-graph.png)

图中的节点表示系统中发生的事件，边表示因果关系，权重可以表示事件发生的时间差，它们共同组成一个有向无环图。流程图甚至可以表达 fan-outs 和 fan-ins，即 forks 和 joins 的因果关系，能保留更多的调用链细节信息。

### 调用图 (Call graphs)

调用图被用于展示多个请求的聚合信息，这些请求的调用链结构无需完全一致。调用图上的节点表示系统中的服务、模块或接口，边表示因果关系，权重则可以表示流量、资源占用等自定义信息。调用图中可能出现环，意味着系统中存在环形依赖。调用图示例如下：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/call-graph.png)

### 调用树 (Calling Context Trees)

调用树被用于展示多个请求的聚合信息，这些请求的调用链结构通常不同。调用树根节点到任意叶子节点的路径都是分布式系统中真实存在的调用路径，举例如下：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/cct.png)

## 火焰图 (Flame graph)

火焰图常被用于展示单机程序调用栈耗时信息，如 Go 中的 pprof。它与调用树的结构类似，常被用于展示多个请求的聚合信息，但展示形式不同，能更直观地展示各个组件的耗时信息，举例如下：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/flame-graph.png)

## 从维度到场景

了解各个设计维度之后，我们一起回顾本文开头提到的场景，试着分析在这些维度上该如何选择。下面以异常检测和分布式侧写为例：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/use-case-dimension-matrix.png)

**异常检测**：某个请求出问题开发者需要查看完整调用链信息，因此需要连贯采样，又由于问题请求的发生是小概率事件，只能通过尾部连贯采样来保证数据都能被捕获。开发者习惯以从每个请求造成的影响分析问题，因此请求内部的因果关系应该选择触发者视角。甘特图、流程图都是适用单个调用链的可视化方案。元数据结构中，动态定长相对静态定长能更准确地采集上下游关系，相对动态变长能节省网络成本，且后者带来的实时性上的优化对异常检测并不重要，因此动态定长元数据是更合适的选择。

**分布式侧写**：侧写能够帮助开发者查看调用链级别的性能瓶颈问题，但分析对象是聚合的数据，对单个调用链的完整性并无要求，单元采样是成本最低的采样方案。与异常检测类似，触发者视角更符合开发者直觉，且没有额外开销，因此选择触发者视角。侧写的可视化选择毫无悬念：调用树和火焰图。元数据结构中，如果调用链深度可控，动态变长能帮助开发者更快地看到侧写数据；如果深度不可控，动态定长同样满足需求，只是在数据处理环节需要消耗计算资源。

调用链数据模型会影响各个场景的最终实现效果和能力边界，但不影响场景解决方案的有效性，因此这里没有专门讨论。如果在实践中你需要同时解决多个场景，就需要考虑在各个设计维度上取一个包集。

# 案例分析: Jaeger

## 项目历史

Jaeger 的名字源于德语中的猎人，是由 Uber 内部 Observability 团队开发，集成埋点、收集到可视化的完整调用链追踪解决方案。2017 年 4 月 Jaeger 正式开源；2017 年 9 月进入 CNCF 孵化；2019 年 10 月正式从 CNCF 毕业，成为 CNCF 顶级项目。

## 基本架构

Jaeger 的架构与上文提到的调用链追踪系统的基本架构十分类似，它有两种部署架构选择，分别如下面两张图所示：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/jaeger-architecture-1.png)

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/jaeger-architecture-2.png)

二者结构大致相同，主要区别在于 jaeger-collector 与 DB 之间加了 Kafka 做缓冲，解决峰值流量过载问题。整个 Jaeger 后端不存在单点故障，Jaeger-collector、Kafka、DB (Cassandra 和 ElasticSearch) 都支持横向扩展。

## 使用场景: 稳态分析

Jaeger 在官网上介绍自己的主要功能如下：

- 分布式上下文传播 (Distributed context propagation)
- 分布式事务监控 (Distributed transaction monitoring)
- 根因分析 (Root cause analysis)
- 服务依赖分析 (Service dependency analysis)
- 性能/时延优化 (Performance/latency optimization)

重建调用链关系需要在进程间传播元数据，因此分布式上下文传播其实是实现调用链追踪数据建模的基础，我们通常不会使用它来传播非调用链追踪相关的数据，如 uid、did 等等。这些数据一般会通过微服务治理框架来传播。后面的分布式事务监控、根因分析、服务依赖分析、性能/时延优化，主要是通过采集侧收集上来的调用链数据及服务 (service)、操作 (operation) 的依赖关系，分析系统行为。

## 调用链数据模型: Span Model

Jaeger 中调用链数据模型遵守了 opentracing 标准，使用的是典型的 Span Model，其核心数据结构如下图所示：

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/data-model.png)

下面是一个具体的例子：

|   |   |
|---|---|
|1  <br>2  <br>3  <br>4  <br>5  <br>6  <br>7  <br>8  <br>9  <br>10  <br>11  <br>12  <br>13  <br>14  <br>15  <br>16  <br>17|# source: https://github.com/opentracing/specification/blob/master/specification.md  <br>Causal relationships between Spans in a single Trace  <br>  <br>  <br>        [Span A]  ←←←(the root span)  <br>            \|  <br>     +------+------+  <br>     \|             \|  <br> [Span B]      [Span C] ←←←(Span C is a `ChildOf` Span A)  <br>     \|             \|  <br> [Span D]      +---+-------+  <br>               \|           \|  <br>           [Span E]    [Span F] >>> [Span G] >>> [Span H]  <br>                                       ↑  <br>                                       ↑  <br>                                       ↑  <br>                         (Span G `FollowsFrom` Span F)|

其中 Span 与 Span 之间存在两种因果关系，ChildOf 和 FollowsFrom。ChildOf 关系中，父节点依赖于子节点执行的结果；FollowsFrom 关系中，父节点不依赖于子节点执行的结果，但与之存在因果关系。

## 因果关系: 用户决定，触发者视角为主

Jaeger 采用的调用链数据模型完全能够关联同一个请求中的不同进程，是提交者视角还是触发者视角则取决于 Jaeger 的接入方，选择触发者视角对接入方不存在额外的成本，而选择提交者视角则需要接入方投入额外的精力做定制化开发。因此在绝大多数情况下使用的是触发者视角。

## 元数据结构: 动态定长

Jaeger 在进程间传递的元数据结构如下：

|   |   |
|---|---|
|1  <br>2  <br>3  <br>4  <br>5  <br>6  <br>7  <br>8  <br>9  <br>10  <br>11  <br>12  <br>13  <br>14  <br>15  <br>16  <br>17  <br>18  <br>19  <br>20  <br>21  <br>22  <br>23  <br>24|// source: https://github.com/jaegertracing/jaeger-client-go/blob/master/span_context.go  <br>// SpanContext represents propagated span identity and state  <br>type SpanContext struct {  <br>	// traceID represents globally unique ID of the trace.  <br>	// Usually generated as a random number.  <br>	traceID TraceID  <br>	// spanID represents span ID that must be unique within its trace,  <br>	// but does not have to be globally unique.  <br>	spanID SpanID  <br>	// parentID refers to the ID of the parent span.  <br>	// Should be 0 if the current span is a root span.  <br>	parentID SpanID  <br>	// Distributed Context baggage. The is a snapshot in time.  <br>	baggage map[string]string  <br>	// debugID can be set to some correlation ID when the context is being  <br>	// extracted from a TextMap carrier.  <br>	//  <br>	// See JaegerDebugHeader in constants.go  <br>	debugID string  <br>	// samplingState is shared across all spans  <br>	samplingState *samplingState  <br>	// remote indicates that span context represents a remote parent  <br>	remote bool  <br>}|

利用 traceID 可以确认当前 span 的归属关系；利用 spanID 和 parentID 可以建立上下游进程的父子关系。通常 baggage 中的数据量不会变化。综合考虑：Jaeger 的元数据结构属于动态定长。

## 采样策略: 头部连贯采样

目前 Jaeger 支持三种采样方式：

- Const：要么全采样，要么不采样
- Probabilistic：按固定概率采样
- Rate Limiting：限流采样，即保证每个进程每隔一段时间最多采 k 个

除了在 sdk 初始化时直接写死采样配置外，Jaeger 还支持远程动态调整采样方式，但调整的选择范围仍然必须为上面三种之一。为了防止一些调用量小的请求因为出现概率低而无法获得调用链信息，Jaeger 团队也提出了适应性采样 (Adaptive Sampling) ，但这个提议从 2017 年至今仍然未有推进。

无论是上述哪种方式，是否采样这一决定都是在请求进入系统之时决定，因此结论是：目前 Jaeger 支持头部连贯采样。值得一提的是，Jaeger 团队也在讨论[引入尾部连贯采样的可能性](https://github.com/jaegertracing/jaeger/issues/425)，但尚未有实质性的进展。

## 数据可视化: 甘特图、调用树、调用图

jaeger-ui 项目提供了丰富的调用链数据可视化支持，包括针对单个请求的甘特图、调用树，以及全局服务的调用图。

### 甘特图

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/gantt.png)

### 调用树

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/cct.png)

调用树目前仍在实验阶段，暂时还不是正式功能。

### 调用图

![](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/call-graph.png)

同时还可以聚焦到某个节点，让调用图只显示与该节点相关的服务，即焦点图 (focus graph)。

# 调用链追踪系统数据库

[AP](http://www.cs.cmu.edu/~pavlo/) 在 2014 年建立了网站 [dbdb.io](https://dbdb.io/)，即 Database of Databases，从一些固定的维度来分析市面上琳琅满目的数据库系统。受这个项目启发，我们也可以用本文提到的设计维度，来分析市面上的调用链追踪系统，从而获得更加系统化的理解，并将分析调研的结果沉淀下来。于是我就建立了这个项目 [Database of Tracing Systems](https://github.com/ZhengHe-MD/database-of-tracing-systems)，如果你对此感兴趣，欢迎参与调研，共同建立调用链追踪系统的数据库。


# 调用链追踪系统在伴鱼：实践篇

 发表于 2021-03-04  更新于 2023-11-05  分类于 [系统设计](https://zhenghe-md.github.io/blog/categories/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/)  Disqus： [1 Comment](https://zhenghe-md.github.io/blog/2021/03/04/implementing-tail-based-sampling/#disqus_thread "disqus")

> 此文同时发表在[伴鱼技术博客](https://tech.ipalfish.com/blog/2021/03/04/implementing-tail-based-sampling/)上

在 [理论篇](https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/) 中，我们介绍了伴鱼在调用链追踪领域的调研工作，本篇继续介绍伴鱼的调用链追踪实践。在正式介绍前，简单交代一下背景：2015 年，在伴鱼服务端起步之时，技术团队就做出统一使用 Go 语言的决定。这个决定的影响主要体现在：

1. 内部基础设施无需做跨语言支持
2. 技术选型会有轻微的语言倾向

## 1. 早期实践

### 1.1 对接 Jaeger

2019 年，公司内部的微服务数量逐步增加，调用关系日趋复杂，工程师做性能分析、问题排查的难度变大。这时亟需一套调用链追踪系统帮助我们增强对服务端全貌的了解。经过调研后，我们决定采用同样基于 Go 语言搭建的、由 CNCF 孵化的项目 [Jaeger](https://www.jaegertracing.io/)。当时，服务的开发和治理都尚未引入 `context`，不论进程内部调用还是跨进程调用，都没有上下文传递。因此早期引入调用链追踪的工作重心就落在了服务及服务治理框架的改造，包括：

- 在 HTTP、Thrift 和 gRPC 的服务端和客户端注入上下文信息
- 在数据库、缓存、消息队列的接入点上埋点
- 快速往既有项目仓库中注入 `context` 的命令行工具

部署方面：测试环境采用 **all-in-one**，线上环境采用 [**direct-to-storage**](https://www.jaegertracing.io/docs/1.22/architecture/) 方案。整个过程前后大约耗时一个月，我们在 2019 年 Q3 上线了第一版调用链追踪系统。配合广泛被采用的 prometheus + grafana 以及 ELK，我们在微服务群的可观测性上终于凑齐了调用链 (traces)、日志 (logs) 和监控指标 (metrics) 三个要素。

下图是第一版调用链追踪系统的数据上报通路示意图。服务运行在容器中，通过 opentracing 的 sdk 埋点，Jaeger 的 go-sdk 上报到宿主机上的 Jaeger-agent，后者再将数据进一步上报到 Jaeger-collector，最终将调用链数据写入 ES，建立索引，即图中的 Jaeger backends。

![distributed-tracing-v1](https://zhenghe-md.github.io/blog/2021/03/04/implementing-tail-based-sampling/distributed-tracing-v1.png)

### 1.2 遇到的问题

在 [理论篇](https://tech.ipalfish.com/blog/2020/12/29/design-dimensions-of-tracing-systems/) 中，我们介绍过 Jaeger 支持三种采样方式：

- Const：要么全采样，要么不采样
- Probabilistic：按固定概率采样
- Rate Limiting：限流采样，即保证每个进程在固定时间内最多采 k 个

这些采样方式都属于头部连贯采样 (head-based coherent sampling)，我们在 [理论篇](https://tech.ipalfish.com/blog/2020/12/29/design-dimensions-of-tracing-systems/) 中曾讨论过其优劣势。伴鱼的生产环境中使用的是限流采样策略：每个进程每秒最多采 1 条 trace。这种策略虽然很节省资源，但其缺点在一次次线上问题排查中逐渐暴露：

1. 一个进程中包含多个接口：不论按固定概率采样还是限流采样，都会导致**小流量接口一直采集不到调用链数据而饿死 (starving)**
2. 线上服务出错是小概率事件，导致出错的请求被采中的概率更小，就导致**采到的调用链信息量不大，引发问题的调用链却丢失**的问题

## 2. 调用链通路改造

### 2.1 使用场景

2020 年，我们不断收到业务研发的反馈：**能不能全量采集 trace？**

这促使我们开始重新思考如何改进调用链追踪系统。我们做了一个简单的容量预估：目前 Jaeger 每天写入 ES 的数据量接近 100GB/天，如果要全量采集 trace 数据，保守假设平均每个 HTTP API 服务的总 QPS 为 100，那么完整存下全量数据需要 10TB/天；乐观假设 100 名服务器研发每人每天查看 1 条 trace，每条 trace 的平均大小为 1KB，则整体信噪比千万分之一。可以看出，这件事情本身的 ROI 很低，考虑到未来业务会持续增长，存储这些数据的价值也会继续降低，因此全量采集的方案被放弃。退一步想：全量采集真的是本质需求吗？实际上并非如此，我们想要的其实是**「有意思」的 trace 全采，「没意思」的 trace 不采**。

根据 [理论篇](https://tech.ipalfish.com/blog/2020/12/29/design-dimensions-of-tracing-systems/) 中介绍的应用场景，实际上第一版调用链追踪系统只支持了稳态分析，而业务研发亟需的是异常检测。要同时支持这两种场景，我们必须借助尾部连贯采样 (tail-based coherent sampling)。相对于头部连贯采样在第一个 span 处就做出是否采样的决定，尾部连贯采样可以让我们在获取 (接近) 完整的 trace 信息后再做出判断。理想情况下，只要能合理地制定采样的判断逻辑，我们的调用链追踪系统就能做到「有意思」的 trace 全采，「没意思」的 trace 不采。

### 2.2 架构设计

Jaeger 团队从 2017 年就开始[讨论](https://github.com/jaegertracing/jaeger/issues/425)引入 tail-based sampling 的可能性，但至今未有定论。在一次与 Jaeger 工程师 [jpkrohling](https://github.com/jpkrohling) 的一对一沟通中，对方也证实了目前 Jaeger 尚没有相关的支持计划。因此，我们不得不另辟蹊径。经过一番调研，我们找到了刚刚进驻 CNCF SandBox 的 [OpenTelemetry](https://github.com/open-telemetry)，它的 [opentelemetry-collector](https://github.com/open-telemetry/opentelemetry-collector) 子项目恰好能支持我们在现有架构上引入尾部连贯采样。

#### 2.2.1 OpenTelemetry Collector

整个 OpenTelemetry 项目目的在于统一市面上可观测性数据 (telemetry data) 的标准，同时提供推动这些标准实施的组件和工具。opentelemetry-collector 就是这个生态中的一个重要组件，它的架构图如下：

![otelcol-architecture](https://zhenghe-md.github.io/blog/2021/03/04/implementing-tail-based-sampling/open-telemetry-collector-architecture.png)

collector 内部有 4 个核心组件：

- Receivers：负责接收不同格式的 telemetry data，对于 trace 来说就是 Zipkin、Jaeger、OpenCensus 以及其自研的 OTLP。除此之外，还可以支持从 Kafka 中接收以上格式的数据
- Processors：负责实施处理逻辑，如打包、过滤、修饰、采样等等，尾部采样逻辑就可以在这里实现
- Exporters：负责将处理后的 telemetry data 按指定的格式重新输出到后端服务中，如 Zipkin、Jaeger、OpenCensus 的 backend，也可以输出到 Kafka 或另一组 collector 中
- Extensions：提供一些核心流程之外的插件，如分析性能问题的 pprof，健康监测的 health 等等

利用不同的 Receivers、Processors、Exporters，我们可以组装出一个或多个 Pipelines。

opentelemetry-collector 项目被拆成两部分：主项目 [opentelemetry-collector](https://github.com/open-telemetry/opentelemetry-collector) 和社区贡献项目 [opentelemetry-collector-contrib](https://github.com/open-telemetry/opentelemetry-collector-contrib)，前者负责管理核心逻辑、数据结构以及通用的 Receivers、Processors、Exporters、Extensions 实现，后者则负责接收一些社区贡献的组件，当然贡献者主要都来自于可观测性 SaaS 解决方案供应商，如 DataDog、Splunk、LightStep 以及一些公有云厂商。opentelemtry-collector 项目的插件化管理方式，使得**定制化开发 Receiver、Processor、Exporter 的成本很低**，我们在做概念验证时，基本可以在一两个小时内开发并测试完毕一个组件。除此之外，opentelemetry-collector-contrib 还提供了开箱即用的 [tailsamplingprocessor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/tailsamplingprocessor)。

由于 opentelemetry-collector 是 OpenTelemetry 团队推动标准实施的重要组件，且 OpenTelemetry 本身并没有提供独立的数据存储、索引实现的计划，因此它在与市面上流行的调用链追踪框架，如 Zipkin、Jaeger、OpenCensus，的兼容性上下了很大功夫。通过 Receivers 和 Exporters 的使用，我们可以用它替换 jaeger-agent，也可以放在 jaeger-agent 与 jaeger-collector 之间，必要时还可以在 jaeger-agent 和 jaeger-collector 之间部署多层 collectors。除此之外，如果有一天想换掉 jaeger-backend，比如新发布的 [Grafana Tempo](https://grafana.com/oss/tempo/)，我们也能很轻松的完成，并且利用多个 Pipelines 或一个 Pipeline 多个 Exporters 灰度上线，逐步替换。

#### 2.2.2 上报通路

基于以上的调研和现有的架构，我们设计了第二版调用链追踪的架构，如下图所示：

![distributed-tracing-v2](https://zhenghe-md.github.io/blog/2021/03/04/implementing-tail-based-sampling/distributed-tracing-v2.png)

用一组 opentelemetry-collector 替换 jaeger-agent，即图中的 otel-agent；同时在 otel-agent 与 jaeger-collector 之间增加另一组 opentelemetry-collector，即图中的 otel-collector。otel-agent 收集宿主机上不同服务上报的 trace 数据，打包批量发送给 otel-collector，后者负责做尾部连贯采样，将**「有意思」**的 trace 继续输出到原始架构中的 jaeger-collector，后者负责将数据投入 ElasticSearch 中，建立索引。

这里还有一个问题需要解决：整个架构要做到高可用，势必要部署多个 otel-collector 实例，如果使用简单的负载均衡策略，不同 otel-agents、以及单个 otel-agent 不同时刻上报的数据，可能被随机上报到某个 otel-collector 实例，这就意味着，同一个 trace 的不同 spans 无法集中到同一个 otel-collector 实例上，既会导致同一个 trace 的不同 spans 的决定不一致，即不是连贯采样；也会导致尾部采样时提供判断的数据不全。解决方案很简单：**让 otel-agent 按照 traceID 做负载均衡**。

在调研阶段我们正好看到 opentelemetry-collector 社区也有此支持[计划](https://github.com/open-telemetry/opentelemetry-collector/issues/1724)，并且前面提到的工程师 jpkrohling 正在通过增加 [loadbalancingexporter](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/loadbalancingexporter) 解决它，虽然直接使用 bleeding edge 的版本有一定的风险，我们还是决定尝试。在概念验证阶段，我们也的确发现了新功能的若干问题，但通过反馈 ([#1621](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/1621)) 和贡献 ([#1677](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/1677)、[#1690](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/1690)) 的方式一一解决，最终获得了可以按预期执行尾部连贯采样的调用链追踪系统。

### 2.3 采样规则

尾部连贯采样的数据通路已经准备就绪，下一步就是确定和实施采样规则。

#### 2.3.1 「有意思」的调用链

什么是「有意思」的调用链？研发在分析、排障过程中想查询的任何调用链就是「有意思」的调用链。但落实到代码的必须是确定性的规则，根据日常排障经验，我们先确定了以下三种情形：

- 在调用链上打印过 ERROR 级别日志
- 在调用链上出现过大于 200ms 的数据库查询
- 整个调用链请求耗时超过 1s

满足任意条件，就认为这个调用链「有意思」。在伴鱼，只要服务打印了 ERROR 级别的日志就会触发报警，研发人员就会收到 im 消息或电话报警，如果能保证触发报警的调用链数据必采，研发人员的排障体验就会有很大的提升；我们的 DBA 团队认为超过 200ms 的查询请求都被判定为慢查询，如果能保证这些请求的调用链必采，就能大大方便研发排查导致慢查询的请求；对于在线服务来说，时延过高会令用户体验下降，但具体高到什么程度会引发明显的体验下降我们暂时没有数据支撑，因此先配置为 1s，支持随时修改阈值。

当然，以上条件并不绝对，我们完全可以在之后的实践中根据反馈调整、新增规则，如单个请求引起的数据库、缓存查询次数超过某阈值等。

#### 2.3.2 采样流水线

在第二版系统中，我们期望同时支持稳态分析与异常检测，因此采样过程既要按概率或限流方式采集一部分 trace，也需要按上文拟定的「有意思」规则采集令一部分 trace。截止到本文撰写前，[tailsamplingprocessor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/tailsamplingprocessor) 支持 4 种策略：

- `always_sample`：全采样
- `numeric_attribute`：某数值属性位于 `[min_value, max_value]` 之间
- `string_attribute`：某字符串属性位于集合 `[value1, value2, ...]` 之中
- `rate_limiting`：按照 spans 数量限流，由参数 `spans_per_second` 控制

其中可以为我们所用的有 `numeric_attribute`、`string_attribute` 以及 `rate_limiting`。

「按概率或限流采集一部分 trace」可以利用 `rate_limiting` 实现吗？`rate_limiting` 只能按照每秒通过的 spans 个数限流，但 spans 数量在高峰期、低峰期、业务所处阶段都不一样，每个 trace 的平均 spans 数量也会随着微服务群规模以及依赖关系发生变化，因此设置 `spans_per_second` 将让我们很难对这个参数的最终效果合理评估，因此直接使用 `rate_limiting` 的方案被否决。

「按规则采集另一部分 trace」可以直接使用 `numeric_attribute` 和 `string_attribute` 实现吗？span 中还存在布尔 (bool) 类型的 tag，以「在调用链上如果打印了 ERROR 级别日志」为例，按照规范我们会记录 `span.SetTag("error" , true)`，但 tailsamplingprocessor 并未支持 `bool_attribute`；此外，未来我们可能会有更复杂的组合条件，这时仅靠 `numeric_attribute` 和 `string_attribute` 也无法实现。

经过再三分析，我们最终决定利用 Processors 的链式结构，组合多个 Processor 完成采样，流水线如下图所示：

![sampling-pipeline](https://zhenghe-md.github.io/blog/2021/03/04/implementing-tail-based-sampling/sampling-pipeline.png)

其中 `probattr` 负责在 trace 级别按概率抽样，`anomaly` 负责分析每个 trace 是否符合「有意思」的规则，如果命中二者之一，trace 就会被打上标记，即 `sampling.priority`。最后在 `tailsamplingprocessor` 上配置一条规则即可，如下所示：

|   |   |
|---|---|
|1  <br>2  <br>3  <br>4  <br>5  <br>6  <br>7  <br>8  <br>9|tail_sampling:  <br>   policies:  <br>     [  <br>       {  <br>         name: sample_with_high_priority,  <br>         type: numeric_attribute,  <br>         numeric_attribute: { key: "sampling.priority", min_value: 1, max_value: 1 }  <br>       }  <br>     ]|

这里 `sampling.priority` 是整数类型，当前取值只有 0 和 1。按上面的配置，所以 `sampling.priority = 1` 的trace 都会被采集。后期可以增加更多的采集优先级，在必要的时候可以多采样 (upsampling) 或降采样 (downsampling)。

## 3. 部署实施

采样规则确立后，整个解决方案就已跑通，下一步就是进入部署实施阶段。

### 3.1 上线准备

#### 3.1.1 基础库改造

##### 动态更新 Tracer

在第一版系统中，每个进程启动时会从 [apollo](https://github.com/ctripcorp/apollo) 上获取采样配置，传递给 Jaeger sdk，后者利用配置初始化 GlobalTracer。GlobalTracer 会在 trace 的第一个 span 出现时，决定是否采集，并把这个决定传递下去，即头部连贯采样。在实施新架构时，我们需要 Jaeger sdk 将所有 trace 数据尽数上报。为了让这个过程更加平滑，我们对 Jaeger sdk 配置做出两方面改造：

- 支持对每个服务下发不同的采样配置，方便灰度发布
- 支持动态更新采样配置，使采样策略配置不依赖发布

##### 日志库改造

为了保证打印过 ERROR 级别日志的调用链必采，我们也在通用日志库的相应位置上给 span 打上 error 标签。

#### 3.1.2 监控看板配置

opentelemetry-collector 内部利用 OpenCensus sdk 埋了很多有用的监控指标，并按照 [Open Metrics](https://github.com/OpenObservability/OpenMetrics/blob/master/specification/OpenMetrics.md) 规范暴露数据。因为指标数量并不多，我们将大多数指标都配置了到 Grafana 看板中，包括：

- Receivers：[obsreport_receiver](https://github.com/open-telemetry/opentelemetry-collector/blob/68c5961f7bc2d8d55fd431ab07367fc16f8b26d0/obsreport/obsreport_receiver.go)
- Processors： [obsreport_processor](https://github.com/open-telemetry/opentelemetry-collector/blob/68c5961f7bc2d8d55fd431ab07367fc16f8b26d0/obsreport/obsreport_processor.go), [tailsamplingprocessor](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/metrics.go)
- Exporters：[obsreport_exporter](https://github.com/open-telemetry/opentelemetry-collector/blob/68c5961f7bc2d8d55fd431ab07367fc16f8b26d0/obsreport/obsreport_exporter.go)

下面介绍几个在实践中我们认为比较重要的指标：

##### xxx_receiver_accepted/refused_spans

这里的 xxx 指代任意一个 pipeline 中使用的 receiver。实际上这里有两个具体指标：receiver 接收的 spans 数量和 receiver 拒绝的 spans 数量。二者可以与其它指标结合，分析系统在当前状况下的入口流量瓶颈。

##### xxx_exporter_send(failed)_spans

这里的 xxx 指代任意一个 pipeline 中使用的 exporter。实际上这里有两个具体指标：exporter 发送成功的 spans 数量和 exporter 发送失败的 spans 数量。二者可以与其它指标结合，分析系统在当前状况下的出口流量瓶颈。

##### otelcol_processor_tail_sampling_sampling_trace_dropped_too_early

要介绍上面这个指标，需要简单了解 tailsamplingprocessor 的工作原理。在分布式环境中，tailsamplingprocessor 永远无法确定一个 trace 的所有 spans 在当前时刻是否收集完毕，因此需要设置一个超时时间，即 [这里](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/tailsamplingprocessor) 的 `decision_wait`，下面假设 `decision_wait = 5s`。Trace Data 进入 processor 后，会被分别放入两个数据结构：

![tailsamplingprocessor-traces](https://zhenghe-md.github.io/blog/2021/03/04/implementing-tail-based-sampling/tailsamplingprocessor-traces.png)

一个固定大小的队列和一个哈希表，二者合起来实现 trace data 的 LRU cache。同时 processor 会将所有进入到其中的 traces 按照每秒一个 batch 组织起来，内部一共维持 5 个 batch (`decision_wait`)。每隔一秒钟，将最老的 batch 取出来，对其中的 traces 分别判断是否符合采样规则，符合则将其传递给后续的 processors：

![tailsamplingprocessor-ticker](https://zhenghe-md.github.io/blog/2021/03/04/implementing-tail-based-sampling/tailsamplingprocessor-ticker.png)

如果在做采样决策时，发现相应的 trace 已经被 LRU cache 清出，则认为「trace dropped too early」，后者意味着 tailsamplingprocessor 已经超负荷。理论上这个指标如果不等于 0，尾部连贯采样功能就处于异常状态。

### 3.2 灰度方案

上文提到过，实施改造需要让 Jaeger sdk 全量上报 trace。由于「是否上报」这个决定在请求入口服务 (即 HTTP 服务) 做出，并会随着跨进程调用传播到下游服务，同时伴鱼服务端内部的入口服务已经按照业务拆分，因此灰度的过程就可以按入口服务进行，从流量小的、级别低入口服务开始上线观察，再逐渐加入流量大的、级别高的入口服务，最终默认打开全量采样，并在这个过程中发现、解决潜在问题。

### 3.3 资源消耗优化

新版架构所需资源与旧版差别不大：

- 存储：ES 索引
- 计算：部署在每个 k8s 节点上的 otel-agent，以及 otel-collector 集群 (新增)

在逐步上线到所有入口服务之前，我们做了比较充分的风险评估。开启全量采集后，主要增加了宿主机的网络 i/o，在千兆网卡 (约 300MB/s) 支持下，增加后的 i/o 量远远未达到瓶颈。实施过程中，业务团队也确实没有感知。不过在灰度上线过程中，我们也发现并解决了若干问题。

#### 3.3.1 热点服务问题

不同服务的请求量不同。个别服务的上报量过大会导致不同 otel-agent 的流量不均衡，在高峰期造成 otel-agent 的 CPU 经常超过预警线。我们通过增加热点服务实例，减小单个实例的请求量的方式，间接地均衡了每个 otel-agent 承载的流量。

#### 3.3.2 过滤下推

在生产环境中，我们默认维持最近 7 天的 trace 数据。在分析 ES 中索引 `jaeger-span-*` 的过程中，意料之中地，我们看到了 power law 的存在：

![](https://zhenghe-md.github.io/blog/2021/03/04/implementing-tail-based-sampling/span-distribution.png)

仔细分析可以发现，50% 以上的 span 都属于 `apolloConfigCenter.*`。熟悉 apollo 的研发应该知道，通常 apollo sdk 会通过长轮询的方式从元数据中心拉取配置信息，并缓存到本地，而服务在应用层获取配置都是通过本地缓存获取。因此实际上这里的所有 `apolloConfigCenter.*` 只是本地访问，而不是跨进程调用，span 数据价值比较低，可以忽略。于是我们开发了通过正则匹配 spanName 过滤 span 的 processor，并部署到 otel-agent 上，我们称之为过滤下推。部署上线后，ES 索引体积下降超过 50%，目前每天索引体积为 40-50 GB；otel-collector 和 otel-agent 的 CPU 消耗也降低了接近 50%。

### 3.4 制定 SLO

在伴鱼服务端内，线上问题排查的第一入口通常是 im 消息，报警平台会将导致报警的 traceID 以及日志注入到消息中，并提供一个链接页面，方便研发快速查看报警相关的调用链信息及其在整个调用链上每个服务打印的日志。基于此，我们制定了新版调用链追踪系统的 SLO：

|名称|研发关心的 trace 数据采集成功率|
|---|---|
|SLI 规范|研发关心且被采集的 trace 个数/研发关心的 trace 个数|
|SLI 实现|包含 trace 数据的服务报警信息条数/服务报警信息条数|
|查询|`sum(alertmanager_alert_total{trace_exists="true"})/sum(alertmanager_alert_total)`|
|目标|99%|

目前我们刚刚在报警平台中支持此 SLI 的埋点。目前还有个别服务尚未升级相关依赖，因此该指标尚不能反映所有服务的情况，我们会继续推动各服务全面升级，按照以上 SLO 来要求新版系统。

## 4. 小结

借助开源项目，我们得以通过花费极少的人力，解决当前伴鱼内部调用链追踪应用的稳态分析及异常检测需求，同时也为开源项目和社区做出微小的贡献。调用链追踪是可观测性平台的重要组件，未来我们将继续把一些精力放在 telemetry data 的整合上，为研发提供更全面、一致的服务观测分析体验。

# 七猫分布式追踪实践

[刘海港](https://tech.qimao.com/author/liu-hai-gang/) 发表于 2023/10/19

# 背景

Metrics（指标）、Logging（日志记录）、和Tracing（追踪）通常被称为可观测性的三大支柱。在微服务架构下，分布式追踪是一种关键工具，用于帮助排查和理解服务问题，它允许跟踪请求流程并提供关键的信息，以便更容易发现和解决问题。

随着服务数量的日益增加，承接全量的 Tracing 数据所需的资源越来越多。如果全量采集，链路中产生的span 所占的存储成本将会很高。目前生产环境采用0.1% 的采集率，存储成本较低。这种策略虽然很节省资源，但其缺点在一次次线上问题排查中逐渐暴露：

- 一个进程中包含多个接口：不论按固定概率采样还是限流采样，都会导致小流量接口一直采集不到调用链数据
- 线上服务出错是小概率事件，导致出错的请求被采中的概率更小，就导致采到的调用链信息量不大，引发问题的调用链却丢失的问题

OpenTelemetry 生态中提供了一些简单的手段应对上面的问题。

# OpenTelemetry

OpenTelemetry合并了OpenTracing和OpenCensus项目，提供了一组API和库来标准化遥测数据的采集和传输。OpenTelemetry提供了一个安全，厂商中立的工具，这样就可以按照需要将数据发往不同的后端。

OpenTelemetry项目由如下组件构成：

- 推动在所有项目中使用一致的规范
- 基于规范的，包含接口和实现的APIs
- 不同语言的SDK(APIs的实现)，如 Java, Python, Go, Erlang等
- Exporters：可以将数据发往一个选择的后端
- Collectors：厂商中立的实现，用于处理和导出遥测数据

# Tail Sampling Processor

为了解决这些问题，我们可以采取一种方法，即将所有生成的数据发送到一个中间平台，该平台会先进行暂存和清洗，然后再决定是否要保留或丢弃整个跟踪信息。具体操作如下图所示：当一个请求到达时，每个服务会将其生成的跟踪信息（Span）发送到一个名为OpenTelemetry Collector的中间组件。在Collector上有一个尾部采样组件，该组件会在接收到第一个Span后，等待一段时间（例如5秒），以便继续收集来自其他服务、具有相同Trace ID的Span。等待时间结束后，大量的Span会按照它们的Trace ID进行分类汇总，然后对属于同一个Trace ID的Span进行遍历，以检查是否包含错误信息，或者累计耗时是否超过了预设的阈值等。基于这些信息，可以有依据地筛选出高价值的跟踪信息，将它们纳入后续的处理流程。这种方法有助于更有效地处理跟踪数据，识别潜在问题，以及提高整体性能。

这种采样的模式称为**尾部采样**，它由 Collector 依据完整 Trace 的信息进行决策，决策的依据比头部采样丰富很多。但是由于需要承载大量临时数据，所以对基础设施要求很高。它的效果在于：

- 持久化的数据有依据、有规律、有价值；
- 减少了展示有价值数据所需的成本，例如存储资源，并且对提高查询速度也有帮助。

需要注意的是，在实际部署中，整个架构要做到高可用，往往会存在多个 Collector 节点，而同一个 Trace 的不同 Span 是由不同服务产生的，这些服务位于不同地方，尾部采样要求他们都落入相同的 Collector 节点，那么必然需要一层负载均衡架设在 Collector 之前，依照 Trace ID 进行转发。让 otel-agent 按照 traceID 做负载均衡，使用exporters 中的loadbalancing 组件。

![](https://tech.qimao.com/content/images/2023/10/11.png)

架构图如下：

pod 通过sdk 的方式上报到otel-agent，agent 的receivers使用otel grpc的方式接收trace；processors 使用 batch 来批量处理；exporters 使用 loadbalancing 方式根据traceID 负载到下游同一个collector。

otel-collector 由grpc 接收相同traceID的span，本地批量处理，基于这些信息根据 tail_sampling 配置规则决定是否上报到阿里云analysis平台。同时可配置不同的processor 处理不同数据导出到不同的平台。

![](https://tech.qimao.com/content/images/2023/10/12.png)

## 部署和配置

loadbalancing 采用k8s的方式部署，配置如下：

```
  exporters:
    loadbalancing:
      protocol:
        otlp:
          timeout: 5s
          sending_queue: # 发送队列
            enabled: true
            num_consumers: 50 # 消费者数量
            queue_size: 5000000 # 队列长度
          retry_on_failure:
            enabled: false # 是否重试
          tls:
            insecure: true
      resolver:
        k8s: # k8s service 方式获取pod 的IP 进行负载均衡
          service: otel-collector.observable
```

> otlp 中的队列，queue_size 这个参数指定了队列的最大容量，即可以在队列中缓存等待发送的数据点的最大数量。如果队列已满，新的数据点可能会被丢弃或被替代。这个参数可以用来控制队列的内存使用和数据传输的稳定性。

配置 retry_on_failure = true，增大queue_size值，会导致内存增加；后续根据请求的qps 和span 产生量，配置合理的queue_size。

![](https://tech.qimao.com/content/images/2023/10/13.png)

使用k8s 负载均衡方式，需要创建对应的账号权限

- 创建 service account

```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-loadbalancer
  namespace: observable
```

- 创建 role

```
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: otel-loadbalancer-role
  namespace: observable
rules:
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - list
      - watch
      - get
```

- 创建role binding

```
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: otel-loadbalancer-rolebinding
  namespace: observable
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: otel-loadbalancer-role
subjects:
  - kind: ServiceAccount
    name: otel-loadbalancer
    namespace: observable
```

关注的调用链全采样：研发在分析、排障过程中想查询的任何调用链都是重要调用链。总结以下优先级高场景：

- 在调用链上error 级别日志
- 整个调用链请求耗时超过 250ms

```
processors:
  tail_sampling:
    decision_wait: 5s # 等待5秒，超过5秒后的traceid将不再处理
    num_traces: 1500000 
    expected_new_traces_per_sec: 10 # 新增的trace 数量
    policies: # 上报规则策略
      [
        {
          name: error-policy,
          type: status_code, # 状态码，err
          status_code: { status_codes: [ ERROR ] }
        },
        {
          name: timeout-policy,
          type: latency, # 耗时，超过250ms 上报
          latency: { threshold_ms: 250 }
        }
      ]
```

> num_traces 推荐的计算规则：假设每秒有100 traces(不是span)，配置 decision_wait:5，则 num_traces=100*_5_*2=1000；num_traces 越大，对应的内存也会越大。

> decision_wait 是一个全链路的上报采集的等待时间，超时则丢弃后续traceID 的链路信息；请根据业务设置相应的超时时间，但是如果超时时间设置的特别大，会导致占有内存增加。

相关issues [https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/17275](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/17275)

## 效果

部分服务的有效链路追踪信息如下：

- 系统消息，每天产生的有效链路追踪只有2条，很容易处理对应的问题

![](https://tech.qimao.com/content/images/2023/10/14.png)

![](https://tech.qimao.com/content/images/2023/10/15.png)

- 其他服务，redis操作产生的error，都被全量采集

![](https://tech.qimao.com/content/images/2023/10/16.png)

# 资源使用

以下是官方给出的资源使用情况，[测试脚本](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/cmd/oteltestbedcol/main.go)。测试配置如下：

- FlushInterval(ms) [default: 1000] (刷新间隔 1秒)
- MaxQueueSize [default: 100] （最大队列100）
- SubmissionRate(spans/sec): 100,000 （每秒产生10w 的span）

## 头部采集法

![](https://tech.qimao.com/content/images/2023/10/image-70.png)

## 尾部采集法

![](https://tech.qimao.com/content/images/2023/10/image-71.png)

尾部采样需要更多的基础设施资源，这在内存上体现得比较明显；目前生产环境的资源消耗也是符合预期的。

## 本地压测

测试代码 [https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/testbed/tests/trace_test.go#L399](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/testbed/tests/trace_test.go#L399)，需要修改为tail sampling processor，默认是没有加载的；可以参考README 来实现。

![](https://tech.qimao.com/content/images/2023/10/11-1.png)

![](https://tech.qimao.com/content/images/2023/10/12-1.png)

## 生产资源使用情况

![](https://tech.qimao.com/content/images/2023/10/11-2.png)

otel-agent 的CPU 内存资源消耗：

![](https://tech.qimao.com/content/images/2023/10/11-3.png)

otel-collector 的CPU内存 资源消耗：

![](https://tech.qimao.com/content/images/2023/10/11-4.png)

320k/8k = 40CPU，目前collector使用了30CPU，优化的点在于使用k8s方式部署，每个pod的cpu被限制使用4C，没有配置automaxprocs，导致CPU使用40C左右，使用automaxprocs后优化了 `1C/pod` 左右。对比图如下：

![](https://tech.qimao.com/content/images/2023/10/11-5.png)

# 小结

借助开源项目，我们得以通过花费极少的人力，解决当前内部调用链追踪应用的稳态分析及异常检测需求，相比较头部全量采集的存储成本，我们使用OpenTelemetry tail-based sampling 的方式大大降低了成本，存储成本降了几十倍。调用链追踪是可观测性平台的重要组件，未来将继续把一些精力放在 telemetry data 的整合上，为研发提供更全面、一致的服务观测分析体验。


# Reference
https://zhenghe-md.github.io/blog/2020/12/20/design-dimensions-of-tracing-systems/
https://zhenghe-md.github.io/blog/2021/03/04/implementing-tail-based-sampling/
https://tech.qimao.com/qi-mao-fen-bu-shi-zhui-zong-shi-jian/

