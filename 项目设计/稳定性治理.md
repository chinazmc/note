# 下单稳定性治理 ｜ 得物技术

## **1** **为什么写这篇文章**

在工作期间，笔者有幸参与了下单链路的开发、维护工作，在这期间有经历下单从0到1的搭建，也有随着业务发展不得不进行系统重构的经验。“提交订单”这一词大家应该都是再熟悉不过了，不管你是不是软件研发人员，还是普通使用电商APP购买商品的用户，只要你在购买商品时必然会遇到。既然“提交订单”这么频繁的被使用到，作为任何电商APP来说，那么它的稳定性就尤为重要。

那么站在技术视角看下单链路，会发现几个特点

- 高QPS/TPS，流量大
- 订单数据正确性要求极高
- 监控告警时快速定位能力
- 结算页到订单创建成功的所见即所得
- 易被恶意流量刷单
- 依赖下游服务非常之多
- 业务逻辑很复杂

本篇文章就挑几个在日常研发中可能会遇到比较明显的问题，以及是怎么进行应对的。

## **2** **可能遇到的问题**

### **2.1  线上****告警****频繁，精准定位问题耗时较长**

告警机制，这个大家最熟悉不过的了，作为技术人的对这可以说是又爱又恨吧。即讨厌线上频繁告警的打扰，又担心真正发生告警时的定位难。常见的主流监控，Zabbix、Promethues、Open-Falcon等主要监控的指标还是以应用维度为主，主要监控指标如下。

- Dubbo接口：请求量、耗时、异常量。
- JVM ：GC次数、GC耗时、各个内存区域的大小、当前线程数、死锁线程数。
- 线程池：活跃线程数、任务队列大小、任务执行耗时、拒绝任务数。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74D0Low6p5eSO6m9uCy8VDt7ytnxEh7SqjH33RDEIKiajz2XdiaM9tSrVJmYaF2y2dk7sJ0Q83HAO7QA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

如图，类似于这种告警应该是比较熟悉的。那么这里的问题也很明显，下游接口异常到底影响的是哪个链路呢？针对这种特定业务场景，如订单结算页、提交订单，这类接口级别的监控又该怎么做呢？那首先简单介绍下在一次下单请求中可能遇到的问题

1. **下游接口调用告警**

1. 强依赖接口和业务可降级接口，怎么进行区分？  
2. 当告警来了，怎么确认是下单链路所依赖的接口呢？
3. 下游接口告警了，是预期内的业务异常还是非预期内的呢？


1. **接口rt&接口QPS抖动告警**

_**！**__由于热门商品、大促等活动节日的存在，所以下单链路会经常出现这类告警_
3. **AVG RT的下降，怎么识别是否正常？**
4. **QPS的突然升高，升高的原因是啥呢？到底是下单链路阻塞了导致用户一直重试，还是发生了抢购呢？**    
5. **依赖的中间件发生抖动告警**

1. 怎么快速感知是MQ、Redis、DB等的异常？

7. **应用自身出现异常告警**    

1. 普通业务异常：例如当前APP版本不支持XXX新业务，非法请求核心参数缺失
2. 非预期异常：新上线的业务代码整出了异常导致下单阻断
3. 怎么区分普通业务异常和非预期异常？
1. 普通业务异常：例如当前APP版本不支持XXX新业务，非法请求核心参数缺失
2. 非预期异常：新上线的业务代码整出了异常导致下单阻断

### **2.2  当购买期间商品信息发生变更，怎么保障用户的购买体验呢**

在用户购买东西时，首先会看到订单结算页面，这个上面会展示`商品价格`，`售后保障`，`到货时效`，`优惠信息`等，这时用户在确认条款后会提交订单，那么在订单生成后订单详情看到的理论是需要和在结算页看到的信息是完全一致的。但是由于结算页和提交订单是分开的请求，那么这个时间GAP以及实现差异终究可能会带来不一致的情况发生。如果是普通库存的话，给用户直接重新展示订单结算页也还行，要是抢购商品的话，那这个体验就会有比较大的影响。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74D0Low6p5eSO6m9uCy8VDt7wBQIfLsZx6Rrd3CeoXEqVgicTs6DLoctwuYD9XeR8cmk0FTJHYliceCA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### **2.3  依赖方数据返回不合法，该如何及时感知**

订单的数据是相当复杂的，需要依赖商品、库存、营销、商家等数据信息，不同的业务场景对生成的订单数据就会存在一定的要求。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74D0Low6p5eSO6m9uCy8VDt7fS1ykKqqZziarIMiblKkt9pjRIckWiaDoYCASibibaWnucFnTibAGREIpqaQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

那么这件事情的必要性，就在于可以在系统上线之前，通过回归测试及流量回放验证来及时发现是依赖方接口导致的问题还是自身系统代码bug带来的影响。

## **3** **解决方案**

那么问题来了，既然决定好好治理，那么怎么治理呢？怎么以最小的人力、技术成本实现这些治理呢？这个时候大量参考了现在同行业内针对下单场景稳定性相关的方案。现在就逐一介绍以上问题最终选择的解决方案。

### **3.1 自定义实现告警机制的基础日志数据埋点**

针对接口级的定制化告警，采用了自定义日志埋点的方式，格式如下：

{current_time}|{trace_id}|{span_id}| {function_name}|{rt}|{error_code}|{error_message}|{user_id}

- function_name：用来具体区分哪个接口
- error_code：接口错误码，用来唯一标识接口异常原因，重点就是这个，这个指标数据输出的精细程度决定了定位问题的速度
- rt：接口响应时间

这里简单画个图，直观的体现下需要关注下单链路中哪些指标

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74D0Low6p5eSO6m9uCy8VDt7U0vXtCzZ7icWRxT9g48D4CKCRrBUheFC9gSba2HFibIkJbOIcsAbD8ow/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

现在介绍一下每个指标的作用：

- **网关****QPS**：观察C端的实时入口流量
- **自身****服务****QPS**：观察到达服务本身的流量

- 网关QPS  > 自身QPS，可以考虑是否网关侧发生了限流
- 当自身QPS下降过高
- 网关QPS没什么波动，那么这个时候考虑网关问题
- 网关QPS也同步下降，前置导购链路流量问题，如商详/购买浮层 是否发生阻断性异常

- **自身业务****异常**：输出下单阻断的业务原因，又称为预期内异常
- **自身其它运行时****异常**：如NPE，称为非预期内异常，此时错误码会统一输出SYS_ERROR，一般此类会重点关注
- **下游****接口****RPC****异常**：此时会输出是下游哪个接口导致的阻断，如
- 商品查询接口超时 -> QEURY_SKU(RPC_TIMEOUT)
- 用户接口查询网络异常 -> QUERY_USER(_NETWORK_EXCEPTION_)

- **下游****接口****业务****异常****：**如
- 优惠已失效 -> CONSUME_DISCOUNT(INVALID)，这里会通过识别下游接口返回的code码来区分不同的业务异常，所以在日常需求中要求下游接口提供方确保返回码的含义就是这个原因
- 返回了未约定的code码，统一会返回如XXX(BIZ_ERR)，看到此类错误码的时候，就会及时反馈给下游服务Owner去跟进这个问题

- **中间件访问****异常**：
- SQL执行异常
- 网络连接RST异常
- **自身****服务****接口****AVG** **RT****/SUCCESS RT**

- 这里主要说一下SUCCESS RT，这个指标是可以最准确的反馈出最近RT是否存在波动
- **自身****服务****接口****AVG** **QPS****/SUCCESS QPS**

- 这里的success qps很重要，当发生抢购的时候，整体QPS会大幅上升，这个时候可以SUCCESS QPS来判断当前成单量是不是稳定
- 如果是浅库存抢购，这个指标不会有太大波动
- 接口被刷了，这个指标也不会有太大波动，且会出现OPERATION_TOO_FREQUENTLY频次限流错误码

**_！_**_通过将接口每次请求的埋点日志输出到指定文件中，后续经过监控组采集以及分析得到了如下几个主要的大盘：_

**1. 确认订单****&****创建订单****错误码大盘**

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74D0Low6p5eSO6m9uCy8VDt7gJ5O1EdmzmCZZgUtrdl8Pz25SKJC0EtPsepcsuGpkQOGvIZmBcs9bA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

从图中可很直观的发现当前有哪些原因导致的下单失败，如版本过低限制、库存售罄、下单频次过高等原因，这样就能很直观的发现

- 从异常名可以看出是有很明显业务语义的，这样便于大家理解
- 针对下游接口调用，会输出具体某个接口（也可以给对应接口定义别名）的某个类型错误，如优惠核销的超时、优惠已失效、优惠已使用

另外还设计了基于机器IP的过滤，这种做法的好处是，在发布过程中，如果下单出现了任何阻塞性异常，都可以很快的感知到，从而可以快速做到SOP响应处理。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74D0Low6p5eSO6m9uCy8VDt7nhDrhcSxKg4xv1jSojXR1GUa9sTyhfHK3xbA0JJNM0K21bcIibYVyiaQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

对于链路中的业务弱依赖接口，这里不会有错误码体现，这里依然还是借助于监告警机制。

**2. QPS****&****RT****指标****数据**

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74D0Low6p5eSO6m9uCy8VDt739z5cKF5eH5zRHMp0ZsN36cUIJCKDZ856agZgzTI8tnjoaDf3ycqwA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

这里主要日常监控观察主要会注重成功量QPS，特别是发布期间完全可以依赖于这些指标数据。例如发布期间这个时候在抢购，有了这个就能做到心中有数了。这里简单说明一下成功量就是接口业务执行成功的含义。

**3. 告警机制**

有了如上的这些指标数据，那么基于这些做告警机制就成了顺理成章的事情啦，目前已经有如下指标告警：
- 错误码环比涨幅超指定阈值
- 接口RT环比涨幅超指定阈值
- 接口成功量QPS环比下跌超过指定阈值

然后再将这些告警机制接入飞书、短信等通知，那么哪怕是在周末外出游玩的时间，有任何下单链路的异常告警，只需要打开手机看一眼就能快速定位到问题的根因所在了，岂不美哉？

以上就是针对下单告警机制的精细化处理了，除此之外，有了这些数据后，也对其它一些指标数据也进行了完善，如：
- 高频访问用户
- 不同入口的实时下单量
- 当前热门购买商品

### **3.2  基于版本号的商品信息&数据一致性校验**

**1. 商品价格变更**

商品改价这个在电商中应该是比较常见的，那么如果是在秒杀时改价，那么此时提示用户“商品价格”变更可能对用户的体感就没那么好。针对这类问题可以采用商品信息+版本号机制。

用户在订单结算页看到的商品数据版本会交由客户端携带至提交订单，此时提交订单可以校验该版本的生效时间是XX秒内，确保这个时间内订单提交不受改价影响，这样可以给到用户一个较好的购买体验。这个XX时间就需要业务来进行权衡了。

**2. 数据一致性校验**

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74D0Low6p5eSO6m9uCy8VDt7NUEQlpy110icJeOf5jvsicv8vnm016bpVyjibo8ueWTPl4licneK2AX4zw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

通过以上的UML图可以看到，由于确认订单和创单是两次请求，那么保证数据防篡改是第一要求，而且有了这个验签机制后，用户自己通过简单传参刷创单接口就变得更加困难了。对于迭代版本中新增生成sign的参数，这边主要采用version版本的方式，不同的version对应参与生成version的参数有所不同。

- version1，参数 a、b、c
- version2，参数a、b、c、d

有了防篡改的保障后，那么接下来就只需要在下单资源扣减之前，针对这些核心数据进行一致性校验即可，如订单金额、展示给用户的售后标签等等。这样的话在出现不一致时可以给到用户友好的提示，并且对可以及时进行告警通知。

### **3.3 订单数据正确性校验&及时告警机制**

一致性校验节点旨在创单落库节点前给恒久不变的规则（如：订单支付金额 = 应收金额 - 优惠 ）提供下单前的兜底校验及可选告警措施。不太适合落地多变的规则。如果是多变规则需要写到对应业务模块以异常形式告出。大家自行判断所属业务属于哪一种。

订单数据完整性校验致力于保障**订单在整个生命周期中数据的正确性**。为用户打造一站式的校验、预警解决方案。提供以下能力：

- 可插拔式接入
- 场景定制化
- 动态降级
- 规则、预警可扩展
- 统一流程处理

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74D0Low6p5eSO6m9uCy8VDt7BBicbDm19rNPKtpcEoD2SOQWPBnJStTC5t6AjwsI4GwGx2jKxdDTuvA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**适用的场景：**

1. 商家地址返回手机号存在掩码问题，必要数据缺失
2. 优惠接口在某种特定业务场景下未返回对应的优惠信息
3. 订单金额计算是否一致与用户看到的一致

## **4** **雨过天晴后的🌈**

1. ### **基于错误码大盘及监控机制的问题快速定位**
    ### ![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74D0Low6p5eSO6m9uCy8VDt7gXjbSMMHpJbx7VWDvE4L3NmajBDBoriackhubXYYvQySLWtDsHEtr7A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
    

- 核心接口全局监控，高灵敏度感知任何阻塞下单的问题
- 监控机制实时告警

|   |   |
|---|---|
|![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74D0Low6p5eSO6m9uCy8VDt7h3rUXLgxJ0g1iaNBu9Dlc6jgWdLmFTLxGria6NVU5PCcns8lDjibTWuJg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)|![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74D0Low6p5eSO6m9uCy8VDt7QIlE7HpAueib2ogwx7leUVUIsib8IUicM55M90rJ5pLdU96ec1dpvMlZQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)|

2. ### **下单链路一致性机制保障，所见即所得**

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74D0Low6p5eSO6m9uCy8VDt7ye6nSYKNcSjRIgicLN4gVHqBIib1Dq7mgnhZgl7ib0XCnzUGcoGTbiaupQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

3. ### **创单数据正确性兜底校验**

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74D0Low6p5eSO6m9uCy8VDt7INgQIojmeIjC02HW9U81UicXiaNaMljKu7P1tqpE00ibJT9xmDYQaOqpw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

## **5*  总结**

在下单的稳定性治理过程中，从面对线上告警的盲目无措，逐渐演进到面对日常迭代变更、突发流量场景的镇定自若。在日常工作中，持续关注、发现线上潜在的问题以及不合理的设计，然后尽量通过合理机制&实现来进行保障。作为一名研发人员，不能确保不犯错，但能尽最大努力及时发现错误，敬畏生产。几套打完收工，可以手握小茶壶，静看风波了。

# 浅谈系统稳定性与高可用保障的几种思路 ｜ 得物技术

原创 新一 得物技术 _2022-10-17 18:30_ _发表于上海_

## **一、前言**

高并发、高可用、高性能被称为互联网三高架构，这三者都是工程师和架构师在系统架构设计中必须考虑的因素之一。今天我们就来聊一聊三H中的高可用，也是我们常说的系统稳定性。

> 本篇文章只聊思路，没有太多的深入细节。阅读全文大概需要5~10分钟。

# **二、高可用的定义**

业界常用 N 个 9 来量化一个系统可用性程度，可以直接映射到网站正常运行时间的百分比上。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74B6pcMSyzAfl6EyVUxYHKNsrGm3dsAQdZJKEBRqAdibGDOfLfDn90vpXcTUChX10PDHiaTw5alnZBAg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

可用性的计算公式：

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74B6pcMSyzAfl6EyVUxYHKNs5NW8K2YaHbwiaoKQU81q11hg0dgXFick2IrK8XSn2K8k7MxUCyv0AVtA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)


大部分公司的要求是4个9，也就是年度宕机时长不能超过53分钟，实际要达到这个目标还是非常困难的，需要各个子模块相互配合。

要想提升一个系统的可用性，首先需要知道影响系统稳定性的因素有哪些。
# **三、影响稳定性的因素**

首先我们先梳理一下影响系统稳定性的一些常见的问题场景，大致可分为三类：
- **人为因素**
不合理的变更、外部攻击等等
- **软件因素**
代码bug、设计漏洞、GC问题、线程池异常、上下游异常
- **硬件因素**
网络故障、机器故障等

下面就是对症下药，**首先是故障前的预防，其次是故障后的快速恢复能力**，下面我们就聊聊几种常见的解决思路。
# **四、提升稳定性的几种思路**

## **4.1 系统拆分**

拆分不是以减少不可用时间为目的，而是以减少故障影响面为目的。因为一个大的系统拆分成了几个小的独立模块，一个模块出了问题不会影响到其他的模块，从而降低故障的影响面。系统拆分又包括接入层拆分、服务拆分、数据库拆分。
- **接入层&服务层**
一般是按照业务模块、重要程度、变更频次等维度拆分。

- **数据层**
一般先按照业务拆分后，如果有需要还可以做垂直拆分也就是数据分片、读写分离、数据冷热分离等。
## **4.2 解耦**

系统进行拆分之后，会分成多个模块。模块之间的依赖有强弱之分。如果是强依赖的，那么如果依赖方出问题了，也会受到牵连出问题。这时可以梳理整个流程的调用关系，做成弱依赖调用。弱依赖调用可以用MQ的方式来实现解耦。即使下游出现问题，也不会影响当前模块。
## **4.3 技术选型**
可以在适用性、优缺点、产品口碑、社区活跃度、实战案例、扩展性等多个方面进行全量评估，挑选出适合当前业务场景的中间件&数据库。前期的调研一定要充分，**先对比、测试、研究，再决定，磨刀不误砍柴工。**
## **4.4 冗余部署&故障自动转移**

服务层的冗余部署很好理解，一个服务部署多个节点，有了冗余之后还不够，每次出现故障需要人工介入恢复势必会增加系统的不可服务时间。所以，又往往是通过“自动故障转移”来实现系统的高可用。即某个节点宕机后需要能自动摘除上游流量，这些能力基本上都可以通过负载均衡的探活机制来实现。

涉及到数据层就比较复杂了，但是一般都有成熟的方案可以做参考。一般分为一主一从、一主多从、多主多从。不过大致的原理都是**数据同步实现多从，数据分片实现多主**，故障转移时都是**通过选举算法选出新的主节点**后在对外提供服务（这里如果写入的时候不做强一致同步，故障转移时会丢失一部分数据）。具体可以参考Redis Cluster、ZK、Kafka等集群架构。

## **4.5 容量评估**

在系统上线前需要对整个服务用到的机器、DB、cache都要做容量评估，机器容量的容量可以采用以下方式评估：
- 明确预期流量指标-QPS；
- 明确可接受的时延和安全水位指标（比如CPU%≤40%，核心链路RT≤50ms）；
- 通过压测评估单机在安全水位以下能支持的最高QPS（建议通过混合场景来验证，比如按照预估流量配比同时压测多个核心接口）；
- 最后就可以估算出具体的机器数量了。

DB和cache评估除了QPS之外还需要评估数据量，方法大致相同，等到系统上线后就可以根据监控指标做扩缩容了。

## **4.6 服务****快速扩容能力&泄洪能力**

现阶段不论是容器还是ECS，单纯的节点复制扩容是很容易的，扩容的重点需要评估的是服务本身是不是无状态的，比如：

- 下游DB的连接数最多支持当前服务扩容几台？
- 扩容后缓存是否需要预热？
- 放量策略

这些因素都是需要提前做好准备，整理出完备的SOP文档，当然最好的方式是进行演练，实际上手操作，有备无患。

泄洪能力一般是指冗余部署的情况下，选择几个节点作为备用节点，平时承担很小一部分流量，当流量洪峰来临时，通过调整流量路由策略把热节点的一部分流量转移到备用节点上。

对比扩容方案这种成本相对较高，但是好处就是**响应快，风险小。**
## **4.7 流量整形&熔断降级**

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74B6pcMSyzAfl6EyVUxYHKNsh1UxsZDfN0OyicoGe5RrucBGsMWISzQvhicNnYhspXhAJ8wAH2hYdv1w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

流量整形也就是常说的限流，主要是防止超过预期外的流量把服务打垮，熔断则是为了自身组件或者依赖下游故障时，可以快速失败防止长期阻塞导致雪崩。关于限流熔断的能力，开源组件Sentinel基本上都具备了，用起来也很简单方便，但是有一些点需要注意。

- 限流阈值一般是配置为服务的某个资源能支撑的最高水位，这个需要通过压测摸底来评估。随着系统的迭代，这个值可能是需要持续调整的。如果配置的过高，会导致系统崩溃时还没触发保护，配置的过低会导致误伤。
    
- 熔断降级-某个接口或者某个资源熔断后，要根据业务场景跟熔断资源的重要程度来评估应该抛出异常还是返回一个兜底结果。比如下单场景如果扣减库存接口发生熔断，由于扣减库存在下单接口是必要条件，所以熔断后只能抛出异常让整个链路失败回滚，如果是获取商品评论相关的接口发生熔断，那么可以选择返回一个空，不影响整个链路。

## **4.8资源隔离**

如果一个服务的多个下游同时出现阻塞，单个下游接口一直达不到熔断标准（比如异常比例跟慢请求比例没达到阈值），那么将会导致整个服务的吞吐量下降和更多的线程数占用，极端情况下甚至导致线程池耗尽。引入资源隔离后，可以限制单个下游接口可使用的最大线程资源，确保在未熔断前尽可能小的影响整个服务的吞吐量。

说到隔离机制，这里可以扩展说一下，由于每个接口的流量跟RT都不一样，很难去设置一个比较合理的可用最大线程数，并且随着业务迭代，这个阈值也难以维护。这里可以采用共享加独占来解决这个问题，每个接口有自己的独占线程资源，当独占资源占满后，使用共享资源，共享池在达到一定水位后，强制使用独占资源，排队等待。这种机制优点比较明显就是可以在资源利用最大化的同时保证隔离性。

**这里的线程数只是资源的一种，资源也可以是连接数、****内存****等等。**

## **4.9系统性保护**

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74B6pcMSyzAfl6EyVUxYHKNsTHRKJeHXsPbyfEc1O2W1F2kHBLZpUQ2Xm62AtccLsl8Yicf65glgw9A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

系统性保护是一种无差别限流，一句话概念就是在系统快要崩溃之前对所有流量入口进行无差别限流，当系统恢复到健康水位后停止限流。具体一点就是结合应用的 Load、总体平均 RT、入口 QPS 和线程数等几个维度的监控指标，让系统的入口流量和系统的负载达到一个平衡，让系统尽可能跑在最大吞吐量的同时保证系统整体的稳定性。

## **4.10 可观测性&告警**

![图片](https://mmbiz.qpic.cn/mmbiz_png/AAQtmjCc74B6pcMSyzAfl6EyVUxYHKNs7oYRsnF6xXWz1CDdl2giasZfStgBCrF1g8umBIHYz709my3Rp0RWn6w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

当系统出现故障时，我们首先需找到故障的原因，然后才是解决问题，最后让系统恢复。排障的速度很大程度上决定了整个故障恢复的时长，而可观测性的最大价值在于快速排障。其次基于Metrics、Traces、Logs三大支柱配置告警规则，可以提前发现系统可能存在的风险&问题，避免故障的发生。

## **4.11 变更流程三板斧**

变更是可用性最大的敌人，99%的故障都是来自于变更，可能是配置变更，代码变更，机器变更等等。那么如何减少变更带来的故障呢？
- **可灰度**
用小比例的一部分流量来验证变更后的内容，减小影响用户群。

- **可回滚**

出现问题后，能有有效的回滚机制。涉及到数据修改的，发布后会引起脏数据的写入，需要有可靠的回滚流程，保证脏数据的清除。
- **可观测**
通过观察变更前后的指标变化，很大程度上可以提前发现问题。

除了以上三板斧外，还应该在其他开发流程上做规范，比如代码控制，集成编译、自动化测试、静态代码扫描等。
# **五、总结**

对于一个动态演进的系统而言，我们没有办法将故障发生的概率降为0，能做的只有尽可能的预防和缩短故障时的恢复时间。当然我们也不用一味的追求可用性，毕竟提升稳定性的同时，维护成本、机器成本等也会跟着上涨，所以需要结合系统的业务SLO要求，适合的才是最好的。

如何做好稳定性和高可用保障是一个很庞大的命题，本篇文章没有太多的深入细节，只聊了整体的一些思路，主要是为了大家在以后的系统高可用建设过程中，有一套系统的框架可以参考。最后感谢耐心看完的同学。

 # Reference
https://mp.weixin.qq.com/s/-4JrbkutRdFB2-RyrBzSeg
https://mp.weixin.qq.com/s/eHb5AIXANR_lDdggcq67-w