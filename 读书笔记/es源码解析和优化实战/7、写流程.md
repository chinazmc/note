#es 

本章分析ES写入单个和批量文档写请求的处理流程，仅限于ES内部实现，并不涉及Lucene内部处理。在ES中，写入单个文档的请求称为Index请求，批量写入的请求称为Bulk请求。写单个和多个文档使用相同的处理逻辑，请求被统一封装为BulkRequest。在分析写流程时，我们把流程按不同节点执行的操作进行划分。写请求的例子可以参考上一章.

# 文档操作的定义
在ES中，对文档的操作有下面几种类型：
```text
enum OpType {
	INDEX（0）,
	CREATE（1）,
	UPDATE（2）,
	DELETE（3）;
}
```
· INDEX：向索引中“put”一个文档的操作称为“索引”一个文档。此处“索引”为动词。
· CREATE:put 请求可以通过 op_type 参数设置操作类型为 create，在这种操作下，如果文档已存在，则请求将失败。
· UPDATE：默认情况下，“put”一个文档时，如果文档已存在，则更新它。
· DELETE：删除文档。
在put API中，通过op_type参数来指定操作类型。
![[Pasted image 20220321152904.png]]

# Index/Bulk基本流程
新建、索引（这里的索引是动词，指写入操作，将文档添加到Lucene的过程称为索引一个文档）和删除请求都是写操作。写操作必须先在主分片执行成功后才能复制到相关的副分片。
写单个文档的流程（图片来自官网）如下图所示。
![[Pasted image 20220321153230.png]]
以下是写单个文档所需的步骤：
（1）客户端向NODE1发送写请求。
（2）NODE1使用文档ID来确定文档属于分片0，通过集群状态中的内容路由表信息获知分片0的主分片位于NODE3，因此请求被转发到NODE3上。
（3）NODE3上的主分片执行写操作。如果写入成功，则它将请求并行转发到 NODE1和NODE2的副分片上，等待返回结果。当所有的副分片都报告成功，NODE3将向协调节点报告成功，协调节点再向客户端报告成功。
在客户端收到成功响应时，意味着写操作已经在主分片和所有副分片都执行完成。
写一致性的默认策略是quorum，即多数的分片（其中分片副本可以是主分片或副分片）在写入操作时处于可用状态。
quorum = int（ （primary +number_of_replicas） / 2 ） + 1


使用 bulk 修改多个文档步骤顺序：
![[Pasted image 20230406112430.png]]
1.  客户端向 Node 1 发送 bulk 请求。
2.  Node 1 为每个节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机。
3.  主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。 一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端

# 整体的索引流程

> 先看下整体的索引流程
![[Pasted image 20230406112650.png]]
-   协调节点默认使用文档ID参与计算（也支持通过routing），以便为路由提供合适的分片。

```
shard = hash(document_id) % (num_of_primary_shards)
```

-   当分片所在的节点接收到来自协调节点的请求后，会将请求写入到Memory Buffer，然后定时（默认是每隔1秒）写入到Filesystem Cache，这个从Momery Buffer到Filesystem Cache的过程就叫做refresh；
-   当然在某些情况下，存在Momery Buffer和Filesystem Cache的数据可能会丢失，ES是通过translog的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到translog中，当Filesystem cache中的数据写入到磁盘中时，才会清除掉，这个过程叫做flush。
-   在flush过程中，内存中的缓冲将被清除，内容被写入一个新段，段的fsync将创建一个新的提交点，并将内容刷新到磁盘，旧的translog将被删除并开始一个新的translog。 flush触发的时机是定时触发（默认30分钟）或者translog变得太大（默认为512M）时。

## 分步骤看数据持久化过程

> **通过分步骤看数据持久化过程**：**write -> refresh -> flush -> merge**

-   **write 过程**
![[Pasted image 20230406112948.png]]
一个新文档过来，会存储在 in-memory buffer 内存缓存区中，顺便会记录 Translog（Elasticsearch 增加了一个 translog ，或者叫事务日志，在每一次对 Elasticsearch 进行操作时均进行了日志记录）。

这时候数据还没到 segment ，是搜不到这个新文档的。数据只有被 refresh 后，才可以被搜索到。

-   **refresh 过程**
![[Pasted image 20230406113009.png]]
refresh 默认 1 秒钟，执行一次上图流程。ES 是支持修改这个值的，通过 index.refresh_interval 设置 refresh （冲刷）间隔时间。refresh 流程大致如下：

1.  in-memory buffer 中的文档写入到新的 segment 中，但 segment 是存储在文件系统的缓存中。此时文档可以被搜索到
2.  最后清空 in-memory buffer。注意: Translog 没有被清空，为了将 segment 数据写到磁盘
3.  文档经过 refresh 后， segment 暂时写到文件系统缓存，这样避免了性能 IO 操作，又可以使文档搜索到。refresh 默认 1 秒执行一次，性能损耗太大。一般建议稍微延长这个 refresh 时间间隔，比如 5 s。因此，ES 其实就是准实时，达不到真正的实时。

-   **flush 过程**

每隔一段时间—​例如 translog 变得越来越大—​索引被刷新（flush）；一个新的 translog 被创建，并且一个全量提交被执行
![[Pasted image 20230406113039.png]]
上个过程中 segment 在文件系统缓存中，会有意外故障文档丢失。那么，为了保证文档不会丢失，需要将文档写入磁盘。那么文档从文件缓存写入磁盘的过程就是 flush。写入磁盘后，清空 translog。具体过程如下：

1.  所有在内存缓冲区的文档都被写入一个新的段。
2.  缓冲区被清空。
3.  一个Commit Point被写入硬盘。
4.  文件系统缓存通过 fsync 被刷新（flush）。
5.  老的 translog 被删除。

-   **merge 过程**

由于自动刷新流程每秒会创建一个新的段 ，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。 每一个段都会消耗文件句柄、内存和cpu运行周期。更重要的是，每个搜索请求都必须轮流检查每个段；所以段越多，搜索也就越慢。

Elasticsearch通过在后台进行Merge Segment来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。

当索引的时候，刷新（refresh）操作会创建新的段并将段打开以供搜索使用。合并进程选择一小部分大小相似的段，并且在后台将它们合并到更大的段中。这并不会中断索引和搜索。
![[Pasted image 20230406113057.png]]
一旦合并结束，老的段被删除：

1.  新的段被刷新（flush）到了磁盘。 ** 写入一个包含新段且排除旧的和较小的段的新提交点。
2.  新的段被打开用来搜索。
3.  老的段被删除。
![[Pasted image 20230406113131.png]]
合并大的段需要消耗大量的I/O和CPU资源，如果任其发展会影响搜索性能。Elasticsearch在默认情况下会对合并流程进行资源限制，所以搜索仍然 有足够的资源很好地执行。

# 写操作的关键点

在考虑或分析一个分布式系统的写操作时，一般需要从下面几个方面考虑：

-   可靠性：或者是持久性，数据写入系统成功后，数据不会被回滚或丢失。
-   一致性：数据写入成功后，再次查询时必须能保证读取到最新版本的数据，不能读取到旧数据。
-   原子性：一个写入或者更新操作，要么完全成功，要么完全失败，不允许出现中间状态。
-   隔离性：多个写入操作相互不影响。
-   实时性：写入后是否可以立即被查询到。
-   性能：写入性能，吞吐量到底怎么样。

Elasticsearch作为分布式系统，也需要在写入的时候满足上述的四个特点，我们在后面的写流程介绍中会涉及到上述四个方面。

接下来,我们一层一层剖析Elasticsearch内部的写机制。
# Lucene的写

众所周知，Elasticsearch内部使用了Lucene完成索引创建和搜索功能，Lucene中写操作主要是通过IndexWriter类实现，IndexWriter提供三个接口：

```
 public long addDocument();
 public long updateDocuments();
 public long deleteDocuments();
```

通过这三个接口可以完成单个文档的写入，更新和删除功能，包括了分词，倒排创建，正排创建等等所有搜索相关的流程。只要Doc通过IndesWriter写入后，后面就可以通过IndexSearcher搜索了，看起来功能已经完善了，但是仍然有一些问题没有解：

-   上述操作是单机的，而不是我们需要的分布式。
-   文档写入Lucene后并不是立即可查询的，需要生成完整的Segment后才可被搜索，如何保证实时性？
-   Lucene生成的Segment是在内存中，如果机器宕机或掉电后，内存中的Segment会丢失，如何保证数据可靠性 ？
-   Lucene不支持部分文档更新，但是这又是一个强需求，如何支持部分更新？

上述问题，在Lucene中是没有解决的，那么就需要Elasticsearch中解决上述问题。

我们再来看Elasticsearch中的写机制。

# Elasticsearch的写

Elasticsearch采用多Shard方式，通过配置routing规则将数据分成多个数据子集，每个数据子集提供独立的索引和搜索功能。当写入文档的时候，根据routing规则，将文档发送给特定Shard中建立索引。这样就能实现分布式了。

此外，Elasticsearch整体架构上采用了一主多副的方式：
![[Pasted image 20230406113444.png]]
每个Index由多个Shard组成，每个Shard有一个主节点和多个副本节点，副本个数可配。但每次写入的时候，写入请求会先根据_routing规则选择发给哪个Shard，Index Request中可以设置使用哪个Filed的值作为路由参数，如果没有设置，则使用Mapping中的配置，如果mapping中也没有配置，则使用_id作为路由参数，然后通过_routing的Hash值选择出Shard（在OperationRouting类中），最后从集群的Meta中找出出该Shard的Primary节点。

请求接着会发送给Primary Shard，在Primary Shard上执行成功后，再从Primary Shard上将请求同时发送给多个Replica Shard，请求在多个Replica Shard上执行成功并返回给Primary Shard后，写入请求执行成功，返回结果给客户端。

这种模式下，写入操作的延时就等于latency = Latency(Primary Write) + Max(Replicas Write)。只要有副本在，写入延时最小也是两次单Shard的写入时延总和，写入效率会较低，但是这样的好处也很明显，避免写入后，单机或磁盘故障导致数据丢失，在数据重要性和性能方面，一般都是优先选择数据，除非一些允许丢数据的特殊场景。

采用多个副本后，避免了单机或磁盘故障发生时，对已经持久化后的数据造成损害，但是Elasticsearch里为了减少磁盘IO保证读写性能，一般是每隔一段时间（比如5分钟）才会把Lucene的Segment写入磁盘持久化，对于写入内存，但还未Flush到磁盘的Lucene数据，如果发生机器宕机或者掉电，那么内存中的数据也会丢失，这时候如何保证？

对于这种问题，Elasticsearch学习了数据库中的处理方式：增加CommitLog模块，Elasticsearch中叫TransLog。
![[Pasted image 20230406113835.png]]
在每一个Shard中，写入流程分为两部分，先写入Lucene，再写入TransLog。

写入请求到达Shard后，先写Lucene文件，创建好索引，此时索引还在内存里面，接着去写TransLog，写完TransLog后，刷新TransLog数据到磁盘上，写磁盘成功后，请求返回给用户。这里有几个关键点:

-   一是和数据库不同，数据库是先写CommitLog，然后再写内存，而Elasticsearch是先写内存，最后才写TransLog，一种可能的原因是Lucene的内存写入会有很复杂的逻辑，很容易失败，比如分词，字段长度超过限制等，比较重，为了避免TransLog中有大量无效记录，减少recover的复杂度和提高速度，所以就把写Lucene放在了最前面。
-   二是写Lucene内存后，并不是可被搜索的，需要通过Refresh把内存的对象转成完整的Segment后，然后再次reopen后才能被搜索，一般这个时间设置为1秒钟，导致写入Elasticsearch的文档，最快要1秒钟才可被从搜索到，所以Elasticsearch在搜索方面是NRT（Near Real Time）近实时的系统。
-   三是当Elasticsearch作为NoSQL数据库时，查询方式是GetById，这种查询可以直接从TransLog中查询，这时候就成了RT（Real Time）实时系统。四是每隔一段比较长的时间，比如30分钟后，Lucene会把内存中生成的新Segment刷新到磁盘上，刷新后索引文件已经持久化了，历史的TransLog就没用了，会清空掉旧的TransLog。

上面介绍了Elasticsearch在写入时的两个关键模块，Replica和TransLog，接下来，我们看一下Update流程：
![[Pasted image 20230406113949.png]]
Lucene中不支持部分字段的Update，所以需要在Elasticsearch中实现该功能，具体流程如下：

-   收到Update请求后，从Segment或者TransLog中读取同id的完整Doc，记录版本号为V1。
-   将版本V1的全量Doc和请求中的部分字段Doc合并为一个完整的Doc，同时更新内存中的VersionMap。获取到完整Doc后，Update请求就变成了Index请求。 加锁。
-   再次从versionMap中读取该id的最大版本号V2，如果versionMap中没有，则从Segment或者TransLog中读取，这里基本都会从versionMap中获取到。
-   检查版本是否冲突(V1\==V2)，如果冲突，则回退到开始的“Update doc”阶段，重新执行。如果不冲突，则执行最新的Add请求。
-   在Index Doc阶段，首先将Version + 1得到V3，再将Doc加入到Lucene中去，Lucene中会先删同id下的已存在doc id，然后再增加新Doc。写入Lucene成功后，将当前V3更新到versionMap中。
-   释放锁，部分更新的流程就结束了。

介绍完部分更新的流程后，大家应该从整体架构上对Elasticsearch的写入有了一个初步的映象，接下来我们详细剖析下写入的详细步骤。

# Elasticsearch写入请求类型

> Elasticsearch中的写入请求类型，主要包括下列几个：Index(Create)，Update，Delete和Bulk，其中前3个是单文档操作，后一个Bulk是多文档操作，其中Bulk中可以包括Index(Create)，Update和Delete。

在6.0.0及其之后的版本中，前3个单文档操作的实现基本都和Bulk操作一致，甚至有些就是通过调用Bulk的接口实现的。估计接下来几个版本后，Index(Create)，Update，Delete都会被当做Bulk的一种特例化操作被处理。这样，代码和逻辑都会更清晰一些。

下面，我们就以Bulk请求为例来介绍写入流程。
![[Pasted image 20230406114934.png]]
-   红色：Client Node。
-   绿色：Primary Node。
-   蓝色：Replica Node。

##  Client Node

> Client Node 也包括了前面说过的Parse Request，这里就不再赘述了，接下来看一下其他的部分。

1.  **Ingest Pipeline**

在这一步可以对原始文档做一些处理，比如HTML解析，自定义的处理，具体处理逻辑可以通过插件来实现。在Elasticsearch中，由于Ingest Pipeline会比较耗费CPU等资源，可以设置专门的Ingest Node，专门用来处理Ingest Pipeline逻辑。

如果当前Node不能执行Ingest Pipeline，则会将请求发给另一台可以执行Ingest Pipeline的Node。

2.  **Auto Create Index**

判断当前Index是否存在，如果不存在，则需要自动创建Index，这里需要和Master交互。也可以通过配置关闭自动创建Index的功能。

3.  **Set Routing**

设置路由条件，如果Request中指定了路由条件，则直接使用Request中的Routing，否则使用Mapping中配置的，如果Mapping中无配置，则使用默认的_id字段值。

在这一步中，如果没有指定id字段，则会自动生成一个唯一的_id字段，目前使用的是UUID。

4.  **Construct BulkShardRequest**

由于Bulk Request中会包括多个(Index/Update/Delete)请求，这些请求根据routing可能会落在多个Shard上执行，这一步会按Shard挑拣Single Write Request，同一个Shard中的请求聚集在一起，构建BulkShardRequest，每个BulkShardRequest对应一个Shard。

5.  **Send Request To Primary**

这一步会将每一个BulkShardRequest请求发送给相应Shard的Primary Node。

## Primary Node

> Primary 请求的入口是在PrimaryOperationTransportHandler的messageReceived，我们来看一下相关的逻辑流程。

1.  **Index or Update or Delete**

循环执行每个Single Write Request，对于每个Request，根据操作类型(CREATE/INDEX/UPDATE/DELETE)选择不同的处理逻辑。

其中，Create/Index是直接新增Doc，Delete是直接根据_id删除Doc，Update会稍微复杂些，我们下面就以Update为例来介绍。

2.  **Translate Update To Index or Delete**

这一步是Update操作的特有步骤，在这里，会将Update请求转换为Index或者Delete请求。首先，会通过GetRequest查询到已经存在的同_id Doc（如果有）的完整字段和值（依赖_source字段），然后和请求中的Doc合并。同时，这里会获取到读到的Doc版本号，记做V1。

3.  **Parse Doc**

这里会解析Doc中各个字段。生成ParsedDocument对象，同时会生成uid Term。在Elasticsearch中，\_uid = type # \_id，对用户，\_Id可见，而Elasticsearch中存储的是_uid。这一部分生成的ParsedDocument中也有Elasticsearch的系统字段，大部分会根据当前内容填充，部分未知的会在后面继续填充ParsedDocument。

4.  **Update Mapping**

Elasticsearch中有个自动更新Mapping的功能，就在这一步生效。会先挑选出Mapping中未包含的新Field，然后判断是否运行自动更新Mapping，如果允许，则更新Mapping。

5.  **Get Sequence Id and Version**

由于当前是Primary Shard，则会从SequenceNumber Service获取一个sequenceID和Version。SequenceID在Shard级别每次递增1，SequenceID在写入Doc成功后，会用来初始化LocalCheckpoint。Version则是根据当前Doc的最大Version递增1。

6.  **Add Doc To Lucene**

这一步开始的时候会给特定_uid加锁，然后判断该_uid对应的Version是否等于之前Translate Update To Index步骤里获取到的Version，如果不相等，则说明刚才读取Doc后，该Doc发生了变化，出现了版本冲突，这时候会抛出一个VersionConflict的异常，该异常会在Primary Node最开始处捕获，重新从“Translate Update To Index or Delete”开始执行。

如果Version相等，则继续执行，如果已经存在同id的Doc，则会调用Lucene的UpdateDocument(uid, doc)接口，先根据uid删除Doc，然后再Index新Doc。如果是首次写入，则直接调用Lucene的AddDocument接口完成Doc的Index，AddDocument也是通过UpdateDocument实现。

这一步中有个问题是，如何保证Delete-Then-Add的原子性，怎么避免中间状态时被Refresh？答案是在开始Delete之前，会加一个Refresh Lock，禁止被Refresh，只有等Add完后释放了Refresh Lock后才能被Refresh，这样就保证了Delete-Then-Add的原子性。

Lucene的UpdateDocument接口中就只是处理多个Field，会遍历每个Field逐个处理，处理顺序是invert index，store field，doc values，point dimension，后续会有文章专门介绍Lucene中的写入。

7.  **Write Translog**

写完Lucene的Segment后，会以keyvalue的形式写TransLog，Key是_id，Value是Doc内容。当查询的时候，如果请求是GetDocByID，则可以直接根据_id从TransLog中读取到，满足NoSQL场景下的实时性要去。

需要注意的是，这里只是写入到内存的TransLog，是否Sync到磁盘的逻辑还在后面。

这一步的最后，会标记当前SequenceID已经成功执行，接着会更新当前Shard的LocalCheckPoint。

8.  **Renew Bulk Request**

这里会重新构造Bulk Request，原因是前面已经将UpdateRequest翻译成了Index或Delete请求，则后续所有Replica中只需要执行Index或Delete请求就可以了，不需要再执行Update逻辑，一是保证Replica中逻辑更简单，性能更好，二是保证同一个请求在Primary和Replica中的执行结果一样。

9.  **Flush Translog**

这里会根据TransLog的策略，选择不同的执行方式，要么是立即Flush到磁盘，要么是等到以后再Flush。Flush的频率越高，可靠性越高，对写入性能影响越大。

10.  **Send Requests To Replicas**

这里会将刚才构造的新的Bulk Request并行发送给多个Replica，然后等待Replica的返回，这里需要等待所有Replica返回后（可能有成功，也有可能失败），Primary Node才会返回用户。如果某个Replica失败了，则Primary会给Master发送一个Remove Shard请求，要求Master将该Replica Shard从可用节点中移除。

这里，同时会将SequenceID，PrimaryTerm，GlobalCheckPoint等传递给Replica。

发送给Replica的请求中，Action Name等于原始ActionName + [R]，这里的R表示Replica。通过这个[R]的不同，可以找到处理Replica请求的Handler。

11.  **Receive Response From Replicas**

Replica中请求都处理完后，会更新Primary Node的LocalCheckPoint。

## Replica Node

> Replica 请求的入口是在ReplicaOperationTransportHandler的messageReceived，我们来看一下相关的逻辑流程。

1.  **Index or Delete**

根据请求类型是Index还是Delete，选择不同的执行逻辑。这里没有Update，是因为在Primary Node中已经将Update转换成了Index或Delete请求了。

2.  **Parse Doc**
    
3.  **Update Mapping**
    

以上都和Primary Node中逻辑一致。

4.  **Get Sequence Id and Version**

Primary Node中会生成Sequence ID和Version，然后放入ReplicaRequest中，这里只需要从Request中获取到就行。

5.  **Add Doc To Lucene**

由于已经在Primary Node中将部分Update请求转换成了Index或Delete请求，这里只需要处理Index和Delete两种请求，不再需要处理Update请求了。比Primary Node会更简单一些。

6.  **Write Translog**
    
7.  **Flush Translog**
    

以上都和Primary Node中逻辑一致。


# Index/Bulk详细流程
以不同角色节点执行的任务整理流程如下图所示。
![[Pasted image 20220321153918.png]]
下面分别讨论各个节点上执行的流程。

## 协调节点流程
协调节点负责创建索引、转发请求到主分片节点、等待响应、回复客户端。
实现位于TransportBulkAction。执行本流程的线程池：http_server_worker。
### 1． 参数检查
如同我们平常设计的任何一个对外服务的接口处理一样，收到用户请求后首先检测请求的合法性，把检查操作放在处理流程的第一步，有问题就直接拒绝，对异常请求的处理代价是最小的。
### 2． 处理pipeline请求
数据预处理（ingest）工作通过定义pipeline和processors实现。pipeline是一系列processors的定义，processors按照声明的顺序执行。
如果Index或Bulk请求中指定了pipeline参数，则先使用相应的pipeline进行处理。如果本节点不具备预处理资格，则将请求随机转发到其他具备预处理资格的节点。预处理节点资格的配置参考第1章中的节点角色。
### 3． 自动创建索引
如果配置为允许自动创建索引（默认允许），则计算请求中涉及的索引，可能有多个，其中有哪些索引是不存在的，然后创建它。如果部分索引创建失败，则涉及创建失败索引的请求被标记为失败。其他索引正常执行写流程。
创建索引请求被发送到Master节点，待收到全部创建请求的Response（无论成功还是失败的）之后，才进入下一个流程。Master节点什么时候返回Response？在Master节点执行完创建索引流程，将新的clusterState发布完毕才会返回。那什么才算发布完毕呢？默认情况下，Master发布clusterState的Request收到半数以上的节点Response，认为发布成功。负责写数据的节点会先执行一遍内容路由的过程以处理没有收到最新clusterState的情况。

### 4． 对请求的预先处理
这里不同于对数据的预处理，对请求的预先处理只是检查参数、自动生成id、处理routing等。由于上一步可能有创建索引操作，所以在此先获取最新集群状态信息。然后遍历所有请求，从集群状态中获取对应索引的元信息，检查mapping、routing、id等信息。如果id不存在，则生成一个UUID作为文档id。

### 5． 检测集群状态
协调节点在开始处理时会先检测集群状态，若集群异常则取消写入。例如，Master节点不存在，会阻塞等待Master节点直至超时。
因此索引为Red时，如果Master节点存在，则数据可以写到正常shard,Master节点不存在，协调节点会阻塞等待或取消写入。
### 6． 内容路由，构建基于shard的请求
将用户的 bulkRequest 重新组织为基于 shard 的请求列表。例如，原始用户请求可能有10个写操作，如果这些文档的主分片都属于同一个，则写请求被合并为1个。所以这里本质上是合并请求的过程。此处尚未确定主分片节点。
基于shard的请求结构如下：
```java
Map<ShardId, List<BulkItemRequest>> requestsByShard = new HashMap<>（）；
```
根据路由算法计算某文档属于哪个分片。遍历所有的用户请求，重新封装后添加到上述map结构。ShardId类的主要结构如下，shard编号是从0开始递增的序号
![[Pasted image 20220321161219.png]]

### 7.路由算法
路由算法就是根据routing和文档id计算目标shardid的过程。一般情况下，路由计算方式为下面的公式：shard_num = hash（\_routing） % num_primary_shards

默认情况下，\_routing值就是文档id。ES使用随机id和Hash算法来确保文档均匀地分配给分片。当使用自定义id或routing时， id 或routing 值可能不够随机，造成数据倾斜，部分分片过大。在这种情况下，可以使用index.routing_partition_size 配置来减少倾斜的风险。routing_partition_size 越大，数据的分布越均匀。
在设置了index.routing_partition_size的情况下，计算公式为：shard_num = （hash（\_routing） + hash（\_id） % routing_partition_size） % num_primary_shards

也就是说，\_routing字段用于计算索引中的一组分片，然后使用_id来选择该组内的分片。index.routing_partition_size取值应具有大于1且小于index.number_of_shards的值。

### 8.转发请求并等待响应
主要是根据集群状态中的内容路由表确定主分片所在节点，转发请求并等待响应。
遍历所有需要写的 shard，将位于某个 shard 的请求封装为 BulkShardRequest 类，调用TransportShardBulkAction#execute执行发送，在listener中等待响应，每个响应也是以shard为单位的。如果某个shard的响应中部分doc写失败了，则将异常信息填充到Response中，整体请求做成功处理。
待收到所有响应后（无论成功还是失败的），回复给客户端。
转发前先获取最新集群状态，根据集群状态中的内容路由表找到目的shard所在的主分片，如果主分片不在本机，则转发到相应的节点，否则在本地执行。

## 主分片节点流程
执行本流程的线程池：bulk。主分片所在节点负责在本地写主分片，写成功后，转发写副本片请求，等待响应，回复协调节点。
### 1.检查请求
主分片所在节点收到协调节点发来的请求后也是先做了校验工作，主要检测要写的是否是主分片，AllocationId（后续章节会介绍）是否符合预期，索引是否处于关闭状态等。
### 2.是否延迟执行
判断请求是否需要延迟执行，如果需要延迟则放入队列，否则继续下面的流程。
### 3.判断主分片是否已经发生迁移
如果已经发生迁移，则转发请求到迁移的节点。
### 4.检测写一致性
在开始写之前，检测本次写操作涉及的shard，活跃shard数量是否足够，不足则不执行写入。默认为1，只要主分片可用就执行写入。
### 5.写Lucene和事务日志
遍历请求，处理动态更新字段映射，然后调用InternalEngine#index逐条对doc进行索引。
Engine封装了Lucene和translog的调用，对外提供读写接口。
在写入Lucene之前，先生成Sequence Number和Version。这些都是在InternalEngine类中实现的。Sequence Number每次递增1,Version根据当前doc的最大版本加1。
索引过程为先写Lucene，后写translog。
因为Lucene写入时对数据有检查，写操作可能会失败。如果先写translog，写入Lucene 时失败，则还需要对translog进行回滚处理。
### 6.flush translog
根据配置的translog flush策略进行刷盘控制，定时或立即刷盘
### 7.写副分片
现在已经为要写的副本shard准备了一个列表，循环处理每个shard，跳过unassigned状态的shard，向目标节点发送请求，等待响应。这个过程是异步并行的。
转发请求时会将SequenceID、PrimaryTerm、GlobalCheckPoint、version等传递给副分片。
在等待Response的过程中，本节点发出了多少个Request，就要等待多少个Response。无论这些Response是成功的还是失败的，直到超时。收集到全部的Response后，执行finish（）。给协调节点返回消息，告知其哪些成功、哪些失败了。
### 8.处理副分片写失败情况
主分片所在节点将发送一个shardFailed请求给Master
然后Master会更新集群状态，在新的集群状态中，这个shard将：
· 从in_sync_allocations列表中删除；
· 在routing_table的shard列表中将state由STARTED更改为UNASSIGNED；
· 添加到routingNodes的unassignedShards列表。

## I/O异常处理
（1）如果请求在协调节点的路由阶段失败，则会等待集群状态更新，拿到更新后，进行重试，如果再次失败，则仍旧等集群状态更新，直到超时1分钟为止。超时后仍失败则进行整体请求失败处理。
（2）在主分片写入过程中，写入是阻塞的。只有写入成功，才会发起写副本请求。如果主shard写失败，则整个请求被认为处理失败。如果有部分副本写失败，则整个请求被认为处理成功。
（3）无论主分片还是副分片，当写一个doc失败时，集群不会重试，而是关闭本地shard，然后向Master汇报，删除是以shard为单位的。

## 系统特性
ES本身也是一个分布式存储系统，如同其他分布式系统一样，我们经常关注的一些特性如下。
· 数据可靠性：通过分片副本和事务日志机制保障数据安全。
· 服务可用性：在可用性和一致性的取舍方面，默认情况下 ES 更倾向于可用性，只要主分片可用即可执行写入操作。
· 一致性：笔者认为是弱一致性。只要主分片写成功，数据就可能被读取。因此读取操作在主分片和副分片上可能会得到不同结果。
· 原子性：索引的读写、别名更新是原子操作，不会出现中间状态。但bulk不是原子操作，不能用来实现事务。
· 扩展性：主副分片都可以承担读请求，分担系统负载。

## 7.7 思考
分析完写入流程后，也许读者已经意识到了这个过程的一些缺点：
· 副分片写入过程需要重新生成索引，不能单纯复制数据，浪费计算能力，影响入库速度。
· 磁盘管理能力较差，对坏盘检查和容忍性比HDFS差不少。例如，在配置多磁盘路径的情况下，有一块坏盘就无法启动节点。

# 最后

上面详细介绍了Elasticsearch的写入流程及其各个流程的工作机制，我们在这里再次总结下之前提出的分布式系统中的六大特性：

-   可靠性：由于Lucene的设计中不考虑可靠性，在Elasticsearch中通过Replica和TransLog两套机制保证数据的可靠性。
-   一致性：Lucene中的Flush锁只保证Update接口里面Delete和Add中间不会Flush，但是Add完成后仍然有可能立即发生Flush，导致Segment可读。这样就没法保证Primary和所有其他Replica可以同一时间Flush，就会出现查询不稳定的情况，这里只能实现最终一致性。
-   原子性：Add和Delete都是直接调用Lucene的接口，是原子的。当部分更新时，使用Version和锁保证更新是原子的。
-   隔离性：仍然采用Version和局部锁来保证更新的是特定版本的数据。
-   实时性：使用定期Refresh Segment到内存，并且Reopen Segment方式保证搜索可以在较短时间（比如1秒）内被搜索到。通过将未刷新到磁盘数据记入TransLog，保证对未提交数据可以通过ID实时访问到。
-   性能：性能是一个系统性工程，所有环节都要考虑对性能的影响，在Elasticsearch中，在很多地方的设计都考虑到了性能，一是不需要所有Replica都返回后才能返回给用户，只需要返回特定数目的就行；二是生成的Segment现在内存中提供服务，等一段时间后才刷新到磁盘，Segment在内存这段时间的可靠性由TransLog保证；三是TransLog可以配置为周期性的Flush，但这个会给可靠性带来伤害；四是每个线程持有一个Segment，多线程时相互不影响，相互独立，性能更好；五是系统的写入流程对版本依赖较重，读取频率较高，因此采用了versionMap，减少热点数据的多次磁盘IO开销。Lucene中针对性能做了大量的优化。
# Reference
https://pdai.tech/md/db/nosql-es/elasticsearch-y-th-3.html