#es 

# 设计思想
所有分布式系统都需要以某种方式处理一致性问题。一般情况下，可以将策略分为两组：试图避免不一致及定义发生不一致之后如何协调它们。后者在适用场景下非常强大，但对数据模型有比较严格的限制。因此这里研究前者，以及如何应对网络故障。

# 为什么使用主从模式
ES的典型场景中的另一个简化是集群中没有那么多节点。通常，节点的数量远远小于单个节点能够维护的连接数，并且网络环境不必经常处理节点的加入和离开。这就是为什么主从模式更适合ES。

# 选举算法
1. Bully算法
Leader选举的基本算法之一。它假定所有节点都有一个唯一的ID，使用该ID对节点进行排序。任何时候的当前Leader都是参与集群的最高ID节点。该算法的优点是易于实现。但是，当拥有最大ID的节点处于不稳定状态的场景下会有问题。例如，Master负载过重而假死，集群拥有第二大ID的节点被选为新主，这时原来的Master恢复，再次被选为新主，然后又假死……ES 通过推迟选举，直到当前的 Master 失效来解决上述问题，只要当前主节点不挂掉，就不重新选主。但是容易产生脑裂（双主），为此，再通过“法定得票人数过半”解决脑裂问题。

2. Paxos算法
Paxos非常强大，尤其在什么时机，以及如何进行选举方面的灵活性比简单的Bully算法有很大的优势，因为在现实生活中，存在比网络连接异常更多的故障模式。但 Paxos 实现起来非常复杂。

# 相关配置
discovery.zen.minimum_master_nodes：最小主节点数，这是防止脑裂、防止数据丢失的极其重要的参数。这个参数的实际作用早已超越了其表面的含义。除了在选主时用于决定“多数”，还用于多处重要的判断，至少包含以下时机：
· 触发选主 进入选主的流程之前，参选的节点数需要达到法定人数。
· 决定Master 选出临时的Master之后，这个临时Master需要判断加入它的节点达到法定人数，才确认选主成功。
· gateway选举元信息 向有Master资格的节点发起请求，获取元数据，获取的响应数量必须达到法定人数，也就是参与元信息选举的节点数。
· Master发布集群状态 发布成功数量为多数。

discovery.zen.ping.unicast.hosts：集群的种子节点列表，构建集群时本节点会尝试连接这个节点列表，那么列表中的主机会看到整个集群中都有哪些主机。可以配置为部分或全部集群节点。
discovery.zen.ping.unicast.hosts.resolve_timeout:DNS 解析超时时间，默认为5秒。discovery.zen.join_timeout：节点加入现有集群时的超时时间，默认为 ping_timeout的20倍。discovery.zen.join_retry_attemptsjoin_timeout：超时之后的重试次数，默认为3次。discovery.zen.join_retry_delay join_timeout：超时之后，重试前的延迟时间，默认为100毫秒。discovery.zen.master_election.ignore_non_master_pings：设置为true时，选主阶段将忽略来自不具备Master资格节点（node.master: false）的ping请求，默认为false。discovery.zen.fd.ping_interval：故障检测间隔周期，默认为1秒。
discovery.zen.fd.ping_timeout：故障检测请求超时时间，默认为30秒。
discovery.zen.fd.ping_retries：故障检测超时后的重试次数，默认为3次。

# 流程概叙
ZenDiscovery的选主过程如下：
· 每个节点计算最小的已知节点ID，该节点为临时Master。向该节点发送领导投票。
· 如果一个节点收到足够多的票数，并且该节点也为自己投票，那么它将扮演领导者的角色，开始发布集群状态。所有节点都会参与选举，并参与投票，但是，只有有资格成为Master的节点（node.master为true）的投票才有效．获得多少选票可以赢得选举胜利，就是所谓的法定人数。在 ES 中，法定大小是一个可配置的参数。配置项：discovery.zen.minimum_master_nodes。为了避免脑裂，最小值应该是有Master资格的节点数n/2+1。

# 流程分析
整体流程可以概括为：选举临时Master，如果本节点当选，则等待确立Master，如果其他节点当选，则尝试加入集群，然后启动节点失效探测器。具体如下图所示。
![[Pasted image 20220316170315.png]]
执行本流程的线程池：generic。下面我们具体分析每个步骤的实现。

## 选举临时Mater
选举过程的实现位于ZenDiscovery#findMaster。该函数查找当前集群的活跃 Master，或者从候选者中选择新的Master。如果选主成功，则返回选定的Master，否则返回空。为什么是临时Master？因为还需要等待下一个步骤，该节点的得票数足够时，才确立为真正的Master。
临时Master的选举过程如下：
（1）“ping”所有节点，获取节点列表fullPingResponses,ping结果不包含本节点，把本节点单独添加到fullPingResponses中。
（2）构建两个列表。activeMasters列表：存储集群当前活跃Master列表。遍历第一步获取的所有节点，将每个节点所认为的当前Master节点加入activeMasters列表中（不包括本节点）。在遍历过程中，如果配置了discovery.zen.master_election.ignore_non_master_pings 为 true（默认为false），而节点又不具备Master资格，则跳过该节点。
![[Pasted image 20220316170845.png]]
这个过程是将集群当前已存在的Master加入activeMasters列表，正常情况下只有一个。如果集群已存在Master，则每个节点都记录了当前Master是哪个，考虑到异常情况下，可能各个节点看到的当前Master不同。在构建activeMasters列表过程中，如果节点不具备Master资格，则可以通过ignore_non_master_pings选项忽略它认为的那个Master。
masterCandidates列表：存储master候选者列表。遍历第一步获取列表，去掉不具备Master资格的节点，添加到这个列表中。
（3）如果activeMasters为空，则从masterCandidates中选举，结果可能选举成功，也可能选举失败。如果不为空，则从activeMasters中选择最合适的作为Master。
![[Pasted image 20220316171006.png]]

从masterCandidates中选主只是将节点排序后选择最小的节点作为Master。但是排序时使用自定义的比较函数MasterCandidate::compare，早期的版本中只是对节点 ID 进行排序，现在会优先把集群状态版本号高的节点放在前面。

从activeMasters列表中选择列表存储着集群当前存在活跃的Master，从这些已知的Master节点中选择一个作为选举结果。选择过程非常简单，取列表中的最小值，比较函数仍然通过compareNodes实现，activeMasters列表中的节点理论情况下都是具备Master资格的。

## 投票与得票的实现
在ES中，发送投票就是发送加入集群（JoinRequest）请求。得票就是申请加入该节点的请求的数量。收集投票，进行统计的实现在ZenDiscovery#handleJoinRequest方法中，收到的连接被存储到ElectionContext#joinRequestAccumulator中。当节点检查收到的投票是否足够时，就是检查加入它的连接数是否足够，其中会去掉没有Master资格节点的投票。

## 确立Master或加入集群
选举出的临时Master有两种情况：该临时Master是本节点或非本节点。为此单独处理。现在准备向其发送投票。
如果临时Master是本节点：
（1）等待足够多的具备Master资格的节点加入本节点（投票达到法定人数），以完成选举。
（2）超时（默认为30秒，可配置）后还没有满足数量的join请求，则选举失败，需要进行新一轮选举。（3）成功后发布新的clusterState。

如果其他节点被选为Master：
（1）不再接受其他节点的join请求。
（2）向Master发送加入请求，并等待回复。超时时间默认为1分钟（可配置），如果遇到异常，则默认重试3次（可配置）。这个步骤在joinElectedMaster方法中实现。
（3）最终当选的Master会先发布集群状态，才确认客户的join请求，因此，joinElectedMaster返回代表收到了join请求的确认，并且已经收到了集群状态。本步骤检查收到的集群状态中的Master节点如果为空，或者当选的Master不是之前选择的节点，则重新选举。

# 节点失效检测
到此为止，选主流程已执行完毕，Master身份已确认，非Master节点已加入集群。节点失效检测会监控节点是否离线，然后处理其中的异常。失效检测是选主流程之后不可或缺的步骤，不执行失效检测可能会产生脑裂（双主或多主）。在此我们需要启动两种失效探测器：
· 在Master节点，启动NodesFaultDetection，简称NodesFD。定期探测加入集群的节点是否活跃。
· 在非Master节点启动MasterFaultDetection，简称MasterFD。定期探测Master节点是否活跃。

NodesFaultDetection和MasterFaultDetection都是通过定期（默认为1秒）发送的ping请求探测节点是否正常的，当失败达到一定次数（默认为3次），或者收到来自底层连接模块的节点离线通知时，开始处理节点离开事件。
##  NodesFaultDetection事件处理
检查一下当前集群总节点数是否达到法定节点数（过半），如果不足，则会放弃 Master 身份，重新加入集群。
防止脑裂

## MasterFaultDetection事件处理
探测Master离线的处理很简单，重新加入集群。本质上就是该节点重新执行一遍选主的流程。

整体流程图：
![[Pasted image 20230403215913.png]]

# Bully算法选举
看下代码

如果刚才的那个活动选举的链表activeMasters为空，也就是说不存在活着的master节点，然后再去再去配置文件里面**elasticsearch.yml**，有个重要属性，discovery.zen.minimum_master_nodes，这代表着最小选举节点，当activeMasters为空，并且minimum_master_nodes > masterCandidates，满足匹配数量，直接走bully的选举，选举出最小ID的节点成为master节点。

```java
if (activeMasters.isEmpty()) {  
        if (electMaster.hasEnoughCandidates(masterCandidates)) {  
            final ElectMasterService.MasterCandidate winner = electMaster.electMaster(masterCandidates);  
            logger.trace("candidate {} won election", winner);  
            return winner.getNode();  
        } else {  
            // if we don't have enough master nodes, we bail, because there are not enough master to elect from  
            logger.warn("not enough master nodes discovered during pinging (found [{}], but needed [{}]), pinging again",  
                        masterCandidates, electMaster.minimumMasterNodes());  
            return null;  
        }  
    } else {  
        assert !activeMasters.contains(localNode) : "local node should never be elected as master when other nodes indicate an active master";  
        // lets tie break between discovered nodes  
        return electMaster.tieBreakActiveMasters(activeMasters);  
    }  
```

###  Bully算法的优点

其实Bully算法，简单粗暴，好处就是**简单**

并且算法的难度低啊，怪不得Es最初几个版本要用它，简单就完事儿了

而且bully的选举速度很快，相互通知，比对大小，就得了，所以我想这就是es最初几版用它的原因吧。

###  Bully算法的缺点

缺点的话。。。我感觉bully算法相比于其他主流算法，相对来说简单一些

但是简单很可能就是一种原罪

当有节点频繁加入或者退出的时候，主节点会频繁的进行切换，保存的节点信息的元数据也就会越来越大，越来越大

所以Bully算法的缺点，从我的角度上来看的话，也就是无法满足复杂场景下的选主需求，因为复杂场景下的选主，需要强力的选主算法支撑
###  为什么要替换Bully算法

Es7 版本算是大改，不仅改了选主算法，连type都删了，这算是一个大变动，我记得很多人的Es都是5，或者6

我查询了一些资料，发觉在Es官方的文章上说明了为什么替换的原因

```
Elasticsearch 6.x 及之前的版本使用了一个叫作 Zen Discovery 的集群协调子系统。  
  
这个子系统经过多年的发展，成功地为大大小小的集群提供支持。  
  
然而，我们想做出一些改进，这需要对它的工作方式作出一些根本性的修改。  
  
Zen Discovery 允许用户通过设置   
  
discovery.zen.minimum_master_nodes 来决定多少个符合主节点条件的节点可以形成仲裁。  
  
在每个节点上一定要正确地配置这个参数，如果集群进行动态扩展，也需要正确地更新它，这一点非常重要。  
  
系统不可能检测到用户是否错误配置了这个参数，而且在实践当中，在  
添加或删除节点之后很容易忘记调整这个参数。  
  
Zen Discovery 试图通过在每次主节点选举过程中等待几秒来防止出现这种错误配置。  
  
这意味着，如果所选的主节点失败，在选择替代节点之前，集群至少在几秒钟内是不可用的。  
  
如果集群无法选举出一个主节点，有时候很难知道是为什么。  
```

这个翻译太蹩脚了，我来直接简单扼要的说明一下为什么要替换吧

1.  老版本里面有个discovery.zen.minimum_master_nodes，这个很重要，但是动态扩展的时候有些时候可能会忘记设置这个东西
2.  如果不设置这个东西，Zen Discovery会在每次选举过程中等待一阵，大概是几秒时间来防止这种错误配置，这就造成集群暂时不可用
3.  也有可能造成无法选主的问题，这个非常致命，所以我们要换这套算法。

# 新选主算法 类Raft算法（Es7之后更新）

接上节，那么，Es不用Bully之后，到底要用什么呢？

官方给的文章也说明了这个问题

```
我们重新设计并重建了 Elasticsearch 7.0 的集群协调子系统：  
  
1. 移除 minimum_master_nodes 参数，让 Elasticsearch 自己选择可以形成仲裁的节点。  
  
2. 典型的主节点选举现在只需要很短的时间就可以完成。  
  
3. 集群的伸缩变得更安全、更容易，并且可能造成丢失数据的系统配置选项更少了。  
  
4. 节点更清楚地记录它们的状态，有助于诊断为什么它们不能加入集群或为什么无法选举出主节点  
```

这边说的很清楚，原有的Zen Discovery被替换了，不再使用

那么新版采用的算法是什么呢？

这个在官方文档里也有

```
我们经常被问到的一个问题是，为什么不简单地“插入”像 Raft 一样的标准分布式共识算法。  
  
有很多公认的算法，每种算法都有不同的利弊权衡。  
  
我们仔细评估了所有可以找到的文献，并从中汲取灵感。  
  
在我们早期的概念验证中，有一个概念便使用了非常接近 Raft 的协议。  
```

所以，我将Es7的选主算法命名为类Raft算法，也就是类似Raft的算法。但是不是完全是Raft算法，这个我会在后面详细说明。

### Raft算法的原理

首先Es的选主算法基础是来源于Raft，还是有必要分析下Raft算法的机制和原理

才有助于后期对Es 7 的核心源码进行解读分析

####  什么是Raft算法？

这个又涉及到了一致性问题，这是分布式系统的一个老调常谈的部分

首先，Raft算法是用来解决分布式一致性问题，而编写出来的一种算法。

####  Raft算法是如何工作的？

在Raft算法中，一个节点分别可能有三种角色

1.  Follower 跟随者
2.  Candidate 候选人
3.  Leader 领导者

当初始化的时候，所有的节点都是Follower跟随者

此时此刻，如果没有收到leader的消息，那么节点将会自动转换为Candidate候选人的身份

这时，成为候选人的节点，将向其他节点发送选举投票请求

这时，接收到信号的节点，将会回复这次投票请求

如果大多数节点都赞成选举这个节点为leader，那么这个节点将会成为集群leader节点

这就是领导者选举的步骤，通俗来讲，也就是选主步骤，这也是我今天要讲的核心。

这里我画了个图，将步骤完全简化，大家，凑合看下吧，聪明的你，一定一看就懂！

![](https://blog-1256169066.cos.ap-chengdu.myqcloud.com/%E6%96%B0%E6%97%A7%E7%AE%97%E6%B3%95/3.png)
![[Pasted image 20230403222151.png]]
开始选举的条件是：

1.  当follower节点接收不到leader节点的心跳
2.  leader节点超时

但是要注意，当follower节点收不到leader节点心跳的时候，需要等待一点时间切换为Candidate候选人，才可以开始选举。

看下图吧，大概，也许差不离，就是这么选举。

### 类Raft算法在ElasticSearch中如何运用

大概知道了Raft算法的选举原理，那么在Es里面，如何使用新的算法去选举leader呢？

这里就需要追一下源码了，下面又到了追源码的路子了，大家要看不了的话，真没办法了！

首先，ElasticSearch的选举算法套了Raft的壳子，但是不是真的Raft逻辑

相比于Raft算法，Es的选主算法有如下不同

1.  初始为 Candidate状态
2.  允许多次投票，也就是每个有投票资格的节点可以投多票
3.  候选人可以有投票的机会
4.  可能会产生多个主节点

举例来说，如果node1，node2，node3进行选主

如果node1当选leader，但是node2发来了投票要求，那么node1无条件退出leader状态，node2选为主节点，但是node3也发来了投票要求，那么node2退出leader状态，node3当选主节点。

说明白了，就是**保证最后当选的leader为主leader**

那么，综上所述，选举的流程为

节点的初始状态，为Candidate，当加入集群的时候，如果我们的discovery发现集群已经存在 leader，那么新加入的节点自动转换为follower。

类似下图：
![[Pasted image 20230403222206.png]]
![](https://blog-1256169066.cos.ap-chengdu.myqcloud.com/%E6%96%B0%E6%97%A7%E7%AE%97%E6%B3%95/4.jpg)

假设Candidate开始，那么节点收到足够的投票数量，转换为leader，假设其他节点发送了拉票请求，此节点辞去leader，转换为Candidate，直到选出leader，转换为follower。

角色转变类似这张图：
![[Pasted image 20230403222211.png]]
![](https://blog-1256169066.cos.ap-chengdu.myqcloud.com/%E6%96%B0%E6%97%A7%E7%AE%97%E6%B3%95/5.jpg)

###  类Raft算法的优点

这个资料上面有，我就直接说了

1.  免去了 Zen Discovery 的 discovery.zen.minimum_master_nodes 配置，es 会自己选择可以形成仲裁的节点，用户只需配置一个初始 master 节点列表即可。也即集群扩容或缩容的过程中，不要再担心遗漏或配错 discovery.zen.minimum_master_nodes 配置
2.  新版 Leader 选举速度极大快于旧版。在旧版 Zen Discovery 中，每个节点都需要先通过 3 轮的 ZenPing 才能完成节点发现和 Leader 选举，而新版的算法通常只需要在 100ms 以内
3.  修复了 Zen Discovery 下的疑难问题，如重复的网络分区可能导致群集状态更新丢失问题

### 类Raft算法的缺点

1.  节点多的情况下，会造成重复选主多次，选主缓慢

# Reference
https://www.easyice.cn/archives/332
https://helloyoubeautifulthing.net/blog/2019/08/20/new-cluster-coordination-in-elasticsearch/