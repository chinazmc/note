#es 

索引恢复（indices.recovery）是ES数据恢复过程。待恢复的数据是客户端写入成功，但未执行刷盘（flush）的Lucene分段。例如，当节点异常重启时，写入磁盘的数据先到文件系统的缓冲，未必来得及刷盘，如果不通过某种方式将未刷盘的数据找回来，则会丢失一些数据，这是保持数据完整性的体现；另一方面，由于写入操作在多个分片副本上没有来得及全部执行，副分片需要同步成和主分片完全一致，这是数据副本一致性的体现。根据数据分片性质，索引恢复过程可分为主分片恢复流程和副分片恢复流程。

· 主分片从translog中自我恢复，尚未执行flush到磁盘的Lucene分段可以从translog中重建；
· 副分片需要从主分片中拉取Lucene分段和translog进行恢复。但是有机会跳过拉取Lucene分段的过程。
索引恢复的触发条件包括从快照备份恢复、节点加入和离开、索引的_open操作等。
恢复工作一般经历以下几个阶段（stage），如下表所示。
![[Pasted image 20220322174035.png]]
主分片和副分片恢复都会经历这些阶段，但有时候会跳过具体执行过程，只是在流程上体现出经历了这个短暂阶段。例如，副分片恢复时会跳过TRANSLOG 重放过程；主分片恢复过程中的INDEX阶段不会在节点之间复制数据。

# 相关配置
索引恢复流程有以下配置项，都支持动态调整，如下表所示。
![[Pasted image 20220322174137.png]]

# 主分片恢复流程
### 1.init阶段
一个分片的恢复流程中，从开始执行恢复的那一刻起，被标记为 INIT 阶段，INIT 标记在IndexShard#startRecovery 函数的参数中传入，在判断此分片属于哪种恢复类型之前就被设置为INIT阶段。
接下来，恢复流程在新的线程池中开始执行，开始阶段主要是一些验证工作，例如，校验当前分片是否为主分片，分片状态是否异常等。做完简单的校验工作后，进入INDEX阶段
### 2.INDEX阶段
本阶段从Lucene读取最后一次提交的分段信息，获取其中的版本号，更新当前索引版本
### 3.VERIFY_INDEX阶段
VERIFY_INDEX中的INDEX指Lucene index，因此本阶段的作用是验证当前分片是否损坏，是否进行本项检查取决于配置项：index.shard.check_on_startup
![[Pasted image 20220322175410.png]]
![[Pasted image 20220322175418.png]]
在索引的数据量较大时，分片检查会消耗更多的时间。
默认配置为不执行验证索引，进入最重要的TRANSLOG阶段。

### 4.TRANSLOG阶段
一个Lucene索引由许多分段组成，每次搜索时遍历所有分段。内部维护了一个称为“提交点”的信息，其描述了当前Lucene索引都包括哪些分段，这些分段已经被fsync系统调用，从操作系统的cache刷入磁盘。每次提交操作都会将分段刷入磁盘实现持久化。
本阶段需要重放事务日志中尚未刷入磁盘的信息，因此，根据最后一次提交的信息做快照，来确定事务日志中哪些数据需要重放。重放完毕后将新生成的Lucene数据刷入磁盘。
遍历所有需要重放的事务日志，执行具体的写操作，如同写入过程一样

### 5.FINALIZE阶段
本阶段执行刷新（refresh）操作，将缓冲的数据写入文件，但不刷盘，数据在操作系统的cache中。

### 6.DONE阶段
DONE阶段是恢复工作的最后一个阶段，进入DONE阶段之前再次执行refresh，然后更新分片状态。

至此，主分片恢复完毕，对恢复结果进行处理。

# 副分片恢复流程
## 流程概叙
副分片恢复的核心思想是从主分片拉取Lucene分段和translog进行恢复。按数据传递的方向，主分片节点称为Source，副分片节点称为Target。
为什么需要拉取主分片的translog？因为在副分片恢复期间允许新的写操作，从复制Lucene分段的那一刻开始，所恢复的副分片数据不包括新增的内容，而这些内容存在于主分片的translog中，因此副分片需要从主分片节点拉取translog进行重放，以获取新增内容。这就需要主分片节点的translog不被清理。为了防止主分片节点的translog被清理，这方面的实现机制经历了多次迭代。
在1.x版本时代，通过阻止刷新（refresh）操作，让translog都保留下来。但是这样可能会产生很大的translog。在2.0～5.x版本时代，引入了translog.view的概念：
· 为了安全地完成recoveries/relocations操作，我们必须在recovery过程开始后保证所有的操作已全部处理（一个特定的索引或更新请求称为一个操作），以便重放。目前是通过防止engine flush，从而确保操作operations都在translog中来实现的。这没问题，因为我们确实需要这些operations。如果另一个recovery并发启动，则可能会有不必要的长时间重试。如果我们在这个时候因为某种原因关闭了Engine（比如一个节点重新启动），当再次启动的时候，我们需要恢复一个很大的translog。
· 为了解决这个问题，translog被改为基于多个文件而不是一个文件。这允许recovery保留所需的文件，同时允许Engine执行flush，以及执行Lucene的commit（这将创建一个新的translog文件）。
· 重构了translog文件管理模块，允许存在多个文件。
· translog维护一个引用文件的列表，包括未完成的recovery，以及那些包含尚未提交到Lucene的operations的文件。
· 引入了新的translog.view概念，允许recovery获取一个引用，包括所有当前未提交的translog 文件，以及所有未来新创建的 translog文件，直到view关闭。它们可以使用这个view做operations的遍历操作。

创建一个视图可以获取后续的所有操作，直到关闭视图。刷新可以自由执行，translog文件删除逻辑基于文件级别的引用计数。当获得一个视图时，这些计数器会增加。创建快照时同样会用到视图来读取 translog 中的操作，视图必须一直保存直到最后一个快照被消费完。这在recovery流程中没问题，但是在Primary Replica Resyncer中存在一个小bug，只影响未发布的代码。具体请参考https://github.com/elastic/elasticsearch/pull/25862。

因此从6.0版本开始，translog.view被移除。引入TranslogDeletionPolicy的概念，负责维护活跃（liveness）的translog文件。这个类的实现非常简单，它将translog做一个快照来保持translog不被清理。这样使用者只需创建一个快照，无须担心视图之类。恢复流程实际上确实需要一个视图，现在可以通过获取一个简单的保留锁来防止清理translog。这消除了视图概念的需求。在保证translog不被清理后，恢复核心处理过程由两个内部阶段（phase）组成。
· phase1：在主分片所在节点，获取translog保留锁，从获取保留锁开始，会保留translog不受其刷盘清空的影响。然后调用Lucene接口把shard做快照，快照含有shard中已经刷到磁盘的文件引用，把这些shard数据复制到副本节点。在phase1结束前，会向副分片节点发送告知对方启动Engine，在phase2开始之前，副分片就可以正常处理写请求了。
· phase2：对translog做快照，这个快照里包含从phase1开始，到执行translog快照期间的新增索引。将这些translog发送到副分片所在节点进行重放。
由于phase1需要通过网络复制大量数据，过程非常漫长，在ES 6.x中，有两个机会可以跳过phase1：（1）如果可以基于恢复请求中的SequenceNumber进行恢复，则跳过phase1。
（2）如果主副两分片有相同的syncid且doc数相同，则跳过phase1。在数据模型一章中介绍过SequenceNumber，现在介绍syncid的概念。

## synced flush机制
为了解决副分片恢复过程第一阶段时间太漫长而引入了synced flush，默认情况下5分钟没有写入操作的索引被标记为inactive，执行synced flush，生成一个唯一的syncid，写入分片的所有副本中。这个syncid是分片级，意味着拥有相同syncid的分片具有相同的Lucene索引。
synced flush本质上是一次普通的flush操作，只是在Lucene的commit过程中多写了一个syncid。原则上，在没有数据写入的情况下，各分片在同一时间“flush”成功后，它们理应有相同的Lucene索引内容，无论Lucene分段是否一致。于是给分片分配一个id，表示数据一致。
但是显然synced flush期间不能有新写入的内容，如果syncflush执行期间收到写请求，则ES选择了写入可用性：让synced flush失败，让写操作成功。在没有执行flush的情况下已有syncid不会失效。
在某个分片上执行普通flush操作会删除已有syncid。因此，synced flush操作是一个不可靠操作，只适用于冷索引。下面详细介绍整个副分片恢复过程。

## 副分片节点处理过程
副分片恢复的 VERIFY_INDEX、TRANSLOG、FINALIZE 三个阶段由主分片节点发送的RPC调用触发，如下图所示。
![[Pasted image 20220322193109.png]]
在副分片恢复过程中，副分片节点会向主分片节点发送start_recovery的RPC请求，主分片节点对此请求的处理注册在PeerRecoverySourceService类中，内部类PeerRecoverySourceService.StartRecoveryTransportRequestHandler负责处理此RPC请求。类似的，主分片节点也会向副分片节点发送一些RPC请求，副分片节点对这些请求的处理以XXXRequestHandler的方式注册在PeerRecoveryTargetService类中，包括接收Lucene文件、接收translog并重放、执行清理等操作，如下图所示。

## 主分片节点处理过程
副分片恢复的 INDEX 阶段向主分片节点发送action 为internal:index/shard/recovery/start_recovery的恢复请求，主分片对此请求的处理过程是副分片恢复的核心流程。核心流程如下图所示。
![[Pasted image 20220322193301.png]]

# recovery速度优化
众所周知，索引恢复是集群启动过程中最缓慢的过程，集群完全重启，或者Master节点挂掉后，新选出的Master也有可能执行这个过程。官方也一直在优化索引恢复速度，陆续添加了syncid和SequenceNumber。下面归纳一下有哪些方法可以提升索引恢复速度：
· 配置项cluster.routing.allocation.node_concurrent_recoveries 决定了单个节点执行副分片recovery时的最大并发数（进/出），默认为2，适当提高此值可以增加recovery并发数。
· 配置项indices.recovery.max_bytes_per_sec决定节点间复制数据时的限速，可以适当提高此值或取消限速。
· 配置项cluster.routing.allocation.node_initial_primaries_recoveries决定了单个节点执行主分片recovery时的最大并发数，默认为4。由于主分片的恢复不涉及在网络上复制数据，仅在本地磁盘读写，所以在节点配置了多个数据磁盘的情况下，可以适当提高此值。

· 在重启集群之前，先停止写入端，执行syncflush，让恢复过程有机会跳过phase1。
· 适当地多保留些 translog，配置项index.translog.retention.size 默认最大保留512MB, index.translog.retention.age默认为不超过12小时。调整这两个配置可让恢复过程有机会跳过phase1。
· 合并 Lucene 分段，对于冷索引甚至不再更新的索引执行_forcemerge，较少的 Lucene分段可以提升恢复效率，例如，减少对比，降低文件传输请求数量。
最后，可以考虑允许主副分片存在一定程度的不一致，修改 ES 恢复流程，少量的不一致则跳过phase1。

# 如何保证副分片和主分片一致
索引恢复过程的一个难点在于如何维护主副分片的一致性。假设副分片恢复期间一直有写操作，如何实现一致呢？我们先看看早期的做法：在2.0版本之前，副分片恢复要经历三个阶段。
· phase1：将主分片的 Lucene 做快照，发送到target。期间不阻塞索引操作，新增数据写到主分片的translog。
· phase2：将主分片translog做快照，发送到target重放，期间不阻塞索引操作。
· phase3：为主分片加写锁，将剩余的translog发送到target。此时数据量很小，写入过程的阻塞很短。
从理论上来说，只要流程上允许将写操作阻塞一段时间，实现主副一致是比较容易的。但是后来（从2.0版本开始），也就是引入translog.view概念的同时，phase3被删除。
phase3被删除，这个阶段是重放操作（operations），同时防止新的写入 Engine。这是不必要的，因为自恢复开始，标准的 index 操作会发送所有的操作到正在恢复中的分片。重放恢复开始时获取的view中的所有操作足够保证不丢失任何操作。
阻塞写操作的phase3被删除，恢复期间没有任何写阻塞过程。接下来需要处理的就是解决phase1和phase2之间的写操作与phase2重放操作之间的时序和冲突问题。在副分片节点，phase1结束后，假如新增索引操作和 translog 重放操作并发执行，因为时序的关系会出现新老数据交替。如何实现主副分片一致呢？
假设在第一阶段执行期间，有客户端索引操作要求将docA的内容写为1，主分片执行了这个操作，而副分片由于尚未就绪所以没有执行。第二阶段期间客户端索引操作要求写 docA 的内容为2，此时副分片已经就绪，先执行将docA写为2的新增请求，然后又收到了从主分片所在节点发送过来的translog重复写docA为1的请求该如何处理？具体流程如下图所示。
![[Pasted image 20220322194225.png]]
答案是在写流程中做异常处理，通过版本号来过滤掉过期操作。写操作有三种类型：索引新文档、更新、删除。索引新文档不存在冲突问题，更新和删除操作采用相同的处理机制。每个操作都有一个版本号，这个版本号就是预期doc版本，它必须大于当前Lucene中的doc版本号，否则就放弃本次操作。对于更新操作来说，预期版本号是Lucenedoc版本号+1。主分片节点写成功后新数据的版本号会放到写副本的请求中，这个请求中的版本号就是预期版本号。这样，时序上存在错误的操作被忽略，对于特定doc，只有最新一次操作生效，保证了主副分片一致。我们分别看一下写操作三种类型的处理机制。
1． 索引新文档
不存在冲突问题，不需要处理。
2． 更新
判断本次操作的版本号是否小于Lucene中doc的版本号，如果小于，则放弃本次操作。Index、Delete 都继承自 Operation，每个Operation 都有一个版本号，这个版本号就是doc版本号。对于副分片的写流程来说，正常情况下是主分片写成功后，相应doc写入的版本号被放到转发写副分片的请求中。对于更新来说，就是通过主分片将原doc版本号+1后转发到副分片实现的。在对比版本号的时候：expectedVersion = 写副分片请求中的 version =写主分片成功后的 version通过下面的方法判断当前操作的版本号是否低于Lucene中的版本号
3． 删除
判断本次操作中的版本号是否小于Lucene中doc的版本号，如果小于，则放弃本次操作。
# recovery相关监控命令
在实际生产环境中我们经常需要了解recovery的进度和状态，ES提供了丰富的API可以获取这些信息。
### 1. _cat/recovery
列出活跃的和已完成的recovery信息
使用？v参数可以显示列名：GET _cat/recovery?v这个API提供的信息包括recovery类型，existing_store意味着主分片本地恢复，peer代表副分片从其他节点恢复，以及数据传输的源节点和目的节点信息，数据传输进度，总体文件大小和已传输字节数，等等。

### 2. {index}/_recovery
此API展示特定索引的recovery所处阶段，以及每个分片、每个阶段的详细信息
除了基础信息，此API还给出了index、translog、verify_index三个耗时最长的重要阶段的详细信息。例如，传输字节数、限速信息等。如果更关注进度信息，则可以过滤stage和percent字段。

### 3. _stats
有时我们想知道sync flush是否完成，_stats API可以给出分片级信息，包括分片的sync_id、local_checkpoint、global_checkpoint等，可以通过指定索引名称实现，或者使用_all输出全部索引的信息。
这个示例统计集群所有分片中有多少个分片拥有sync_id。这意味着sync flush成功。

# 小结
· 主分片恢复的主要阶段是TRANSLOG阶段；
· 副分片恢复的主要阶段是INDEX和TRANSLOG阶段；
· 只有phase1有限速配置，phase2不限速；
· Lucene的“提交”概念就是从操作系统内存cache fsync到磁盘的过程。