随着业务的快速发展、业务复杂度越来越高，几乎每个公司的系统都会从单体走向分布式，特别是转向微服务架构。随之而来就必然遇到分布式事务这个难题，DTM致力于提供一个简单易用、跨语言，同时具备高性能、高可用、可扩展的分布式事务解决方案。

## 基础理论 

在讲解具体方案之前，我们先了解一下分布式事务所涉及到的基础理论知识。

我们拿转账作为例子，A需要转100元给B，那么需要给A的余额-100元，给B的余额+100元，整个转账要保证，A-100和B+100同时成功，或者同时失败。看看在各种场景下，是如何解决这个问题的。

## 事务 

把多条语句作为一个整体进行操作的功能，被称为数据库事务。数据库事务可以确保该事务范围内的所有操作都可以全部成功或者全部失败。

事务具有 4 个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为 ACID 特性。

- Atomicity（原子性）：一个事务中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被恢复到事务开始前的状态，就像这个事务从来没有执行过一样。
- Consistency（一致性）：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。完整性包括外键约束、应用定义的等约束不会被破坏。
- Isolation（隔离性）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。
- Durability（持久性）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

主流的数据库例如MySQL、PostgreSQL等，都支持ACID事务，其内部会采用MVCC（多版本并发控制）技术，实现高性能、高并发的本地事务。MVCC技术是数据库的技术基石，非常重要，但是内容很多，这里就不再展开细说，有兴趣的同学，可以自行查找相关资料学习。

## 分布式理论 

分布式事务涉及多个节点，是一个典型的分布式系统，与单机系统有非常大的差别。一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项，这被称为CAP理论。

#### C 一致性 

分布式系统中，数据一般会存在不同节点的副本中，如果对第一个节点的数据成功进行了更新操作，而第二个节点上的数据却没有得到相应更新，这时候读取第二个节点的数据依然是更新前的数据，即脏数据，这就是分布式系统数据不一致的情况。

在分布式系统中，如果能够做到针对一个数据项的更新操作执行成功后，所有的用户都能读取到最新的值，那么这样的系统就被认为具有强一致性（或严格的一致性）。

请注意CAP中的一致性和ACID中的一致性，虽然单词相同，但实际含义不同，请注意区分

#### A 可用性 

在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。

在现代的互联网应用中，如果因为服务器宕机等问题，导致服务长期不可用，是不可接受的

#### P 分区容错性 

以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在 C 和 A 之间做出选择。

提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项仍然能在其他区中读取，容忍性就提高了。然而，把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。

#### 面临的问题 

对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C。

#### BASE理论 

BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的简写，BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的结论，是基于CAP定理逐步演化而来的，其核心思想是即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。接下来我们着重对BASE中的三要素进行详细讲解。

- 基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性——但请注意，这绝不等价于系统不可用。
- 弱状态也称为软状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。
- 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性

总的来说，BASE理论面向的是大型高可用可扩展的分布式系统，提出通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。

许多的NoSQL是按照BASE理论进行设计的，典型的例子包括：DynamoDB、Cassandra、CouchDB。

## 分布式事务 

银行跨行转账业务是一个典型分布式事务场景，假设A需要跨行转账给B，那么就涉及两个银行的数据，无法通过一个数据库的本地事务保证转账的ACID，只能够通过分布式事务来解决。

分布式事务就是指事务的发起者、资源及资源管理器和事务协调者分别位于分布式系统的不同节点之上。在上述转账的业务中，用户A-100操作和用户B+100操作不是位于同一个节点上。本质上来说，分布式事务就是为了保证在分布式场景下，数据操作的正确执行。

分布式事务可以分为两类：

- 第一类为：NewSQL的内部分布式事务，这类事务不是我们今天讨论的重点，仅做简单叙述
- 第二类为：跨数据库、跨服务的分布式事务，这类事务是DTM主要研究的对象，后面会详细讲解

## NewSQL的分布式事务 

以Spanner、TiDB为代表的NewSQL，在内部集群多节点间，实现了ACID的事务，即提供给用户的事务接口与普通本地事务无差别，但是在内部，一个事务是支持多个节点多条数据的写入，此时无法采用本地ACID的MVCC技术，而是会采用一套复杂的分布式MVCC来做到ACID。大多数的NewSQL分布式事务技术都采用这篇论文[Percolator](http://research.google.com/pubs/pub36726.html)介绍的核心技术。

那么从CAP的角度看，三者不能同时兼备，那么NewSQL选择了什么，牺牲了什么呢？

首先我们看C（一致性），这是数据库类的应用必须具备的。只要数据写入了，后续的读，一定能获取到最新写入的结果。你可以想象如果不是这样，那么你的应用处理关键事务如订单时，如果读到的结果不是最新的，那么你就无法确定订单的当前准确状态，就无法进行正确处理，更无从谈起ACID特性。

然后我们看P（分区），只要是分布式系统，那么P就是必然有概率发生的，因此P是分布式系统必须处理，必须具备的特性。

那么我们再看A（可用性），由于架构的发展，系统出现网络分区的频率可以大幅降低。另外分布式共识算法的发展，可以在较短的时间，正确的达成共识，从而从分区故障中恢复。谷歌分布式锁Chubby的公开数据显示，集群能提供99.99958％的平均可用性，一年也就130s的运行中断。这样的可用性相当高，对实际应用的影响微乎其微。

也就是说随着现代工程和共识算法的发展，可以构造出满足CP的系统，同时接近于满足A，可以称之为CP+HA，这里HA代表的是非100%的A，而是很高的可用性。

公开的数据显示，谷歌的Spanner支持ACID的事务特性，同时提供了高达的5个9的可用性，因此这是一个CP+HA。

既然NewSQL已经达到了CP+HA，那么从CAP的角度看，前面介绍BASE中的典型DynamoDB系统等，只达到了AP，他们是否就可以退出历史舞台了呢？不会！NewSQL和BASE的系统之间，性能上差异可能是巨大的，因此在实际高性能高并发应用上，BASE也是有不少的应用场景的。

## 跨服务跨库的分布式事务 

分布式事务

虽然分布式事务分为两类，一类是前面介绍的NewSQL的分布式事务，不是DTM研究的重点，另一类是DTM重点研究的跨服务跨库分布式事务，为了简化描述，本教程如果无特殊说明，关键词：**分布式事务**指的是**跨库跨服务更新数据的分布式事务**

DTM主要研究跨服务，跨数据库的分布式事务，这类分布式事务只是部分遵循 ACID 规范的：

- 原子性：严格遵循
- 一致性：事务完成后的一致性严格遵循；事务中的一致性可适当放宽
- 隔离性：并行事务间不可影响；事务中间结果可见性允许安全放宽
- 持久性：严格遵循

这里面一致性和隔离性都没有严格遵守，但是ACID这四个特性中，AID这三个特性其实是数据库实现的人非常关心，而对于使用数据库的人，最终的用户，最关心的则是C，即用户视角看，分布式事务的一致性是什么样的？

对于这里面的C（一致性），我们以一个非常具体的业务例子，来进行解释。假如我们正在处理一个转账业务，假设是A转给B 30元，在本地事务的支持下，我们的用户看到A+B的总金额，在整个转账前后，以及转账过程中，都是保持不变的。那么这个时候用户认为他看到的数据是一致的，符合业务约束的。

当我们业务变复杂，引入多个数据库和大量微服务时，上述本地事务的一致性，依旧是业务非常关心。假如一个业务更新操作，跨库或者跨服务时，那么此时业务关心的一致性问题，就变成了 分布式事务中的一致性问题。

在单机本地事务中，A+B的总金额在任何时刻去查（以常见的ReadCommitted或ReadRepeatable隔离级别），都是不变的，也就是业务约束一直都保持的这种一致性，我们称之为强一致性。

## 无法强一致 

目前在跨库、跨服务的分布式实际应用中，尚未看到有强一致性的方案。

我们来看看一致性级别最高的XA事务（读者如果还不了解，可以参考[XA事务](https://dtm.pub/practice/xa.html)），是否是强一致的，我们以跨行转账（在这里，我们以跨库更新AB来模拟）作为例子来说明，下面是一个XA事务的时序图：

![xa-no-c](https://dtm.pub/assets/xa-no-c.3b75a269.svg)

在这个时序图中，我们在如图所示的时间点发起查询，就是在两个commit中间进行的查询，那么我们查到的结果数据，将是A+B+30，不等于A+B，不符合强一致的要求。

> 从微服务1查到的结果为A，因为A-30这个事务未提交；从微服务2查到的结果为B+30；总和为A+B+30

从理论上分析，由于是分布式系统，那么一定是无法保证两个commit同时结束，只要两个commit中间有时间差，那么无论如何我们都无法保证强一致性。

## 理论上的强一致性 

我们接下来思考，普通XA事务不是强一致的，但假如完全不考虑性能因素，有没有可能在理论上做到强一致：

我们先看看如果我们把XA事务涉及的数据库，隔离级别设定到Serializable，是否能到到强一致的效果呢？我们来看看前面的时序场景：

![xa-serial-c](https://dtm.pub/assets/xa-serial-c.0512df94.png)

这种情况下，查到结果等于A+B。

> 此时从微服务1查到的结果为A-30，因为在串行化之下，查询会等到A-30这个事务提交才返回；从微服务2查到的结果为B+30；总和为A+B。

但是又有另一些场景出现了问题，如下图所示：

![xa-serial-c2](https://dtm.pub/assets/xa-serial-c2.0055f9cd.png)

按照图中时序查询的结果是：A+B-30，依旧是不一致。

> 此时从微服务1查到的结果为A-30，因为在串行化之下，查询会等到A-30这个事务提交才返回；从微服务2查到的结果为B，因为B+30这个事务还没有开始；总和为A+B-30.

深入思考这个强一致的问题之后，有一种做法可以在普通的读已提交下做到强一致，做法如下：

- 对于查询，也采用XA事务，并且查询数据时，采用select for update的方式，所有数据查完之后，再xa commit
- 具体到当前这个时序图中，要先锁A查A，然后锁B查B，最后再分别提交，否则可能出现死锁

在上述策略下，我们可以看到，在时序图任何一个时间点进行查询，获得的结果都是A+B

![xa-strong-c](https://dtm.pub/assets/xa-strong-c.2e524a0a.png)

- 在T0时间查询，那么修改一定发生在查询全部完成之后，所以查询得到结果A+B
- 在T1，T2，T3查询，那么查询结果返回一定全部发生在修改完成之后，所以查询得到结果也是A+B

很明显这种方案下的强一致，缺点非常多。互联网应用大多是读多写少，在数据库中，原先的读可以通过多版本技术，不被锁定，快速出结果，而上述这种强一致方案：

- 一方面效率极低，所有有数据交集的数据库读读、读写、写写都必须串行执行。
- 另一方面，开发人员进行数据库多个数据查询时，也可能发生死锁，要么让开发人员小心排好访问顺序，要么接受死锁。

目前暂未看到实际应用中有哪些系统采用这种方式来进行强一致的跨库分布式事务。

未来有没有可能借鉴NewSQL的这种方式，来实现跨库、跨微服务这类分布式事务的强一致性？理论上是可以的。

- 实现跨服务但不跨库的分布式事务一致性，会相对简单一些，其中一种方式就是实现XA事务中的TMRESUME选项。我们从前面的分析中看到，XA事务的不一致，来源于分布式系统上的两个commit无法同时完成，现在已经在一个数据库，只是跨服务，那么TMRESUME可以允许我们将某个服务的XA事务继续往前操作，最终提交时，只有一个xa commit，因此避免了两个xa commit中间的不一致时间窗口，那么就是强一致的。
- 实现跨数据库的分布式事务一致性，会困难很多，因为各个数据库的内部版本机制都不一样，想要协同非常困难。困难来自于两点：一是不同厂商之间的MVCC机制不一样，例如Spanner是TrueTime，TiDB是单点授时，还有一些是逻辑时钟，想要兼容多种MVCC非常困难。二是不同厂商难以有足够的商业利益驱动去做这样的协同。

## 最终一致性 

从前面的分析中可以看到，在分布式事务进行的过程中，一致性是无法得到保证的，但是分布式事务完成之后，一致性是没问题的，严格遵守的。因此我们将分布式事务方案称为最终一致性方案，这个最终一致性，与CAP中的最终一致性用了同样的词语，但他们的具体含义是不一样的，在CAP中是指读取操作最终能够读取到最后一次写入的结果，在分布式事务中是指最终事务完成后，数据严格满足业务约束。

既然现有的各种分布式事务方案都无法做到强一致，那么最终一致性之间是否有差别呢？我们进行了以下关于一致性强弱的分类：

一致性由强到弱分别是：

[XA事务](https://dtm.pub/practice/xa.html)>[TCC](https://dtm.pub/practice/tcc.html)>[二阶段消息](https://dtm.pub/practice/msg.html)>[SAGA](https://dtm.pub/practice/saga.html)

他们的分类为： ![c-classify](https://dtm.pub/assets/c-classify.73742507.png)

- **不一致窗口短**：XA和TCC在理想的情况下，可以做到不一致的窗口时间很短
- **不一致窗口长**：SAGA和MSG则缺少控制不一致窗口时间的方法，相对来说会更长
- **XA**：XA虽然不是强一致，但是XA的一致性是多种分布式事务中，一致性最好的，因为他处于不一致的状态时间很短，只有一部分分支开始commit，但还没有全部commit的这个时间窗口，数据是不一致的。因为数据库的commit操作耗时，通常是10ms内，因此不一致的窗口期很短。
- **TCC**：理论上，TCC可以用XA来实现，例如Try-Prepare，Confirm-Commit，Cancel-Rollback。但绝大多数时候，TCC会在业务层自己实现Try|Confirm|Cancel，因此Confirm操作耗时，通常高于XA中的Commit，不一致的窗口时间比XA长
- **MSG**：二阶段消息型事务在第一个操作完成后，在所有操作完成之前，这个时间窗口是不一致的，持续时长一般比前两者更久。
- **SAGA**：SAGA的不一致窗口时长与消息接近，但是如果发生回滚，而子事务中正向操作修改的数据又会被用户看到，这部分数据就是错误数据，容易给用户带来较差的体验，因此一致性是最差的。

我们这里的分类仅仅从我们关心的几个维度进行了归纳，适用于多数场景，但并不一定适用所有情况。

## 小结 

在这里，我们介绍了分布式事务相关的理论，分析了分布式事务的最终一致性，接下来，我们将介绍DTM的架构与实践

# DTM架构 

我们先简单介绍一个完整的分布式事务，大家了解了分布式事务的过程后，再详细介绍DTM的架构。

## 分布式事务过程 

假设我们要进行一个跨行转账，A转给B30元，因为A、B不在同一个数据库，无法通过本地事务解决，因此使用了SAGA分布式事务来解决这个问题，下面是一个成功的SAGA事务（更多SAGA详情，参考[SAGA](https://dtm.pub/practice/saga.html)）的时序图：

![saga_normal](https://dtm.pub/assets/saga_normal.a2849672.jpg)

整个全局事务分为如下几步：

1. 用户定义好全局事务所有的事务分支，包括action/compensate，然后提交给DTM，DTM持久化全局事务信息后，立即返回
2. DTM取出第一个事务分支，这里是TransOut，调用该服务，该服务成功返回
3. DTM取出第二个事务分支，这里是TransIn，调用该服务，该服务也成功返回
4. DTM已完成所有的事务分支，将全局事务的状态，修改为已完成

在理解了上述的这个时序图的基础上，我们来看dtm的架构

## DTM架构图 

![arch](https://dtm.pub/assets/arch.8ecd5239.jpg)

整个DTM架构中，一共有三个角色，分别承担了不同的功能

- RM-资源管理器：RM是一个应用服务，负责管理全局事务中的本地事务，他通常会连接到一个数据库，负责相关数据的修改、提交、回滚、补偿等操作。例如在前面的这个SAGA事务中，步骤2、3中被调用的TransIn，TransOut服务都是RM，业务上负责A、B账户余额的修改
- AP-应用程序：AP是一个应用服务，负责全局事务的编排，他会注册全局事务，注册子事务，调用RM接口。例如在前面的这个SAGA事务中，发起步骤1的是AP，它编排了一个包含TransOut、TransIn的全局事务，然后提交给TM
- TM-事务管理器：TM就是DTM服务，负责全局事务的管理，每个全局事务都注册到TM，每个事务分支也注册到TM。TM会协调所有的RM，将同一个全局事务的不同分支，全部提交或全部回滚。例如在前面的SAGA事务中，TM在步骤2、3中调用了各个RM，在步骤4中，完成这个全局事务

这里的角色划分中，有时候一个服务可能会同时承担AP和RM的功能，例如后续讲到的嵌套TCC情况。

## 跨语言特性 

dtm本身由Go编写，他提供了多语言分布式事务的支持，为什么DTM支持了多语言，而其他的分布式事务框架，目前未看到多语言支持呢？

目前看到其他框架中，在SDK层的设计里，都比较重，包括的逻辑多，还有部分框架在SDK层实现了自定义的XA协议，因此整个SDK很重，拓展到更多语言，需要大量大量的工作。

而在上述DTM的架构中，SDK是AP中的蓝色部分，他在设计之初，定位就是很薄的一层，主要功能就是调用HTTP/gRPC协议通信，加上一些ID生成等。对于一门新语言，开发一个简单可用的SDK，一般只需要几十行到几百行的代码。目前dtm-labs下，已经有多门语言的SDK了，详情参见[多语言SDK](https://dtm.pub/ref/sdk.html)

## 高可用 

在上述这个架构中，TM不是单点，而是一个服务集群。集群由多个dtm实例进程构成，多个dtm实例访问一个共享的高可用数据库（可以选Redis/Mysql/Postgres）。

几乎所有的云厂商都会提供高可用的数据库，几乎所有要求业务高可用的公司内部，都会有高可用的数据库，因此将dtm对接到一个高可用数据库是毫无困难的。

在实际的生产部署中，DTM是多实例的，连接的共享存储是高可用的，因此不存在单点，从而提供了高可用的DTM服务。

## 多进程同时轮询 

全局事务的运行过程中，可能出现各种网络错误，导致全局事务无法一次完成，需要定时重试。

DTM（默认配置下）会在每个进程实例中，去轮询超时需要处理的全局任务。为了保证上述高可用多进程部署中，一个超时任务不会被多个实例同时取出，dtm使用了类似乐观锁的机制，保证一个任务只会被一个dtm实例取出。

- **DB存储引擎：** 采用update ... set owner=..., time=... where ... 这种方式，因为update的原子性，能够保证一条记录只被一个实例更新
- **Redis存储引擎：** 采用lua脚本原子操作，保证一条记录只被一个实例取出

当然极端情况下，还是有可能出现这个任务被两个进程取出，这个时候由于每个分支的结果是幂等的，所以最终不会影响结果的正确性。

## 术语 

DTM在分布式事务的理论与实践中，常常会提到一些专业术语，下面定义这些术语：

- 事务分支：我们把每个服务管理的全局事务组成部分，称为事务分支，例如前面的转账，分为转出和转入两个事务分支
- 分支操作：每个事务分支，在SAGA、XA、TCC等事务模式下，会有多个操作，例如转出事务分支，包括正向操作TransOut和TransOutCompensate
- 本地事务：转出事务分支中的正向操作，通常会开启一个事务，对余额进行扣减，我们将数据操作的这个事务，称为本地事务
- GID：全局事务ID，用于标记全局事务，必须唯一。该ID可以采用UUID生成，也可以使用业务上的ID，例如订单ID等

## 接口协议

目前dtm只支持http和grpc协议，以及在这基础上的部分微服务协议。由于分布式事务涉及分布式协作，某些参与者可能出现暂时不可用或者返回500等异常情况是不可避免的。这些暂时不可用和500，与业务上的失败有非常大的区别。

接口错误与业务失败

例如前面的转出金额操作，如果遇见暂时不可用，或者500，此时不应当认为转账失败，而进行回滚。

有个dtm的用户，在未使用分布式事务的旧系统中，曾经遇到过这类事故，起因是开发人员在调用发红包后，因为调用超时，认为发红包失败，没有扣减用户的余额，但是当红包服务恢复正常后，发现红包已发出，这就导致了金额错误，造成了事故。

因此进行分布式事务开发时，切记 **接口返回错误 不等于 业务失败**

对于上述的 ”暂时不可用“ 及 ”500“，DTM 会进行重试。而对于成功或失败等确定的结果，则会更新分布式事务的进度。

dtm系统中，调用分支事务的服务，有四种结果：

- **SUCCESS:** 表示成功
- **FAILURE:** 表示失败，这个失败是指确定的失败结果，不需要重试。例如子事务因转账余额不足而失败，dtm服务因为事务已完成不允许再次提交等
- **ONGOING:** 表示未完成，还在正常进行中，此时dtm服务器需要采用固定间隔重试，而不是指数退避算法重试
- **其他:** 表示临时错误，采用指数退避算法重试，避免出现故障或者bug，导致大量重试，导致负载过高

ONGOING

DTM 引入了一个特殊的结果 ONGOING，当 DTM 收到这个返回值时，认为这个子事务操作还在正常进行中，还未完成，需要进行重试。假如你需要预定旅游出行的机票，第三方可能需要1个小时才能够确认机票预定的结果，那么应用可以指定分布式事务的重试间隔时间，并在未获得确定结果时，返回 ONGOING，这样 DTM 会按照固定间隔时间重试。详情可以参见 [SAGA](https://dtm.pub/practice/saga.html)

下面分别说明 HTTP 和 gRPC 中，如何表示含义明确的三种结果

#### HTTP 

- **SUCCESS:** 状态码 200 StatusOK
- **FAILURE:** 状态码 409 StatusConflict
- **ONGOING:** 状态码 425 StatusTooEarly

#### gRPC 

- **SUCCESS:** 状态码 OK
- **FAILURE:** 状态码 Aborted
- **ONGOING:** 状态码 FailedPrecondition

以上 HTTP 和 gRPC 的几个结果定义，对于云原生上的重试策略是友好兼容的，默认情况下，如果微服务配置了重试策略，那么对于其他结果（通常为 HTTP 502 或者 gRPC Unavailable）会进行重试，而确定的结果，则不会被云原生上的重试策略重试。

还有一点，对于某些不支持回滚的事务分支，例如Msg模式的分支，Saga模式的Compensation，Tcc模式的Confirm/Cancel，在这些分支里面不应当返回FAILURE。

接口协议变更

dtm的接口协议在 1.10.0进行了变更升级，升级的同时保持了与旧版本的兼容，详情参见[升级指南](https://dtm.pub/deploy/upgrade.html)

## 与XA DTP模型异同 

DTM的架构和角色，与X/Open XA的DTP模型一致，RM、AP、TM担当的功能都是一致的，只是将XA的DTP模型拓展到了服务/微服务架构上。

DTM对比了其他事务框架的角色划分和DTP的角色划分，认为DTP在总体架构，依旧能够适用于跨服务这种分布式事务形式，因此保留了AP、RM、TM的划分

![xa-dtp](https://dtm.pub/assets/xa-dtp.62f4d7d0.png)

主要区别如下：

- TM不再是单点，而是集群部署，本身就具备高可用。原先2PC中的单点问题，就被集群化的dtm实例+共享的高可用存储解决了。这其中没有复杂的选举过程，而是依赖云服务上提供的高可用共享存储引擎（云服务商户进行故障重新选举，用户不用关心）
- RM不是直接的数据库，而是服务，服务的与他背后数据库交互。TM与RM服务交互，而不会与数据库直接交互
- AP不是直接的本地程序，而是服务，他对RM的访问，是通过网络的api请求，而不是本地SDK调用。

## 小结 

DTM的整体架构具备高可用，易扩容等特点，复杂性都在内部解决，暴露给用户的是非常简单易上手的形式。

# SAGA事务模式 

SAGA事务模式是DTM中最常用的模式，主要是因为SAGA模式简单易用，工作量少，并且能够解决绝大部分业务的需求。

> dtm 的SAGA模式与Seata的SAGA在设计理念上是不一样的，整体使用难度大幅度降低，非常容易上手

SAGA最初出现在1987年Hector Garcaa-Molrna & Kenneth Salem发表的论文[SAGAS](https://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf)里。其核心思想是将长事务拆分为多个短事务，由Saga事务协调器协调，如果每个短事务都成功提交完成，那么全局事务就正常完成，如果某个步骤失败，则根据相反顺序一次调用补偿操作。

## 拆分为子事务 

例如我们要进行一个类似于银行跨行转账的业务，将A中的30元转给B，根据Saga事务的原理，我们将整个全局事务，切分为以下服务：

- 转出（TransOut）服务，这里转出将会进行操作A-30
- 转出补偿（TransOutCompensate）服务，回滚上面的转出操作，即A+30
- 转入（TransIn）服务，转入将会进行B+30
- 转入补偿（TransInCompensate）服务，回滚上面的转入操作，即B-30

整个SAGA事务的逻辑是：

执行转出成功=>执行转入成功=>全局事务完成

如果在中间发生错误，例如转入B发生错误，则会调用已执行分支的补偿操作，即：

执行转出成功=>执行转入失败=>执行转入补偿成功=>执行转出补偿成功=>全局事务回滚完成

下面我们看一个成功完成的SAGA事务典型的时序图：

![saga_normal](https://dtm.pub/assets/saga_normal.a2849672.jpg)

在这个图中，我们的全局事务发起人，将整个全局事务的编排信息，包括每个步骤的正向操作和反向补偿操作定义好之后，提交给服务器，服务器就会按步骤执行前面SAGA的逻辑。

## SAGA的接入 

我们看看Go如何接入一个SAGA事务

```go
req := &gin.H{"amount": 30} // 微服务的请求Body
// DtmServer为DTM服务的地址
saga := dtmcli.NewSaga(DtmServer, shortuuid.New()).
  // 添加一个TransOut的子事务，正向操作为url: qsBusi+"/TransOut"， 逆向操作为url: qsBusi+"/TransOutCompensate"
  Add(qsBusi+"/TransOut", qsBusi+"/TransOutCompensate", req).
  // 添加一个TransIn的子事务，正向操作为url: qsBusi+"/TransIn"， 逆向操作为url: qsBusi+"/TransInCompensate"
  Add(qsBusi+"/TransIn", qsBusi+"/TransInCompensate", req)
// 提交saga事务，dtm会完成所有的子事务/回滚所有的子事务
err := saga.Submit()
```

上面的代码首先创建了一个SAGA事务，然后添加了两个子事务TransOut、TransIn，每个事务分支包括action和compensate两个操作，分别为Add函数的第一第二个参数。子事务定好之后提交给dtm。dtm收到saga提交的全局事务后，会调用所有子事务的正向操作，如果所有正向操作成功完成，那么事务成功结束。

详细例子代码参考[dtm-examples](https://github.com/dtm-labs/dtm-examples)

我们前面的的例子，是基于HTTP协议SDK进行DTM接入，gRPC协议的接入基本一样，详细例子代码可以在[dtm-examples](https://github.com/dtm-labs/dtm-examples)

## 失败回滚 

如果有正向操作失败，例如账户余额不足或者账户被冻结，那么dtm会调用各分支的补偿操作，进行回滚，最后事务成功回滚。

我们将上述的第二个分支调用，传递参数，让他失败

```sql
  Add(qsBusi+"/TransIn", qsBusi+"/TransInCompensate", &TransReq{Amount: 30, TransInResult: "FAILURE"})
```

失败的时序图如下：

![saga_rollback](https://dtm.pub/assets/saga_rollback.8da8593f.jpg)

补偿执行顺序

dtm的SAGA事务在1.10.0及之前，补偿操作是并发执行的，1.10.1之后，是根据用户指定的分支顺序，进行回滚的。

如果是普通SAGA，没有打开并发选项，那么SAGA事务的补偿分支是完全按照正向分支的反向顺序进行补偿的。

如果是并发SAGA，补偿分支也会并发执行，补偿分支的执行顺序与指定的正向分支顺序相反。假如并发SAGA指定A分支之后才能执行B，那么进行并发补偿时，DTM保证A的补偿操作在B的补偿操作之后执行

## 如何做补偿 

当SAGA对分支A进行失败补偿时，A的正向操作可能1. 已执行；2. 未执行；3. 甚至有可能处于执行中，最终执行成功或者失败是未知的。那么对A进行补偿时，要妥善处理好这三种情况，难度很大。

dtm提供了子事务屏障技术，自动处理上述三种情况，开发人员只需要编写好针对1的补偿操作情况即可，相关工作大幅简化，详细原理，参见下面的异常章节。

失败的分支是否需要补偿

dtm 常被问到的一个问题是，TransIn返回失败，那么这个时候是否还需要调用TransIn的补偿操作？DTM 的做法是，统一进行一次调用，这种的设计考虑点如下：

- XA, TCC 等事务模式是必须要的，SAGA 为了保持简单和统一，设计为总是调用补偿
- DTM 支持单服务多数据源，可能出现数据源1成功，数据源2失败，这种情况下，需要确保补偿被调用，数据源1的补偿被执行
- DTM 提供的子事务屏障，自动处理了补偿操作中的各种情况，用户只需要执行与正向操作完全相反的补偿即可

## 异常 

在事务领域，异常是需要重点考虑的问题，例如宕机失败，进程crash都有可能导致不一致。当我们面对分布式事务，那么分布式中的异常出现更加频繁，对于异常的设计和处理，更是重中之重。

我们将异常分为以下几类：

- **偶发失败：** 在微服务领域，由于网络抖动、机器宕机、进程Crash会导致微小比例的请求失败。这类问题的解决方案是重试，第二次进行重试，就能够成功，因此微服务框架或者网关类的产品，都会支持重试，例如配置重试3次，每次间隔2s。DTM的设计对重试非常友好，应当支持幂等的各个接口都已支持幂等，不会发生因为重试导致事务bug的情况
- **故障宕机：** 大量公司内部都有复杂的多项业务，这些业务中偶尔有一两个非核心业务故障也是常态。DTM也考虑了这样的情况，在重试方面做了指数退避算法，如果遇见了故障宕机情况，那么指数退避可以避免大量请求不断发往故障应用，避免雪崩。
- **网络乱序：** 分布式系统中，网络延时是难以避免的，所以会发生一些乱序的情况，例如转账的例子中，可能发生服务器先收到撤销转账的请求，再收到转账请求。这类的问题是分布式事务中的一个重点难点问题，详情参考：[异常与子事务屏障](https://dtm.pub/practice/barrier.html)

业务上的失败与异常是需要做严格区分的，例如前面的余额不足，是业务上的失败，必须回滚，重试毫无意义。分布式事务中，有很多模式的某些阶段，要求最终成功。例如dtm的补偿操作，是要求最终成功的，只要还没成功，就会不断进行重试，直到成功。关于这部分的更详细的论述，参见[最终成功](https://dtm.pub/practice/must-succeed.html)

介绍到这里，您已经具备足够的知识，开发完成一个普通的SAGA任务。下面我们将介绍SAGA更加高级的知识与用法

## 高级用法 

我们以一个真实用户案例，来讲解dtm的saga部分高级功能。

问题场景：一个用户出行旅游的应用，收到一个用户出行计划，需要预定去三亚的机票，三亚的酒店，返程的机票。

要求：

1. 两张机票和酒店要么都预定成功，要么都回滚（酒店和航空公司提供了相关的回滚接口）
2. 预订机票和酒店是并发的，避免串行的情况下，因为某一个预定最后确认时间晚，导致其他的预定错过时间
3. 预定结果的确认时间可能从1分钟到1天不等

上述这些要求，正是saga事务模式擅长的领域，我们来看看dtm怎么解决。

首先我们根据要求1，创建一个saga事务，这个saga包含三个分支，分别是，预定去三亚机票，预定酒店，预定返程机票

```go
		saga := dtmcli.NewSaga(DtmServer, gid).
			Add(Busi+"/BookTicket", Busi+"/BookTicketRevert", bookTicketInfo1).
			Add(Busi+"/BookHotel", Busi+"/BookHotelRevert", bookHotelInfo2).
			Add(Busi+"/BookTicket", Busi+"/BookTicketRevert", bookTicketBackInfo3)
```

然后我们根据要求2，让saga并发执行（默认是顺序执行）

```go
  saga.EnableConcurrent()
```

最后我们处理3里面的“预定结果的确认时间”不是即时响应的问题。由于不是即时响应，所以我们不能够让预定操作等待第三方的结果，而是提交预定请求后，就立即返回状态-进行中。我们的分支事务未完成，dtm会重试我们的事务分支，我们把重试间隔指定为1分钟。

```
  saga.RetryInterval = 60
  saga.Submit()
// ........
func bookTicket() string {
	order := loadOrder()
	if order == nil { // 尚未下单，进行第三方下单操作
		order = submitTicketOrder()
		order.save()
	}
	order.Query() // 查询第三方订单状态
	return order.Status // 成功-SUCCESS 失败-FAILURE 进行中-ONGOING
}
```

固定间隔重试

dtm默认情况下，重试策略是指数退避算法，可以避免出现故障时，过多的重试导致负载过高。但是这里订票结果不应当采用指数退避算法重试，否则最终用户不能及时收到通知。因此在bookTicket中，返回结果ONGOING，当dtm收到这个结果时，会采用固定间隔重试，这样能及时通知到用户。

## 更多高级场景 

在实际应用中，还遇见过一些业务场景，需要一些额外的技巧进行处理

#### 部分第三方操作无法回滚 

例如一个订单中的发货，一旦给出了发货指令，那么涉及线下相关操作，那么很难直接回滚。对于涉及这类情况的saga如何处理呢？

我们把一个事务中的操作分为可回滚的操作，以及不可回滚的操作。那么把可回滚的操作放到前面，把不可回滚的操作放在后面执行，那么就可以解决这类问题

```
		saga := dtmcli.NewSaga(DtmServer, shortuuid.New()).
			Add(Busi+"/CanRollback1", Busi+"/CanRollback1Revert", req).
			Add(Busi+"/CanRollback2", Busi+"/CanRollback2Revert", req).
			Add(Busi+"/UnRollback1", "", req).
			Add(Busi+"/UnRollback2", "", req).
			EnableConcurrent().
			AddBranchOrder(2, []int{0, 1}). // 指定step 2，需要在0，1完成后执行
			AddBranchOrder(3, []int{0, 1}) // 指定step 3，需要在0，1完成后执行
```

示例中的代码，指定Step 2，3 中的 UnRollback 操作，必须在Step 0，1 完成后执行。

对于不可回滚的操作，DTM的设计建议是，不可回滚的操作在业务上也不允许返回失败。可以这么思考，如果发货的操作返回了失败，那么这个失败的含义是不够清晰的，调用方不知道这个失败是修改了部分数据的失败，还是修改数据前的业务校验失败，因为这个操作不可回滚，所以调用方收到这个失败，是不知道如何正确处理这个错误的。

另外当你的一个全局事务中，如果出现了两个既不可回滚的又可能返回失败的操作，那么到了实际运行中，一个执行成功，一个执行失败，此时执行成功的那个事务无法回滚，那么这个事务的一致性就不可能保证了。

对于发货操作，如果可能在校验数据上可能发生失败，那么将发货操作拆分为发货校验、发货两个服务则会清晰很多，发货校验可回滚，发货不可回滚同时也不会失败。

#### 超时回滚[#](https://dtm.pub/practice/saga.html#%E8%B6%85%E6%97%B6%E5%9B%9E%E6%BB%9A)

saga属于长事务，因此持续的时间跨度很大，可能是100ms到1天，因此saga没有默认的超时时间。

dtm支持saga事务单独指定超时时间，到了超时时间，全局事务就会回滚。

```
	saga.TimeoutToFail = 1800
```

在saga事务中，设置超时时间一定要注意，这类事务里不能够包含无法回滚的事务分支，因为超时回滚时，已执行的无法回滚的分支，数据就是错的。

#### 其他分支的结果作为输入[#](https://dtm.pub/practice/saga.html#%E5%85%B6%E4%BB%96%E5%88%86%E6%94%AF%E7%9A%84%E7%BB%93%E6%9E%9C%E4%BD%9C%E4%B8%BA%E8%BE%93%E5%85%A5)

前面的设计环节讲了为什么dtm没有支持这样的需求，那么如果极少数的实际业务有这样的需求怎么处理？例如B分支需要A分支的执行结果

dtm的建议做法是，在ServiceA再提供一个接口，让B可以获取到相关的数据。这种方案虽然效率稍低，但是易理解已维护，开发工作量也不会太大。

PS：有个小细节请注意，尽量在你的事务外部进行网络请求，避免事务时间跨度变长，导致并发问题。

如果您需要其他分支的结果作为输入，也可以考虑一下dtm里面的 TCC 模式，该模式有不同的适用场景，但是提供了非常便捷的获取其他分支结果的接口

## SAGA 设计原则 

Seata的SAGA采用了状态机实现，而DTM的SAGA没有采用状态机，因此常常有用户会问，为什么DTM没有采用状态机，状态机可以提供更加灵活的事务自定义。

我在DTM设计SAGA高级用法时，充分调研了状态机实现，经过仔细权衡之后，决定不采用状态机实现，主要原因如下：

#### 易用性对比 

可能在阿里内部，需要SAGA提供类似状态机的灵活性，但是在阿里外部，看到使用Seata的Saga事务的用户特别少。我调研了Seata中SAGA的开发资料，想要上手写一个简单的SAGA事务，需要

1. 了解状态机的原理
2. 上手状态机的图形界面工具，生成状态机定义Json（一个简单的分布式事务任务，需要大约90多行的Json定义）
3. 将上述Json配置到Java项目中
4. 如果遇见问题，需要跟踪调试状态机定义的调用关系，非常复杂

而对比之下，DTM的SAGA事务，则非常简单易用，开发者没有理解成本，通常五六行代码就完成了一个全局事务的编写，因此也成为DTM中，应用最为广泛的事务模式。而对于高级场景，DTM也经过实践的检验，以极简单的选项，例如EnableConcurrent、RetryInterval，解决了复杂的应用场景。目前收集到的用户需求中，暂未看到状态机能解决，而DTM的SAGA不能解决的案例。

#### gRPC友好度 

gRPC 是云原生时代中应用非常广泛的协议。而Seata的状态机，对HTTP的支持度较好，而对gRPC的支持度不友好。一个gRPC服务中返回的结果，如果没有相关的pb定义文件，就无法解析出其中的字段，因此就无法采用状态机做灵活的判断，那么想用状态机的话，就必须固定结果类型，这样对应用的侵入性就比较强，适用范围就比较窄。

DTM则对gRPC的支持更加友好，对结果类型无任何要求，适用范围更加广泛。

## 小结 

这里详细介绍了SAGA的简单用法到高级用法，如果您熟练掌握了DTM中的SAGA事务，那么就可以解决分布式事务中的绝大部分问题了

# 如何选择事务模式 

## 特性对比 

- 二阶段消息模式: 适合不需要回滚的场景
- saga模式: 适合需要回滚的场景
- tcc事务模式: 适合一致性要求较高的场景
- xa事务模式: 适合并发要求不高，没有数据库行锁争抢的场景

# 事务选项 

## 概况 

dtm的事务可以设定以下的选项:

```go
type TransOptions struct {
	WaitResult         bool              `json:"wait_result,omitempty" gorm:"-"`
	TimeoutToFail      int64             `json:"timeout_to_fail,omitempty" gorm:"-"` 
	// for trans type: saga xa tcc
	RetryInterval      int64             `json:"retry_interval,omitempty" gorm:"-"`  
	// for trans type: msg saga xa tcc
	BranchHeaders      map[string]string `json:"branch_headers,omitempty" gorm:"-"`
}
```

在 Saga、Msg事务模式中，可以在事务对象生成之后，设定这些选项

XA 则需要在 XaGlobalTransaction2 的第二个参数回调函数中设定

TCC 则需要在 TccGlobalTransaction2 的第三个参数回调函数中设定

## 等待事务结果 

上面介绍了各种模式，每种模式将事务提交之后，立即返回，不等待事务结束。但某些实际应用的场景，很多时候希望在整个全局事务完成之后，返回给用户最终结果，dtm对此进行了支持。

dtm 是通过事务中的 WaitResult 选项，对此进行了支持。该标记为true的情况下，Submit提交给dtm后，dtm会同步对事务进行一轮分支事务操作的调用，如果一切正常，所有的分支事务操作全部成功结束，全局事务成功，那么返回SUCCESS。如果全局事务的某个分支操作，出现异常，那么返回错误，后续会超时重试相关的分支操作。

客户端检查Submit返回的错误，如果为nil，则表示整个全局事务已经正常完成。如果不是nil，不意味全局事务已回滚，可能的情况有很多，客户端最好通过 dtm 的 query 接口查询全局事务的状态。

WaitResult 选项适用于：Saga/Xa/Tcc/Msg。

可以在示例项目中搜索 WaitResult，查看相关的例子

## 超时

dtm 服务器有一个配置项 TimeoutToFail：他指定了全局事务默认的超时失败时长（系统默认为33秒）。

当一个TCC、XA事务超过TimeoutToFail之后，会超时失败，进行回滚。您可以修改系统的默认值，也可以单独指定事务的TimeoutToFail

MSG 事务模式对 TimeoutToFail 的解读与其他事务模式不同，它是指在这个时间之后，会对只调用了Prepare，但是没有调用Submit的全局事务进行反查。MSG 事务模式在Submit之后，是不会回滚的。

SAGA 事务可能为短事务，也可能为长事务，超时时间跨度非常大，因此不采用系统设定的值，但是可以单独指定事务的TimeoutToFail

可以在示例项目中搜索 TimeoutToFail，查看相关的例子

## 重试时间 

DTM 会对许多事务分支操作进行重试，重试的间隔时间为 RetryInterval （系统默认为10秒），您可以修改系统的默认值，也可以单独指定事务的RetryInterval

DTM重试时采用退避算法，如果重试失败，则会加倍重试间隔时间后重试；如果某一次重试成功，那么会重置间隔时间，避免后续的正常操作采用过大的间隔时间

如果您不想要退避算法，而是希望间隔固定的时间，例如您预定了机票，需要定时查询结果，那么您可以在您的服务中，返回ONGOING，当DTM收到这个返回值，会不走退避算法，而是按照设定的间隔时间进行重试

可以在示例项目中搜索 RetryInterval ，查看相关的例子

## 自定义header 

有一部分的业务中的子事务，需要自定义header。dtm支持全局事务粒度的header定制，即您可以给不同的全局事务指定不同的自定义header，dtm调用您的子事务服务时，将会添加您指定的header

HTTP和gRPC都支持自定义header，详情可以参考[dtm-examples](https://github.com/dtm-labs/dtm-examples)名字中带有Header的例子

可以在示例项目中搜索 BranchHeaders ，查看相关的例子

# 注意点 

- **dtm服务器如果使用数据库，那么一定要使用主库。一方面dtm是写多读少，读写分离，对于dtm的负载分摊有限；另一方面dtm对数据的一致性要求较高，从库延时会导致各种问题。**
- **dtm服务器的时区需要与数据库的时区保持一致**
- **dtm服务器的时间需要与数据库的时间保持一致，差别不要超过3s**

# 订单系统 

dtm 可以应用于订单系统，可以大幅简化订单系统的架构，下面详细说明

## 现有问题 

大多数的订单系统都已经服务化，会将订单系统拆分为订单服务、库存服务、优惠券服务、支付服务、账户服务等等。整个订单处理过程中，有许多的操作（例如创建订单与扣减库存）需要保证原子性，但是在分布式系统中，保证这些操作的原子性，有大量的难题需要解决，下面我们来详细探讨其中一个典型的问题，逐步给出更好的架构方案。

场景：当前端的用户提交订单，服务端需要完成以下操作：

- 创建订单：需要在订单表中创建订单，唯一键为订单ID
- 扣减库存：需要给用户下单的商品扣减库存
- 扣减优惠券：用户在下单前，选择了可使用的优惠券，提交订单时，则扣减这部分优惠券
- 创建支付单：提交订单后，需要创建支付单，最后告诉用户跳转到支付页

对于上述这个场景，如果在单体订单系统中，很容易使用数据库的事务来解决。但是一旦服务化了之后，那就需要考虑分布式系统中的问题，我们将逐步分析其中的问题，并给出解决方案。

#### 进程 crash 问题 

我们在下单api中，按顺序调用: 创建订单->扣减库存->扣减优惠券->创建支付单，但是在这个调用过程中，有一定的概率会发生进程 crash ，导致这几个步骤，仅完成了一部分，还有一部分未完成。

这是一个常见问题，只要订单量变大，就会常见。有以下几种处理方案：

- 允许不一致：很小的公司可能会允许这种不一致数据出现，反馈到客服，或者监测到这种情况发生，则开发人员人肉处理；
- 消息队列保证最少成功一次：为了避免上述不一致的情况，可以把下单中的各个操作放到消息队列，当出现进程 crash ，那么相关消息还在队列中，会再次被消费，保证至少成功一次，但是队列无法保证有且仅有一次，因此相关操作都必须幂等，详细情况后面小节详述
- 状态机实现：很多大厂通过实现复杂的状态机，来定义订单的每个状态、每个操作的状态流转。这个方案非常复杂，多数中小厂，没有这么强的研发力量

#### 幂等问题

一旦到了分布式系统中，会有一定的概率出现重复请求。例如前面的消息队列方案，保证至少被成功消费一次，但是可能被多次消费，这就会出现重复请求。另外在微服务领域，为了防止网络临时失败，导致请求失败，也常常会配置重试策略。

假设这样的场景，扣减库存的数据库事务提交之后，进程 crash 了，那么这次的请求结果未知，按照前面介绍的场景，会进行重试。当处理重复请求时，库存服务需要判断出这是重复请求，不能再扣减库存，而是直接返回扣减成功。

扣减优惠券、创建支付单等其他操作，也需要做幂等处理，这类的处理虽然难度并不高，但依旧有不小的工作量，而且不易复现所有的异常情况，因此线上出现重复请求时，容易出现问题，耗费开发人员大量的时间

#### 回滚问题 

假如订单的多个步骤中，有一个步骤出现了问题，例如库存不足，或者用户在两个终端同时下单，导致其中一笔订单的优惠券扣减失败，就需要回滚。一旦要处理回滚，就会发现订单系统的实现难度就很高了。如果采用消息队列，那么会往消息队列里插入回滚补偿的消息，而这个补偿消息的处理会很复杂，需要判断进度，然后进行补偿；如果采用状态机，这类回滚操作，也会给状态机增加不少的状态，导致系统变得更加复杂

#### 精准补偿问题 

业务进行补偿时，也是一个难度很高的问题，难在哪？我们考虑库存扣减这个问题，如果库存服务收到库存扣减的请求，进行处理，那么本地事务可能已提交，也可能因为进程 crash 未提交。此时再收到回滚库存的请求，则需要识别出库存是否已修改，对于已修改的库存，需要进行补偿修改；对于未修改的库存，需要略过。

这种补偿问题同时存在于消息队列方案和状态机方案中，想要做到每个资源都妥善处理了这个精准补偿，会耗费大量的开发精力。

## dtm 解决方案 

面对上述问题，dtm 首创了极简方案，方案详情参见 [dtm-cases/order](https://github.com/dtm-labs/dtm-cases/tree/main/order)：

我们下面来解析这个方案，以及该方案是如何解决我们前面提出的问题的，首先看看，下单 api 的主要处理过程：

```go
app.POST("/api/busi/submitOrder", common.WrapHandler(func(c *gin.Context) interface{} {
  req := common.MustGetReq(c)
  saga := dtmcli.NewSaga(conf.DtmServer, "gid-"+req.OrderID).
    Add(conf.BusiUrl+"/orderCreate", conf.BusiUrl+"/orderCreateRevert", &req).
    Add(conf.BusiUrl+"/stockDeduct", conf.BusiUrl+"/stockDeductRevert", &req).
    Add(conf.BusiUrl+"/couponUse", conf.BusiUrl+"couponUseRevert", &req).
    Add(conf.BusiUrl+"/payCreate", conf.BusiUrl+"/payCreateRevert", &req)
  return saga.Submit()
}))
```

在这个代码中，定义了一个saga事务，包含上述下单过程中需要的四个步骤，以及四个步骤需要的补偿操作。

- **进程crash问题** dtm的saga事务进行过程中，如果发生进程crash，那么dtm会进行重试，保证操作会最终完成
- **回滚问题** 上述这个saga事务中，如果扣减库存时发现库存不足，则返回failure，会进行回滚。dtm 会记录哪些操作已完成，并回滚相关的操作

然后我们选取扣减库存和回滚库存的代码进一步分析：

```go
app.POST("/api/busi/stockDeduct", common.WrapHandler(func(c *gin.Context) interface{} {
  req := common.MustGetReq(c)
  return common.MustBarrierFrom(c).CallWithDB(common.DBGet(), func(tx *sql.Tx) error {
    affected, err := dtmimp.DBExec(tx,
      "update busi.stock set stock=stock-?, update_time=now() where product_id=? and stock >= ?",
      req.ProductCount, req.ProductID, req.ProductCount)
    if err == nil && affected == 0 {
      return dtmcli.ErrFailure // not enough stock, return Failure to rollback
    }
    return err
  })
}))
app.POST("/api/busi/stockDeductRevert", common.WrapHandler(func(c *gin.Context) interface{} {
  req := common.MustGetReq(c)
  return common.MustBarrierFrom(c).CallWithDB(common.DBGet(), func(tx *sql.Tx) error {
    _, err := dtmimp.DBExec(tx,
      "update busi.stock set stock=stock+?, update_time=now() where product_id=?",
      req.ProductCount, req.ProductID)
    return err
  })
}))
```

上述的代码，核心的业务逻辑就是扣减库存和回滚库存，那么幂等与精准扣减库存怎么处理？核心就在下面这行代码上：

```
  common.MustBarrierFrom(c).CallWithDB(common.DBGet(), func(tx *sql.Tx) error { /* ... */ })
```

当我们把数据库的操作放到上述代码内部时，就能够自动处理：

- **幂等：** 重复请求里面的业务操作会被上述代码过滤，数据库操作仅在非重复请求时被调用
- **精准补偿：** 如果 stockDeduct 中没有提交相关的数据库操作，stockDeductRevert 中数据库操作，会被上述代码过滤
- **悬挂：** 上述代码不仅处理了幂等和精准补偿问题，还处理了悬挂请求

上述代码的使用了 dtm 首创的子事务屏障技术，详细原理参见 [子事务屏障](https://dtm.pub/practice/barrier.html)

## 例子源码 

详细的源代码可以参见 [dtm-cases/order](https://github.com/dtm-labs/dtm-cases/tree/main/order)

在这个项目中，你可以便捷的试验本文所讲的全部内容

## 小结 

非单体的订单系统，需要花费大量时间处理分布式系统中出现的新问题，而 dtm 则是一个专业的解决方案，提供了非常优雅易用的方案，可以大幅简化现有的消息队列架构或状态机架构。

希望通过本文的分析，以及简单优雅的项目代码，让大家快速了解dtm，改变大家“分布式事务能不用就不用”的旧观念，将分布式事务相关逻辑全部交由 dtm 处理，而让大家关注于业务本身，只需要安心写好相关操作和补偿操作。

# 缓存一致性 
## 概述 

大量的实际的项目中，都会引入 Redis 缓存来缓解数据库的查询压力，此时由于一个数据在 Redis 和数据库两处进行了存储，就会有数据一致性的问题。目前业界尚未见到成熟的能够确保最终一致性的方案，特别是当如下场景发生时，会直接导致缓存数据与数据库数据不一致。

![cache-version](https://dtm.pub/assets/cache-version.1e154c0d.svg)

在上述场景下，缓冲中的数据最终版本为v1，而数据库的最终版本为v2，可能给应用带来较大问题。​

很多人容易想到通过分布式锁，来解决上述问题，但依旧存在问题，参见下图（来自DDIA作者 Martin Kleppmann 关于redis锁的讨论）

![unsafe-lock](https://dtm.pub/assets/unsafe-lock.18a3b149.png)

[dtm-labs](https://github.com/dtm-labs) 致力于解决数据一致性问题，在分析了行业的现有做法后，提出了新解决方案[dtm-labs/dtm](https://github.com/dtm-labs/dtm)+[dtm-labs/rockscache](https://github.com/dtm-labs/rockscache)，彻底解决了上述问题。

该方案具备以下特性：

- 最终一致：极端情况也能确保缓存的最终一致
- 强一致：支持给应用提供强一致的访问
- 高性能：与常见的缓存方案对比，性能无大的差别
- 防击穿：给出更好的防击穿方案
- 防穿透
- 防雪崩
- 非常易用：仅提供极简的两个函数，对应用无要求

关于管理缓存的现有方案，本文不再赘述，不太了解的同学可以参考下面这两篇文章

- 这篇通俗易懂些：[聊聊数据库与缓存数据一致性问题](https://juejin.cn/post/6844903941646319623)
- 这篇更加深入：[携程最终一致和强一致性缓存实践](https://www.infoq.cn/article/hh4iouiijhwb4x46vxeo)

## 问题与方案 

在上述这个时序图中，由于服务1发生了进程暂停（例如由于GC导致），因此当它往缓存当中写入v1时，覆盖了缓存中的v2，导致了最终的不一致（DB中为v2，缓存中为v1）。对于上述这类问题应当如何解决？目前现存的方案，全都没有彻底解决该问题，一般有以下几种方案：

- 设定稍短的过期时间兜底：在这个过期时间之内，会不一致。缺点是较短的过期时间意味着数据库的负载会更高
- 延时双删：先删除一次缓存，延时几百毫秒再删除一次。这种做法只能够进一步降低不一致的概率，但无法保证
- 在应用层引入类似版本的机制：对应用层有要求，通用性受限，不易复用

我们实现的缓存“标记删除”方案，能够彻底解决这个问题，确保缓存与数据库之间的数据保持一致。解决原理如下：

缓存中的数据是一个hash，里面有以下几个字段：

- value: 数据本身
- lockUtil: 数据锁定到期时间，当某个进程查询缓存无数据，那么先锁定缓存一小段时间，然后查询DB，然后更新缓存
- owner: 数据锁定者uuid

查询缓存时：

1. 如果数据为空，且被锁定，则睡眠100ms后，重新查询
2. 如果数据为空，且未被锁定，同步执行"取数据"，返回结果
3. 如果数据不为空，那么立即返回结果，并异步执行"取数据"

其中"取数据"的操作定义为：

1. 判断是否需要更新缓存，下面两个条件满足其一，则需要更新缓存
    - 数据为空，并且未被锁定
    - 数据的锁定已过期
2. 如果需要更新，则锁定缓存，查询DB，校验锁持有者无变化，写入缓存，解锁缓存

当DB数据更新时，通过dtm确保数据更新成功时，将缓存"标记删除"（保证原子操作的原理，后面一节展开详细讲解）

- 标记删除会将数据过期时间设定为10s，将锁设置为已过期，触发下一次查询缓存时的“取数据”

在上述的策略下：

假如最后写入数据库的版本为Vi，最后写入到缓存的版本为V，写入V的uuid为uuidv，那么一定存在以下事件序列：

数据库写入Vi -> 缓存数据被标记为删除 -> 某个查询锁定数据并写入uuidv -> 查询数据库结果V -> 缓存中的锁定者为uuidv，写入结果V

在这个序列中，V的读取发生在写入Vi之后，所以V等于Vi，保证了缓存的数据的最终一致性。

[dtm-labs/rockscache](https://github.com/dtm-labs/rockscache)已经实现了上述方法，能够确保缓存数据的最终一致性。

- `Fetch`函数实现了前面的查询缓存
- `TagDeleted`函数实现了标记删除的逻辑

感兴趣的同学，可以参考[dtm-cases/cache](https://github.com/dtm-labs/dtm-cases/tree/main/cache)，里面有详细的例子

## DB与缓存操作的原子性 

对于缓存的管理，一般业界会采用写完数据库后，删除/更新缓存数据的策略。由于保存到缓存和保存到数据库两个操作之间不是原子的，一定会有时间差，因此这两个数据之间会有一个不一致的时间窗口，通常这个窗口不大，影响较小。但是两个中间可能发生宕机，也可能发生各种网络错误，因此就有可能发生完成了其中一个，但是未完成另一个，导致数据会出现长时间不一致。

举一个场景来说明上述不一致的情况，数据用户将数据 A 修改为 B ，应用修改完数据库之后，再去删除/更新缓存，如果未发生异常，那么数据库和缓存的数据是一致的，没有问题。但是分布式系统中，可能会发生进程crash、宕机等事件，因此如果更新完数据库，尚未删除/更新缓存时，出现进程crash，那么数据库和缓存的数据就可能出现长时间的不一致。

面对这里的长时间不一致的情况，想要彻底解决，并不是一件容易的事，我们下面分各种应用情况来介绍解决方案。

#### 方案一：较短的缓存时间

这个方案，是最简单的方案，适合并发量不大应用。如果应用的并发不高，那么整个缓存系统，只需要设置了一个较短的缓存时间，例如一分钟。这种情况下数据库需要承担的负载是：大约每一分钟，需要将访问到的缓存数据全部生成一遍，在并发量不大的情况下，这种策略是可行的。

上述这种策略非常简单，易于理解和实现，缓存系统提供的语义是，大多数情况下，缓存和数据库之间不一致的时间窗口是很短的，在较低概率发生进程crash的情况下，不一致的时间窗口会达到一分钟。

应用在上述约束下，需要将一致性要求不高的数据读取，从缓存读取；而将一致性要求较高的读，不走缓存，直接从数据库查询。

#### 方案二：消息队列保证一致[#](https://dtm.pub/app/cache.html#%E6%96%B9%E6%A1%88%E4%BA%8C%EF%BC%9A%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%BF%9D%E8%AF%81%E4%B8%80%E8%87%B4)

假如应用的并发量很高，缓存过期时间需要比一分钟更长，而且应用中的大量请求不能够容忍较长时间的不一致，那么这个时候，可以通过使用消息队列的方式，来更新缓存。具体的做法是：

- 更新数据库时，同时将更新缓存的消息写入本地表，随着数据库更新操作的提交而提交。
- 写一个轮询任务，不断轮询这部分消息，发给消息队列。
- 消费消息队列中的消息，更新/删除缓存

这种做法可以保证数据库更新之后，缓存一定会被更新。但这种这种架构方案很重，这几个部分开发维护成本都不低：消息队列的维护；高效轮询任务的开发与维护。

#### 方案三：订阅 binlog[#](https://dtm.pub/app/cache.html#%E6%96%B9%E6%A1%88%E4%B8%89%EF%BC%9A%E8%AE%A2%E9%98%85-binlog)

这个方案适用场景与方案二非常类似，原理又与数据库的主从同步类似，数据库的主从同步是通过订阅binlog，将主库的更新应用到从库上，而这个方案则是通过订阅binlog，将数据库的更新应用到缓存上。具体做法是：

- 部署并配置阿里开源的 canal ，让它订阅数据库的binlog
- 通过 canal等工具 监听数据更新，同步更新/删除缓存

这种方案也可以保证数据库更新之后，缓存一定会被更新，但是这种架构方案跟前面的消息队列方案一样，也非常重。一方面 canal 的学习维护成本不低，另一方面，开发者可能只需要少量数据更新缓存，通过订阅所有的 binlog 来做这个事情，浪费了很多资源。

#### 方案四： dtm 二阶段消息方案

dtm 里的二阶段消息模式，非常适合这里的修改数据库之后更新/删除缓存，主要代码如下：

```
msg := dtmcli.NewMsg(DtmServer, gid).
	Add(busi.Busi+"/DeleteRedis", &Req{Key: key1})
err := msg.DoAndSubmitDB(busi.Busi+"/QueryPrepared", db, func(tx *sql.Tx) error {
  // update db data with key1
})
```

这段代码，DoAndSubmitDB会进行本地数据库操作，进行数据库的数据修改，修改完成后，会提交一个二阶段消息事务，消息事务将会异步调用 DeleteRedis。假如本地事务执行之后，就立刻发生了进程 crash 事件，那么 dtm 会进行回查调用 QueryPrepared ，保证本地事务提交成功的情况下，DeleteRedis 会被最少成功执行一次。

回查的逻辑非常简单，只需要copy类似下面这样的代码即可：

```
	app.GET(BusiAPI+"/QueryPrepared", dtmutil.WrapHandler(func(c *gin.Context) interface{} {
		return MustBarrierFromGin(c).QueryPrepared(dbGet())
	}))
```

这种方案的优点：

- 方案简单易用，代码简短易读
- dtm 本身是一个无状态的普通应用，依赖的存储引擎 redis/mysql 是常见的基础设施，不需要额外维护消息队列或者 canal
- 相关的操作模块化，易维护，不需要像消息队列或者 canal 在其他地方写消费者的逻辑

#### 从库延时

上述的方案中，假定缓存删除后，服务进行数据查询，总是能够查到最新的数据。但是实际的生产环境中，可能会出现主从分离的架构，而主从延时并不是一个可控的变量，那么这时候又要怎么处理？

处理方案两种：一是区分最终一致性很高和不高的缓存数据，查询数据时，将要求很高的数据必须从主库读取，而把要求不高的数据从从库读取。对于使用了rockscache的应用来说，高并发的请求都会在Redis这一层被拦截，对于一个数据，最多只会有一个请求到达数据库，因此数据库的负载已大幅降低，采用主库读取是一个实际可行的方案。

另一种方案是，主从分离需要采用不分叉的单链架构，那么链条末尾的从库必定是延迟最长的从库，此时采用监听binlog的方案，需要监听链条做末端的从库binlog，当收到数据变更通知时，按照上述方案将缓存标记为删除。

这两个方案各有优缺点，业务可以根据自己的特点采用。

## 防缓存击穿 

rockscache还可以防缓存击穿。当数据变更时，业界现有做法既可以选择更新缓存，也可以选择删除缓存，各有优劣。而标记删除法综合了两种方法的优势，并克服了两种方法的劣势：

#### 更新缓存 

采取更新缓存策略，那么会为所有的DB数据更新生成缓存，不区分冷热数据，那么会存在以下问题：

- 内存上，即使一个数据没有被读取，也会保存在缓存里，浪费了宝贵的内存资源；
- 在计算上，即使一个数据没有被读取，也可能因为多次更新，被多次计算，浪费了宝贵的计算资源。
- 上述的乱序不一致发生的概率会较高，当两个临近的更新中出现延迟，就可能触发。

#### 删除缓存 

因为前面的更新缓存做法问题较多，因此大多数的实践采用的是删除缓存策略，查询时再按需生成缓存。这种做法解决了更新缓存中的问题，但是又带来新问题：

- 那么在高并发的情况下，如果删除了一个热点数据，那么此时会有大量请求会无法命中缓存，产生缓存击穿。

为了防止缓存击穿，通用的做法是使用分布式 Redis 锁保证只有一个请求到数据库，等缓存生成之后，其他请求进行共享。这种方案能够适合很多的场景，但有些场景却不适合。

- 例如有一个重要的热点数据，计算代价比较高，需要3s才能够获得结果，那么上述方案在删除一个这种热点数据之后，就会在这个时刻，有大量请求3s才返回结果，一方面可能造成大量请求超时，另一方面3s没有释放链接，会导致并发连接数量突然升高，可能造成系统不稳定。
- 另外使用 Redis 锁时，未获得锁的这部分用户，通常会定时轮询，而这个睡眠时间不好设定。如果设定比较大的睡眠时间1s，那么对于10ms就计算出结果的缓存数据，返回太慢了；如果设定的睡眠时间太短，那么很消耗 CPU 和 Redis 性能

#### 标记删除法 

前面介绍的[dtm-labs/rockscache](https://github.com/dtm-labs/rockscache)实现的标记删除法也属于删除法，但它彻底解决了删除缓存中的击穿问题，以及击穿带来的附带问题。

1. 缓存击穿问题：标记删除法中，如果缓存中的数据不存在，那么会锁定缓存中的这条数据，因此避免了多个请求打到后端数据库。
2. 上述大量请求3s才返回数据，以及定时轮询的问题，在标记删除中也不存在，因为热点数据被标记删除时，旧版本的数据还在缓存中，会被立即返回，无需等待。

我们来看看不同的数据访问频率下，标记删除法的表现如何：

1. 热点数据，每秒1K qps，计算缓存时间5ms，此时标记删除法，大约5~8ms左右的时间里，会返回过期数据，而先更新DB，再更新缓存，因为更新缓存需要时间，也会有大约0~3ms返回过期数据，因此两者差别不大。
2. 热点数据，每秒1K qps，计算缓存时间3s，此时标记删除法，大约3s的时间里，会返回过期数据。对比于等待3s后再返回数据，那么返回旧数据，通常是更好的行为。
3. 普通数据，每秒50 qps，计算缓存时间1s，此时标记删除法的行为分析，类似2，没有问题。
4. 低频数据，5秒访问一次，计算缓存时间3s，此时标记删除法的行为与删除缓存策略基本一样，没有问题
5. 冷数据，10分钟访问一次，此时标记删除法，与删除缓存策略基本一样，只是数据比删除缓存的方式多保存10s，占用空间不大，没有问题

有一种极端情况是，那就是原先缓存中没有数据，突然大量请求到来，这种场景对，更新缓存法删除缓存法，标记删除法，都是不友好的。这种的场景是开发人员需要避免的，需要通过预热来解决，而不应当直接扔给缓存系统。当然，由于标记删除法已经把打到数据库的请求量降到最低，因此表现也不弱于任何其他方案。

## 防缓存穿透与缓存雪崩 

[dtm-labs/rockscache](https://github.com/dtm-labs/rockscache)还实现了防缓存穿透与缓存雪崩。

缓存穿透是指，缓存和数据库都没有的数据，被大量请求。由于数据不存在，缓存就也不会存在该数据，所有的请求都会直接穿透到数据库。rockscache中可以设定`EmptyExipire`设定对空结果的缓存时间，如果设定为0，那么不缓存空数据，关闭防缓存穿透

缓存雪崩是指缓存中有大量的数据，在同一个时间点，或者较短的时间段内，全部过期了，这个时候请求过来，缓存没有数据，都会请求数据库，则数据库的压力就会突增，扛不住就会宕机。rockscache可以设定`RandomExpireAdjustment`，对过期时间加上随机值，避免同时过期。

## 应用能否做到强一致？ 

上面已经介绍了缓存一致性的各种场景，以及相关的解决方案，那么是否可以保证使用缓存的同时，还提供强一致的数据读写呢？强一致的读写需求比前面的最终一致的需求场景少，但是在金融领域，也是有不少场景的。

当我们在这里讨论强一致时，我们需要先把一致性的含义做一下明确。

开发者最直观的强一致性很可能理解为，数据库和缓存保持完全一致，写数据的过程中以及写完之后，无论从数据库直接读，或者从缓存直接读，都能够获得最新写入的结果。对于这种两个独立系统之间的“强一致性”，可以非常明确的说，理论上是不可能的，因为更新数据库和更新缓存在不同的机器上，无法做到同时更新，无论如何都会有时间间隔，在这个时间间隔里，一定是不一致的。

但是应用层的强一致性，则是可以做到的。可以简单考虑我们熟悉的场景：CPU的缓存作为内存的缓存，内存作为磁盘的缓存，这些都是缓存的场景，从来没有发生过一致性问题。为什么？其实很简单，要求所有的数据使用方，只能够从缓存读取数据，而不能同时从缓存和底层存储同时读取数据。

对于DB和Redis，如果所有的数据读取，只能够由缓存提供，就可以很容易的做到强一致，不会出现不一致的情况。下面我们来根据DB和Redis的特点，来分析其中的设计：

#### 先更新缓存还是DB 

类比CPU缓存与内存，内存缓存与磁盘，这两个系统都是先修改缓存，再修改底层存储，那么到了现在的DB缓存场景是否也先修改缓存再修改DB？

在绝大多数的应用场景下，开发者会认为Redis作为缓存，当Redis出现故障时，那么应用需要支持降级处理，依旧能够访问数据库，提供一定的服务能力。考虑这种场景，一旦出现降级，先写缓存再写DB方案就有问题，一方面会丢失数据，另一方面会发生先读取到缓存中的新版本v2，再读取到旧版本v1。因此在Redis作为缓存的场景下，绝大部分系统会采取先写入DB，再写入缓存的这种设计

#### 写入DB成功缓存失败情况 

假如因为进程crash，导致写入DB成功，但是标记删除第一次失败怎么办？虽然间隔几秒之后，会重试成功，但这几秒钟的时间里，用户去读取缓存，依旧还是旧版本的数据。例如用户发起了一笔充值，资金已经进入到DB，只是更新缓存失败，导致从缓存看到的余额还是旧值。这种情况的处理很简单，用户充值时，写入DB成功时，应用不要给用户返回成功，而是等缓存更新也成功了，再给用户返回成功；用户查询充值交易时，要查询DB和缓存是否都成功了（可以查询二阶段消息全局事务是否已成功），只有两者都成功了，才返回成功。

在上述的处理策略下，当用户发起充值后，在缓存更新完成之前，用户看到的是，这笔交易还在处理中，结果未知，此时是符合强一致要求的；当用户看到交易已经处理成功，也就是缓存已更新成功，那么所有从缓存中拿到的数据都是更新后的数据，那么也符合强一致的要求。

[dtm-labs/rockscache](https://github.com/dtm-labs/rockscache)也实现了强一致的读取需求。当打开`StrongConsistency`选项，那么rockscache里`Fetch`函数就提供了强一致的缓存读取。其原理与标记删除法差别不大，仅做了很小的改变，就是不再返回旧版本的数据，而是同步等待“取数据”的最新结果

当然这个改变会带来性能上的下降，对比与最终一致的数据读取，强一致的读取一方面要等待当前“取数据”的最新结果，增加了返回延迟，另一方面要等待其他进程的结果，会产生sleep等待，耗费资源。

## 缓存降级升级中的强一致 

上述的强一致方案中，说明了其强一致的前提是：“所有的数据读取，只能够由缓存”。不过如果Redis如果发生故障，需要进行降级，那么降级的过程可能很短只有几秒，但是这个几秒内如果不能接受不可访问，还严苛的要求提供访问的话，就会出现读取缓存和读取DB混用情况，就不满足这个前提。不过因为Redis故障的频率不高，要求强一致性的应用通常配备专有Redis，因此遇见故障降级的概率很低，很多应用不会在这个地方提出苛刻的要求。

不过dtm-labs作为数据一致性领域的领导者，也深入研究了这个问题，并给出这种苛刻条件下的解决方案。

#### 升降级的过程 

现在我们来考虑应用在Redis缓存出现问题的升降级处理。一般情况下这个升降级的开关在配置中心，当修改配置后，各个应用进程会陆续收到降级配置变更通知，然后在行为上降级。在降级的过程中，会出现缓存与DB混合访问的情况，这时我们上面的方案就有可能出现不一致。那么如何处理才能够保证在这种混合访问的情况下，依旧能够让应用获取到强一致的结果呢？

混合访问的过程中，我们可以采取下面这个策略，来保证DB和缓存混合访问时的数据一致性。

- 更新数据时，使用分布式事务，保证以下操作为原子操作
    - 将缓存标记为“锁定中”
    - 更新DB
    - 将缓存“锁定中”标记去除，标记为删除
- 读取缓存数据时，对于标记为“锁定中”的数据，睡眠等待后再次读取；对于标记删除的数据，不返回旧数据，等待新数据完成再返回。
- 读取DB数据时，直接读取，无需任何额外操作

这个策略跟前面不考虑降级场景的强一致方案，差别不大，读数据部分完全不变，需要变的是更新数据。rockscache假定更新DB是一个业务上可能失败的操作，于是采用一个SAGA事务来保证原子操作，详情参见例子[dtm-cases/cache](https://github.com/dtm-labs/dtm-cases/tree/main/cache)

升降级的开启关闭有顺序要求，不能够同时开启缓存读和写，而是需要在开启缓存读的时候，所有的写操作都已经确保会更新缓存。

降级的详细过程如下：

1. 最初状态：
    - 读：混合读
    - 写：DB+缓存
2. 读降级：
    - 读：关闭缓存读。混合读 => 全部DB读
    - 写：DB+缓存
3. 写降级：
    - 读：全部DB读；
    - 写：关闭缓存写。DB+缓存 => 只写DB

升级的过程与此相反，如下：

1. 最初状态：
    - 读：全部读DB
    - 写：全部只写DB
2. 写升级：
    - 读：全部读DB
    - 写：打开写缓存。只写DB => 写DB+缓存
3. 读升级：
    - 读：部分读缓存。全部读DB => 混合读
    - 写：写DB+缓存

[dtm-labs/rockscache](https://github.com/dtm-labs/rockscache)已实现了上述强一致的缓存管理方法。

感兴趣的同学，可以参考[dtm-cases/cache](https://github.com/dtm-labs/dtm-cases/tree/main/cache)，里面有详尽的例子

## 小结 

这篇文章很长，许多的分析比较晦涩，最后将Redis缓存的使用方式做个总结：

- 最简单的方式为：较短的缓存时间，允许少量数据库修改，未同步删除缓存
- 保证最终一致，并且可防缓存击穿的方式为：二阶段消息+标记删除(rockscache)
- 强一致：二阶段消息+强一致(rockscache)
- 一致性要求最严苛的方式为：二阶段消息+强一致(rockscache)+升降级兼容

对于后两种方式，我们都推荐使用[dtm-labs/rockscache](https://github.com/dtm-labs/rockscache)来作为您的缓存方案

# 异常与子事务屏障 

分布式事务之所以难，主要是因为分布式系统中的各个节点都可能发生各种非预期的情况。本文先介绍分布式系统中的异常问题，然后介绍这些问题带给分布式事务的挑战，接下来指出现有各种常见用法的问题，最后给出正确的方案。

## NPC的挑战 

分布式系统最大的敌人可能就是NPC了，在这里它是Network Delay, Process Pause, Clock Drift的首字母缩写。我们先看看具体的NPC问题是什么：

- Network Delay，网络延迟。虽然网络在多数情况下工作的还可以，虽然TCP保证传输顺序和不会丢失，但它无法消除网络延迟问题。
- Process Pause，进程暂停。有很多种原因可以导致进程暂停：比如编程语言中的GC（垃圾回收机制）会暂停所有正在运行的线程；再比如，我们有时会暂停云服务器，从而可以在不重启的情况下将云服务器从一台主机迁移到另一台主机。我们无法确定性预测进程暂停的时长，你以为持续几百毫秒已经很长了，但实际上持续数分钟之久进程暂停并不罕见。
- Clock Drift，时钟漂移。现实生活中我们通常认为时间是平稳流逝，单调递增的，但在计算机中不是。计算机使用时钟硬件计时，通常是石英钟，计时精度有限，同时受机器温度影响。为了在一定程度上同步网络上多个机器之间的时间，通常使用NTP协议将本地设备的时间与专门的时间服务器对齐，这样做的一个直接结果是设备的本地时间可能会突然向前或向后跳跃。

分布式事务既然是分布式的系统，自然也有NPC问题。因为没有涉及时间戳，带来的困扰主要是NP。

## 异常分类 

我们以分布式事务中的TCC作为例子，看看NP带来的影响。

一般情况下，一个TCC回滚时的执行顺序是，先执行完Try，再执行Cancel，但是由于N，则有可能Try的网络延迟大，导致先执行Cancel，再执行Try。

这种情况就引入了分布式事务中的两个难题：

- **空补偿：** Cancel执行时，Try未执行，事务分支的Cancel操作需要判断出Try未执行，这时需要忽略Cancel中的业务数据更新，直接返回
- **悬挂：** Try执行时，Cancel已执行完成，事务分支的Try操作需要判断出Cancel已执行，这时需要忽略Try中的业务数据更新，直接返回

分布式事务还有一类需要处理的常见问题，就是重复请求

- **幂等：** 由于任何一个请求都可能出现网络异常，出现重复请求，所有的分布式事务分支操作，都需要保证幂等性

因为空补偿、悬挂、重复请求都跟NP有关，我们把他们统称为子事务乱序问题。在业务处理中，需要小心处理好这三种问题，否则会出现错误数据。

## 异常原因 

下面看一个网络异常的时序图，更好的理解上述几种问题

![exception](https://dtm.pub/assets/exception.4254ab59.jpg)

- 业务处理请求4的时候，Cancel在Try之前执行，需要处理空回滚
- 业务处理请求6的时候，Cancel重复执行，需要幂等
- 业务处理请求8的时候，Try在Cancel后执行，需要处理悬挂

## 现有方案的问题 

我们看到开源项目dtm之外，包括各云厂商，各开源项目，他们给出的业务实现建议大多类似如下（这也是大多数用户最容易想到的方案）：

- **空补偿：** “针对该问题，在服务设计时，需要允许空补偿，即在没有找到要补偿的业务主键时，返回补偿成功，并将原业务主键记录下来，标记该业务流水已补偿成功。”
- **防悬挂：** “需要检查当前业务主键是否已经在空补偿记录下来的业务主键中存在，如果存在则要拒绝执行该笔服务，以免造成数据不一致。”

上述的这种实现，能够在大部分情况下正常运行，但是上述做法中的“先查后改”在并发情况下是容易掉坑里的，我们分析以下如下场景：

- 正常执行顺序下，Try执行时，在查完没有空补偿记录的业务主键之后，事务提交之前，如果发生了进程暂停P，或者事务内部进行网络请求出现了拥塞，导致本地事务等待较久
- 全局事务超时后，Cancel执行，因为没有查到要补偿的业务主键，因此判断是空补偿，返回
- Try的进程暂停结束，最后提交本地事务
- 全局事务回滚完成后，Try分支的业务操作没有被回滚，产生了悬挂

事实上，NPC里的P和C，以及P和C的组合，有很多种的场景，都可以导致上述竞态情况，就不一一赘述了。

虽然这种情况发生的概率不高，但是在金融领域，一旦涉及金钱账目，那么带来的影响可能是巨大的。

PS：幂等控制如果也采用“先查再改”，也是一样很容易出现类似的问题。解决这一类问题的关键点是要利用唯一索引，“以改代查”来避免竞态条件。

## 子事务屏障 

我们在dtm中，首创了子事务屏障技术，使用该技术，能够非常便捷的解决异常问题，极大的降低了分布式事务的使用门槛。

子事务屏障能够达到下面这个效果，看示意图：

![barrier](https://dtm.pub/assets/barrier.1b8ad19e.jpg)

所有这些请求，到了子事务屏障后：不正常的请求，会被过滤；正常请求，通过屏障。开发者使用子事务屏障之后，前面所说的各种异常全部被妥善处理，业务开发人员只需要关注实际的业务逻辑，负担大大降低。 子事务屏障提供了方法BranchBarrier.CallWithDB ，方法的原型为：

```
func (bb *BranchBarrier) CallWithDB(db *sql.DB, busiCall BusiFunc) error
```

业务开发人员，在busiCall里面编写自己的相关逻辑，调用 BranchBarrier.CallWithDB 。 BranchBarrier.CallWithDB 保证，在空回滚、悬挂等场景下，busiCall不会被调用；在业务被重复调用时，有幂等控制，保证只被提交一次。

子事务屏障会管理TCC、SAGA、事务消息等，也可以扩展到其他领域

## 原理 

子事务屏障技术的原理是，在本地数据库，建立分支操作状态表dtm_barrier，唯一键为全局事务id-分支id-分支操作（try|confirm|cancel）

1. 开启本地事务
2. 对于当前操作op(try|confirm|cancel)，insert ignore一条数据gid-branchid-op，如果插入不成功，提交事务返回成功（常见的幂等控制方法）
3. 如果当前操作是cancel，那么在insert ignore一条数据gid-branchid-try，如果插入成功（注意是成功），则提交事务返回成功
4. 调用屏障内的业务逻辑，如果业务返回成功，则提交事务返回成功；如果业务返回失败，则回滚事务返回失败

在此机制下，解决了乱序相关的问题

- 空补偿控制--如果Try没有执行，直接执行了Cancel，那么3中Cancel插入gid-branchid-try会成功，不走屏障内的逻辑，保证了空补偿控制
- 幂等控制--2中任何一个操作都无法重复插入唯一键，保证了不会重复执行
- 防悬挂控制--Try在Cancel之后执行，那么Cancel会在3中插入gid-branchid-try，导致Try在2中不成功，就不执行屏障内的逻辑，保证了防悬挂控制

对于SAGA、二阶段消息，也是类似的机制。

## 原理图解(可选读) 

下面我们以图的方式来详解子事务屏障，因为Confirm操作不涉及空补偿和悬挂，所以重点看Try与Cancel，Try对应图中的A，Cancel对应图中的C：

子事务屏障中对应的幂等处理部分：

![barrier-idem](https://dtm.pub/assets/barrier-idem.bdac5e8c.png)

这部分就是常规的幂等处理部分，往数据库中插入一个唯一键，如果是重复请求，那么插入失败，直接失败返回。

子事务屏障技术就是在上述的幂等处理部分，添加一个步骤--补偿服务再插入一条A记录，正常流程下，会因为唯一键冲突导致插入失败，往下执行业务。

![barrier-steps](https://dtm.pub/assets/barrier-steps.e4556b5a.svg)

当发生乱序，假设C在A前面执行，那么会发生下面的时序图：

![barrier-unordered](https://dtm.pub/assets/barrier-unordered.8fee733c.svg)

- 对于C操作，他先于A执行，是一个空补偿；此时C操作插入A记录时，发现插入成功，直接返回
- 对于A操作，他在C之后执行，是一个悬挂；此时A操作插入A记录时，发现插入失败，直接返回

这两种情况都会被子事务屏障拦截返回，而不执行内部的业务操作。可以看到子事务屏障非常巧妙的解决了幂等、空补偿和悬挂三个问题。

## 竞态分析 

上面分析了Try和Cancel的执行时间没有重叠的情况下，能够解决空补偿和悬挂问题。如果出现了Try和Cancel执行时间重叠的情况，我们看看会发生什么。

假设Try和Cancel并发执行，Cancel和Try都会插入同一条记录gid-branchid-try，由于唯一索引冲突，那么两个操作中只有一个能够成功，而另一个则会等持有锁的事务完成后返回。

- 情况1，Try插入gid-branchid-try失败，Cancel操作插入gid-branchid-try成功，此时就是典型的空补偿和悬挂场景，按照子事务屏障算法，Try和Cancel都会直接返回
- 情况2，Try插入gid-branchid-try成功，Cancel操作插入gid-branchid-try失败，按照上述子事务屏障算法，会正常执行业务，而且业务执行的顺序是Try在Cancel前
- 情况3，Try和Cancel的操作在重叠期间又遇见宕机等情况，那么至少Cancel会被dtm重试，那么最终会走到情况1或2。

综上各种情况的详细论述，子事务屏障能够在各种NP情况下，保证最终结果的正确性。

## 优点 

事实上，子事务屏障有大量优点，包括：

- 两个insert判断解决空补偿、防悬挂、幂等这三个问题，比其他方案的三种情况分别判断，逻辑复杂度大幅降低
- dtm的子事务屏障是SDK层解决这三个问题，业务完全不需要关心
- 性能高，对于正常完成的事务（一般失败的事务不超过1%），子事务屏障的额外开销是每个分支操作一个SQL，比其他方案代价更小。

## 支持的存储 

目前子事务屏障已经支持了

- 数据库：包括 Mysql, Postgres, 以及与Mysql，Postgres兼容的数据库
- 缓存 Redis：采用 Lua 脚本事务支持
- Mongo：采用 Mongo 的事务支持

在子事务屏障的支持下，您可以将Redis、Mongo和数据库的事务组合在一起，形成一个全局事务。相关用法，可以在[dtm-examples](https://github.com/dtm-labs/dtm-examples)里面找到

理论上支持事务的各种存储都可以轻松实现子事务屏障，例如 TiKV 等，如果较多用户有这样的需求，我们将会快速支持。

## 对接orm库 

barrier提供了sql标准接口，但大家的应用通常都会引入更高级的orm库，而不是裸用sql接口，因此需要进行转化. 相关的对接参考[对接ORM](https://dtm.pub/ref/sdk.html#db)

## 小结 

子事务屏障技术，为DTM首创，它的意义在于

- 算法简单易实现
- 系统统一的解决方案，易维护
- 提供了简单易用的接口，易使用

在这子事务屏障技术的帮助下，开发人员彻底的从网络异常的处理中解放出来。原先需要投入一位架构师处理这类异常，借助dtm的子事务屏障，只需要一个普通开发工程师就可以完成

该技术目前需要搭配DTM事务管理器，目前SDK已经提供给Go、Java、Python、c#、PHP语言的开发者。其他语言的sdk正在规划中。对于其他的分布式事务框架，只要提供了合适的分布式事务信息，也能够按照上述原理，快速实现该技术。

# 最终成功 

dtm里多种事务模式中，都出现了操作最终成功的要求，这是什么含义呢？哪些场景是最终成功的？为什么要求最终成功？应用怎么设计？

最终成功并不是说要保证100%的成功，它允许暂时性失败：包括网络故障，系统宕机，系统bug；但是一旦暂时性的问题解决之后，在业务恢复之后，需要返回成功。

最终成功的另一个说法是，该操作能够最终成功，即通过不断重试，最后会返回成功。

## 最终成功的情况 

最终成功的情况包括以下方面

- 二阶段消息中的分支操作
- SAGA中的补偿操作
- TCC的Confirm和Cancel操作

您的业务需要保证以上的操作，在业务逻辑上是可以最终成功的。

## 为什么要最终成功 

假如您的业务使用了SAGA，然后依次出现下面情况：

- 某个操作出现失败需要回滚
- 回滚的过程中，遇见回滚失败
- 此时SAGA事务既无法往前执行，也无法往后回滚

这是由于您的业务系统逻辑设计中的问题，分布式事务没有任何办法解决，所以您的业务需要保证回滚是可以最终成功的。

## 应用如何设计

- 二阶段消息的分支操作是最终成功的，因为二阶段消息不支持回滚。如果您需要回滚，那么请采用其他事务模式
    
- SAGA事务如果有些正向操作是无法回滚的，那么您可以用普通非并发的SAGA，将可回滚的分支Ri放在前面，不可回滚的分支Ni放在后面。如果分支Ri正向操作失败，则会回滚Ri，一旦到了Ni，保证Ni的正向操作最终成功，这样也能够保证SAGA事务的正确运行
    
- TCC事务的Confirm/Cancel是最终成功的，一般的设计是在TCC的Try阶段预留资源，检查约束条件，然后在Confirm阶段修改数据，在Cancel阶段释放预留的资源。经过精心的设计，能够在业务逻辑上保证Confirm/Cancel的最终成功
    

## 注意点 

在实际的业务应用中，可能会出现某些应用bug，导致要求最终成功的操作，一直无法成功，导致数据一直无法达到最终一致。建议开发者对全局事务表进行监控，发现重试超过3次的事务，发出报警，由运维人员找开发手动处理，参见[dtm的运维](https://dtm.pub/deploy/maintain.html)
