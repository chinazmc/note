#review/llm

# prompt工程
## 编写具体，清晰的指令
### 使用分隔符清晰表示输入的不同部分
### 寻求结构化的输出
### 要求模型检查是否满足条件
### 提供少量示例

## 给模型时间去思考
### 给出指定完成任务所需步骤
### 指导模型在下结论之前找出一个自己的解法
## 迭代优化
具体来说，首先编写初版 Prompt，然后通过多轮调整逐步改进，直到生成了满意的结果。对于更复杂的应用，可以在多个样本上进行迭代训练，评估 Prompt 的平均表现。在应用较为成熟后，才需要采用在多个样本集上评估 Prompt 性能的方式来进行细致优化。因为这需要较高的计算资源。

总之，Prompt 工程师的核心是掌握 Prompt 的迭代开发和优化技巧，而非一开始就要求100%完美。通过不断调整试错，最终找到可靠适用的 Prompt 形式才是设计 Prompt 的正确方法。

# 使用rag技术之 QA抽取
构建一个基础版的RAG是非常简单的，甚至使用扣子、Dify等平台，熟练的情况下都用不了5分钟，即使使用Langchain、LlamaIndex等框架，搭建完整流程，代码也不会超过100行。但基础版的问答效果往往较差。
先切割文档
```python
from uuid import uuid4  
import os  
import pickle  
  
from langchain.text_splitter import RecursiveCharacterTextSplitter  
  
def split_docs(documents, filepath, chunk_size=400, chunk_overlap=40, seperators=['\n\n\n', '\n\n'], force_split=False):  
    if os.path.exists(filepath) and not force_split:  
        print('found cache, restoring...')  
        return pickle.load(open(filepath, 'rb'))  
  
    splitter = RecursiveCharacterTextSplitter(  
        chunk_size=chunk_size,  
        chunk_overlap=chunk_overlap,  
        separators=seperators  
    )  
    split_docs = splitter.split_documents(documents)  
    for chunk in split_docs:  
        chunk.metadata['uuid'] = str(uuid4())  
  
    pickle.dump(split_docs, open(filepath, 'wb'))  
  
    return split_docs
```

## 构建qa
```python
qa_gen_prompt_tmpl = """  
我会给你一段文本（<document></document>之间的部分），你需要阅读这段文本，分别针对这段文本生成8个问题、用户回答这个问题的上下文，和基于上下文对问题的回答。  
  
对问题、上下文、答案的要求：  
  
问题要与这段文本相关，不要询问类似“这个问题的答案在哪一章”这样的问题  
上下文：上下文必须与原始文本的内容保持一致，不要进行缩写、扩写、改写、摘要、替换词语等  
答案：回答请保持完整且简洁，无须重复问题。答案要能够独立回答问题，而不是引用现有的章节、页码等  
  
返回结果以JSON形式组织，格式为[{"question": "...", "context": ..., "answer": "..."}, ...]。  
如果当前文本主要是目录，或者是一些人名、地址、电子邮箱等没有办法生成有意义的问题时，可以返回[]。  
  
下方是文本：  
<document>  
{{document}}  
</document>  
  
请生成结果：  
"""  
  
qa_gen_prompt_tmpl_large_context = """  
我会给你一段文本（<document></document>之间的部分），你需要阅读这段文本，分别针对这段文本生成2个问题，和基于这段文本对问题的回答，回答请保持完整，无须重复问题。  
尽可能创建一些需要综合*大段*文本才能回答的问题，但不要问类似“这一段主要讲了什么内容”这样的问题，答案要能够独立回答问题，而不是引用现有的章节、页码等；不要问具体过于细节的问题，例如“海湾国家的2024年预期经济增长率是多少”，而是尽可能问类似“2024年全球经济的几大趋势是什么”、“受局部中东地区紧张局势影响，可能对全球原物料有哪些影响”。  
返回结果以JSON形式组织，格式为[{"question": "...", "answer": "..."}, ...]。  
如果当前文本主要是目录，或者是一些人名、地址、电子邮箱等没有办法生成有意义的问题时，可以返回[]。  
  
下方是文本：  
<document>  
{{document}}  
</document>  
  
请生成结果：  
"""
```

```python
from openai import OpenAI  
import time  
import random  
  
client = OpenAI(  
    api_key=os.environ['API_KEY'],  
    base_url=os.environ['BASE_URL']  
)  
  
def build_qa_prompt(prompt_tmpl, text):  
    prompt = prompt_tmpl.replace('{{document}}', text).strip()  
    return prompt  
  
def chat(prompt, max_retry=3, debug=False, model_name='qwen-long', temperature=0.85, top_p=0.95):  
    def do_chat(prompt):  
        completion = client.chat.completions.create(  
            # model="Qwen/Qwen2-72B-Instruct",  
            model=model_name,  
            messages=[      
                {"role": "system", "content": "你是一个有用的人工智能助手"},      
                {"role": "user", "content": prompt}  
            ],  
            top_p=top_p,  
            temperature=temperature  
        )  
        return completion.choices[0].message.content  
  
    while max_retry > 0:  
        try:  
            return do_chat(prompt)  
        except Exception as e:  
            max_retry -= 1  
            sleep_seconds = random.randint(1, 4)  
            if debug:  
                print(f"{str(e)}, remain retry: {max_retry}, sleeping {sleep_seconds}s {prompt}")  
            time.sleep(sleep_seconds)  
    return None
```

## qa质量检查
```python
qa_check_prompt_tmpl = """  
你是一个金融领域的专家，现在有人根据一份经济发展报告，构造了一些问题，并对问题进行了回答。  
你的任务是对这些问题（<question></question>之间的部分）和回答（<answer></answer>）进行打分。  
  
结果请以JSON形式组织，格式如下（<result></result>之间的部分）：  
<result>  
{"score": ..., "reason": ...}  
</result>  
其中score是对问题-回答的打分，分值是一个int类型的值，取值范围为1-5。reason是打分的理由。  
  
好的问题，应该是询问事实、观点等，不好的问题，通常要求做一些文本摘要等初级文字处理工作，类似于“这一段描述了什么”，“文本描述了什么”；或者询问的内容是图相关的，例如“图4展示了什么数据？”。  
好的答案，应该能够回应问题，而不是回答无关的内容，不好的回答，会给出在原文中的引用，例如“第3章”等。  
  
问题：  
<question>  
{{question}}  
</question>  
  
参考答案：  
<answer>  
{{answer}}  
</answer>  
  
请进返回JSON格式的数据即可，不要添加其他任何描述性信息。  
"""
```

大模型：是一组非常多亿的参数组成的集合。

大量的数据，发送给程序进行成语接龙，用来猜测下一个词，这就是无监督学习，这个样子的话就是gpt3,但是这个样子的话返回的结果就是不合人的预期，那么可以通过加入人的答案来进行反馈学习，让模型学到人的喜好，以此来修改自己的模型参数，然后就是再进行人对答案的满意程度来进行反馈，来修改模型参数。

  

llm对于rag来作用来说的话，就是通过导入文本，将文本好好的切分，切分的质量会直接影响到答案，比如如果按行切分，那么可能答案是需要通过多行来组织的，那么按行切分就会导致不能很好回答问题，所以比较粗糙的方案是每一行都带着前面一行或者两行。

切分好的文档，发送给embeading模型来向量化，向量化之后发送给向量数据库，然后从向量数据库中获取数据之后发送给大模型来放回结果。

  

Attention is all you need

将Source中的构成元素想象成是由一系列的<Key,Value>数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。

![](https://h85kvts7sr.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDBmNzkyMWQ2MGI4NDNiYzA1Nzc3ODE3YThlNzhiMzRfeWhkUXk0WVlOaDZKODdqMjNGYXRhTm5Pa1Qya3U2a1hfVG9rZW46VEFKaWI1dENWb0Fkdld4WkhYYmNtZHhRbkVmXzE3MzM0NzQwNjg6MTczMzQ3NzY2OF9WNA)

Self Attention 指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。

I am enjoy the sunny weather。 注意力工程的意思就是关注最明显的关键字，比如 enjoy, sunny,weather.所以通过这些关键字向量化之后获取向量，然后再通过关键字的权重来加权，或者最贴近意思的语句。

  

  

在openai或者qianwen中获取apikey，其中模型大致会分为聊天模型和文本模型，聊天模型和文本模型的返回的结构体是不一样的，例如，聊天模型中会出现role，而文本模型不会（还有其他区别，我忘记了）。

我们通过openai的apikey创建 client，然后通过chatComponent或者component去发起调用（还有对于大文本的stream调用），接受返回的json结构，然后通过choice中的content来获取响应。

如果是chatComponent的话就会有role： system|user|assistant

调用的过程中，可以通过max_token来限制响应使用的toekn数量来防止扣费太多。token的消费是通过输入的token和响应的token来计费的。

  

![](https://h85kvts7sr.feishu.cn/space/api/box/stream/download/asynccode/?code=MDIwNjAxNjZhYTQ5ZmY4OGRhNjM0NmM0OTNhYmU0YmNfVnJkeXd4cmR5b0ZaSDNmYUs2VzFsS0pOUnY3NjBYeGZfVG9rZW46THVJRGJCQWtGb3FKU014d1JwRWN3YWV3bnFlXzE3MzM0NzQwNjg6MTczMzQ3NzY2OF9WNA)

  

上面反复提到了知识库，在RAG的流程中，知识库会经历下面4个步骤处理，如下图所示：

- 加载：可以简单理解成把文档读取成字符串
    
- 切分：按照特定长度，把文档切分成文本片段，做这一步是因为，后面要使用向量模型将切分后的文本片段（其实就是段落或者句子）转换成向量，由于向量模型输入长度限制，所以这一步必须按照特定长度切分
    
- 向量化：这一步会使用一个向量模型，将一个**句子**转换成一个向量，跟word2vec模型其实不是一个东西，word2vec模型是把一个字符或者一个词，转换成一个向量，而在RAG中说的向量模型，是把句子转换成向量，这样后续就可以使用向量计算，来比较句子之间的相似性，所谓RAG中的检索，很大程度是依赖向量，所以这块很重要
    
- 向量存储：这一步一般会使用向量库存储向量化好的文本片段，以及一些元数据信息，如文件名、ID之类的，向量库是类似MySQL、PostgreSQL一样的一个数据库，只不过它专注于存储向量，典型的有Milvus、FAISS、Chroma、Qdrant、Pinecone、Weatiate、PGVector等
    

![](https://h85kvts7sr.feishu.cn/space/api/box/stream/download/asynccode/?code=YWRiOWFlMDhiYTEyODM4Y2E0NWEwMWVlMGQyYmU2YWVfNHlRTmVSelFWc1BEalFTYVA2NVpuQnBnMEJsQXRPemdfVG9rZW46WlVXdGJyVE8zb2w0NE94blFPaWNoMHpCbnJlXzE3MzM0NzQwNjg6MTczMzQ3NzY2OF9WNA)

知识库处理好，保存到向量库之后，当用户提问时，会将用户问题也进行向量化，然后拿用户问题向量，去向量库中，使用余弦相似度（只是原理，后续后再详细展开），检索到最相似的一些句子，然后将用户问题、检索到的相似句子，一同组成一个Prompt，输入大模型，生成答案，如下图所示：

![](https://h85kvts7sr.feishu.cn/space/api/box/stream/download/asynccode/?code=OTFiZGEwZmMxMmI2NzJiNjQ3ZmQyZDNkMjkzZGMwNmVfaGNSVlVLbGU4NU4xWFpNaG5sbWFLVnNwYUZLTW84UE9fVG9rZW46Q1dzTGJyTUJ4b2ZzWlR4Sml2ZGM1amo5bkhnXzE3MzM0NzQwNjg6MTczMzQ3NzY2OF9WNA)

为了方便后续对文档问答效果进行优化，此处对中间环节——检索，进行评估。

注意，一般这一步评估也是比较麻烦的，因为文档问答，答案来源于文档片段，如果回答错误，不能说明检索一定错误，反过来，如果答案正确，那么在检索环节，只要正确回答的文本“来自”所检索的文档片段，就应该算检索正确，但具体回答是否“来自”文档片段时，有技术上的问题，具体来说，有以下几点：

- 不能直接拿字符串匹配，因为生成的答案经过了大模型的加工，不能保整与检索的文档片段中的文字一字不差
    
- 使用向量模型，将两者转换成向量，计算向量相似度，但这样面临卡阈值的问题，到底阈值多少算是答案参考了知识片段
    
- 使用字符串模糊匹配的方式，也有跟计算向量相似度类似的卡阈值问题
    
- 最终答案可能来源于原本相连的段落，但由于文档切分，将整个段落切分到了两个文档片段，这样虽然可能最后回答正确，但单独拿出每一个片段来，跟答案的相似度可能都不高
    

  

  

通常的认知是，自动化评估精度并不高，在吴恩达的DeepLearning.AI网站中的Building and Evaluating Advanced RAG课程提到，目前自动化测评，总体与人工测评的一致性，只有80%多，这个指标供大家参考，但实际情况下一般不会这么低，一方面是这个课程是23年大语言模型总体上还没现在这么强的时候出的，另一方面是，课程中介绍的方法（也是本文要介绍的方法）没有参考答案，而实际工作中一般是会准备参考答案的，在有参考答案的情况下，自动化评估与人工评估的一致性，还是容易达到比较高的水平的

本文所要介绍的方法，借助TruLens库，评估三个指标，TruLens称之为RAG三元组（RAG triad），由三个评估指标组成：上下文相关性（Context Relevance）、依据性（Groundedness）和答案相关性（Answer Relevance）。如下图所示。这三个评估指标，可以看做是RAGAS的子集，我们后面会有文章介绍如何使用RAGAS进行评估。

![](https://h85kvts7sr.feishu.cn/space/api/box/stream/download/asynccode/?code=ODljYzEyMThlZWMwODA3YjBmZTU3MGJmYzY5NDcxNDdfTnVVM2lvZkVpNmNSV244ZHlIUUM0eTh4YnpheVlZNTZfVG9rZW46VFlUb2JyYWJqb3BxUlB4c2NkbGNZVzE3bmNiXzE3MzM0NzQwNjg6MTczMzQ3NzY2OF9WNA)

(https://www.trulens.org/trulens_eval/getting_started/core_concepts/rag_triad/)

  

- 上下文相关性（Context Relevance）：任何RAG应用程序的第一步是信息检索；为了验证检索的质量，需要确保每个上下文片段与输入查询密切相关。这一点至关重要，因为LLM将使用这些上下文来形成答案，因此任何不相关的信息都可能导致幻觉。TruLens通过使用序列化记录的结构来评估上下文的相关性
    
- 依据性（Groundedness）：检索到上下文之后，LLM将其形成答案。LLM经常会偏离提供的事实，夸大或扩展成听起来正确的回答。为了验证RAG的依据性，可以将回答分解成单独的陈述，并在检索到的上下文中独立寻找支持每个陈述的证据
    
- 答案相关性（Answer Relevance）：回答仍然需要帮助解决原始问题，可以通过评估最终回答与用户输入的相关性来验证这一点
    

TruLens默认使用OpenAI的模型作为打分模型，本文提供了更多的选项，可以使用OpenAI兼容的LLM，例如千问，也可以使用Ollama提供的模型

  

提示词一般都三个要素，角色，上下文context,任务task.

  

prompt工程中，要注意防止提示词注入，要记得用分隔符切割（明确只能使用分隔符包含的内容来做操作），或者限制特定输入，限制输入字数，或者通过模型来检测是否有注入。

提示的过程，可以通过 一步步思考(零成本思维链)，提供少量实例（少样本学习），询问大模型不要先给出答案反而询问它是否理解问题并提出更多问题来完善细节。

  

# 提示工程

## 提示原则

### [使用分隔符清晰地表示输入的不同部分](https://datawhalechina.github.io/llm-cookbook/#/C1/2.%20%E6%8F%90%E7%A4%BA%E5%8E%9F%E5%88%99%20Guidelines?id=_11-%e4%bd%bf%e7%94%a8%e5%88%86%e9%9a%94%e7%ac%a6%e6%b8%85%e6%99%b0%e5%9c%b0%e8%a1%a8%e7%a4%ba%e8%be%93%e5%85%a5%e7%9a%84%e4%b8%8d%e5%90%8c%e9%83%a8%e5%88%86)

分隔符还可以用来减少提示词注入的风险

### 寻求结构化的输出

### [1.3 要求模型检查是否满足条件](https://datawhalechina.github.io/llm-cookbook/#/C1/2.%20%E6%8F%90%E7%A4%BA%E5%8E%9F%E5%88%99%20Guidelines?id=_13-%e8%a6%81%e6%b1%82%e6%a8%a1%e5%9e%8b%e6%a3%80%e6%9f%a5%e6%98%af%e5%90%a6%e6%bb%a1%e8%b6%b3%e6%9d%a1%e4%bb%b6)

如果任务包含不一定能满足的假设（条件），我们可以告诉模型先检查这些假设，如果不满足，则会指出并停止执行后续的完整流程。您还可以考虑可能出现的边缘情况及模型的应对，以避免意外的结果或错误发生。

在如下示例中，我们将分别给模型两段文本，分别是制作茶的步骤以及一段没有明确步骤的文本。我们将要求模型判断其是否包含一系列指令，如果包含则按照给定格式重新编写指令，不包含则回答“未提供步骤”。

  

### 提供少量示例用于学习

### Step by step用于给模型思考的时间

可以指导大模型自己一步步来，或者自己给出明确的每一步是什么

  

# 检测输入

通过 OpenAI 的 Moderation API 或者大模型来检查输入是否有不好情绪或者注入行为。

# 思维链

将一个复杂的问题分解为多个小问题，要求大模型依次回答。或者叫大模型自己就要分解成多个问题，并进行回答。

  

# 处理输入，链式

链式提示是将复杂任务分解为多个简单Prompt的策略。在本章中，我们将学习如何通过使用链式 Prompt 将复杂任务拆分为一系列简单的子任务。你可能会想，如果我们可以通过思维链推理一次性完成，那为什么要将任务拆分为多个 Prompt 呢？

主要是因为链式提示它具有以下优点:

1. 分解复杂度，每个 Prompt 仅处理一个具体子任务，避免过于宽泛的要求，提高成功率。这类似于分阶段烹饪，而不是试图一次完成全部。
    
2. 降低计算成本。过长的 Prompt 使用更多 tokens ，增加成本。拆分 Prompt 可以避免不必要的计算。
    
3. 更容易测试和调试。可以逐步分析每个环节的性能。
    
4. 融入外部工具。不同 Prompt 可以调用 API 、数据库等外部资源。
    
5. 更灵活的工作流程。根据不同情况可以进行不同操作。
    

  

# 检查结果

通过openai的Moderation 或者大模型来检查结果是否合理或者是否相关。

  

  

# 增强检索的手段

## embedding模型微调

  

## rerank以及rerank模型微调

rerank是通过模型判断一堆文档的相关性顺序

## Multi query

通过一个问题然后通过一个模型生成多个问题，然后用这些问题一起去向量数据库检索出文档，然后再进行去重之后发给大模型出答案。

## RAG Fusion

这种技术会对多路召回的结果通过一个称为RRF（Reciprocal Rank Fusion）的简单公式进行排序，RRF排序计算简单，并不会大幅增加线上开销，却在不少场景中可以取得很好的效果，原理如下图：

为方便计算，假设RRF公式中的设置为1，设置每个相似问题检索5个文档片段，最终进入LLM生成时截断为3个文档片段，他们三个相似问召回的文档片段分别如下：

- 报告是由哪个组织发布的？→ [A, B, C, D, E]
    
- 谁能找到报告的来源？→ [A, D, C, E, B]
    
- 什么实体负责公布这份报告？→ [A, B, D, E, C]
    

分别计算这5篇文档的RRF分：

RRF(A)=1/(1+1)+1/(1+1)+1/(1+1)=1.5

RRF(B)=1/(2+1)+1/(5+1)+1/(2+1)=0.833

RRF(C)=1/(3+1)+1/(3+1)+1/(5+1)=0.666

RRF(D)=1/(4+1)+1/(2+1)+1/(3+1)=0.783

RRF(E)=1/(5+1)+1/(4+1)+1/(4+1)=0.566

根据RRF的得分，最终进入LLM的知识片段为A, B, D

  

## bm25来混合检索

通过bm25和向量数据库搜索出来之后，再去rag fusion。

  

## HyDE

使用HyDE最核心的部分是Prompt+LLM+Embedding，其中Prompt控制LLM怎样生成答案，Embedding用来在LLM生成答案后，使用这个答案去知识库进行检索，核心代码如下，其实际应用时，用`hyde_embedding`代替常规的`embeddings`即可。使用HyDE比较适合的场景是，让LLM直接生成的答案，和正确的上下文片段，只相差核心的几个词，或者关键数值，这种情况下是比较有效的。

另外，由于对每个问题，都需要先调用LLM生成一个答案，整个链路的耗时会大幅度增加。

## Step-Back Prompting

Take a Step Back这篇论文提出的动机，它通过Prompt让LLM“退后一步”，不要直接尝试解决问题，而是思考解决这个问题更高层次、更抽象的问题是什么。

例如论文中所举的例子：“理想气体的压强P，如果温度增加2倍，体积增加8倍会发生什么变化?”，即使使用CoT，LLM也依然无法回答正确，但使用Step-Back Prompting，让LLM先回忆出来理想气体公式，然后让LLM应用这个公式，问题得到顺利解决。又比如“埃斯特拉·利奥波德在1954年8月至1954年11月期间就读于哪所学校?”，这种问题在RAG中直接检索的话，很可能无法检索到正确的知识片段，但如果换成一个更抽象的问题：“埃斯特拉·利奥波德的教育历史是什么”则很有可能检索到正确片段从而正确回答问题。

```Python
from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate

examples = [
    {
        "input": "美联储预计下半年利率会下调多少？",
        "output": "美联储下半年的利率政策会如何制定",
    },
    {
        "input": "如何评估一家公司股票的内在价值？",
        "output": "什么方法可以用来评估资产的真实价值？",
    }
]
# 转换为消息
example_prompt = ChatPromptTemplate.from_messages(
    [
        ('human', '{input}'),
        ('ai', '{output}')
    ]
)
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples
)
prompt = ChatPromptTemplate.from_messages([
    (
        'system',
        """你是金融领域的专家。你的任务是把一个问题改写成一个更一般或更抽象的问题，但注意仅问题改写得更一般即可，如果问题有主体，问题的主体要保持不变。如果你不确定如何改写，请保持原问题不变。直接输出改写后的问题即可，不需要包含“改写之后的问题”之类的描述性内容。这里有几个例子:"""
    ),
    few_shot_prompt,
    ('user', '{question}')
])
```

  

## Parent Document Retriever

在Langchain中，`ParentDocumentRetriever`通过分割和存储文档的小片段来尝试达到这种平衡。在检索的过程中，`ParentDocumentRetriever`首先获取小的上下文片段，然后通过关联id的方式返回它所在的较大上下文。

在LlamaIndex中，这种技术叫做Sentence-window retrieval，下图是Deeplearning.AI网站课程对应的截图，检索是对小的上下文片段计算相似度，而送入LLM的是小的上下文片段对应的大片段。

  

使用`ParentDocumentRetriever`，它内部会建立一个类似下表的关联，然后对每一个child_doc构建索引，当进行检索时，会使用问题的embedding查找child_doc对应的embedding，然后根据child_doc_id找到对应的parent_doc_id，从而找到parent_doc。这种方式在某种程度上解决了上文提到的困境。

  

## 超长上下文LLM vs. RAG

当前主流的LLM基本上都支持超长上下文了，对于很多文档问答任务，将所有文档输入LLM都不会导致上下文超长，甚至有不少文章声称RAG已死，本文我们进行一番探究，探讨超长上下文的LLM出现后，是否还有必要使用RAG。

大语言模型在诞生之日起，其中一个发展方向就是不断追求更大的上下文长度，它带来的好处也显而易见，首先提升了模型的上下文理解能力，帮助模型更好地捕捉前后文之间的语义联系，特别是在复杂的语言场景中，如法律、技术文档或学术论文等。其次，长上下文能够减少信息碎片化，避免由于上下文切片而导致的信息丢失或逻辑割裂，从而确保生成的文本更加连贯、准确。此外，长上下文还增强了模型的推理能力，使其能够在长时间的对话或复杂的多段落问题中进行跨段推理，提供更加精确的回答。

  

但是qwen模型对于超长上下文的解析不够好，chat4的话很不错，但是很贵。

  

# 实践

在我们的探索之旅中，我们将实现一个完整的问答系统，一种能理解并回应人类语言的人工智能。在这个过程中，我们将使用 OpenAI 的相关API并引用相关函数，来帮助我们快速搭建一个高效且精准的模型。然而，我们需要注意到，在中文的理解和处理方面，由于模型的特性，我们可能会偶尔遇到不理想的结果。在这种情况下，你可以多尝试几次，或者进行深入的研究，以找到更稳定的方法。

让我们先从一个函数开始，它的名称是 `process_user_message_ch`，该函数主要负责处理用户输入的信息。这个函数接收三个参数，用户的输入、所有的历史信息，以及一个表示是否需要调试的标志。 在函数的内部，我们首先使用 OpenAI 的 Moderation API 来检查用户输入的合规性。如果输入被标记为不合规，我们将返回一个信息，告知用户请求不合规。在调试模式下，我们将打印出当前的进度。

接下来，我们利用 `utils_zh.find_category_and_product_only` 函数（详细请见附录代码）抽取出用户输入中的商品和对应的目录。然后，我们将抽取的信息转化为一个列表。 在获取到商品列表后，我们将查询这些商品的具体信息。之后，我们生成一个系统消息，设定一些约束，以确保我们的回应符合期望的标准。我们将生成的消息和历史信息一起送入 `get_completion_from_messages` 函数，得到模型的回应。之后，我们再次使用 Moderation API 检查模型的输出是否合规。如果输出不合规，我们将返回一个信息，告知无法提供该信息。

最后，我们让模型自我评估是否很好地回答了用户的问题。如果模型认为回答是满足要求的，我们则返回模型的回答；否则，我们会告知用户，他们将会被转接到人工客服进行进一步的帮助。

  

```Python
import openai 
import utils_zh
from tool import get_completion_from_messages

'''
注意：限于模型对中文理解能力较弱，中文 Prompt 可能会随机出现不成功，可以多次运行；也非常欢迎同学探究更稳定的中文 Prompt
'''def process_user_message_ch(user_input, all_messages, debug=True):"""
    对用户信息进行预处理
    
    参数:
    user_input : 用户输入
    all_messages : 历史信息
    debug : 是否开启 DEBUG 模式,默认开启
    """# 分隔符
    delimiter = "```"# 第一步: 使用 OpenAI 的 Moderation API 检查用户输入是否合规或者是一个注入的 Prompt
    response = openai.Moderation.create(input=user_input)
    moderation_output = response["results"][0]# 经过 Moderation API 检查该输入不合规if moderation_output["flagged"]:print("第一步：输入被 Moderation 拒绝")return "抱歉，您的请求不合规"# 如果开启了 DEBUG 模式，打印实时进度if debug: print("第一步：输入通过 Moderation 检查")# 第二步：抽取出商品和对应的目录，类似于之前课程中的方法，做了一个封装
    category_and_product_response = utils_zh.find_category_and_product_only(user_input, utils_zh.get_products_and_category())#print(category_and_product_response)# 将抽取出来的字符串转化为列表
    category_and_product_list = utils_zh.read_string_to_list(category_and_product_response)#print(category_and_product_list)if debug: print("第二步：抽取出商品列表")# 第三步：查找商品对应信息
    product_information = utils_zh.generate_output_string(category_and_product_list)if debug: print("第三步：查找抽取出的商品信息")# 第四步：根据信息生成回答
    system_message = f"""
        您是一家大型电子商店的客户服务助理。\
        请以友好和乐于助人的语气回答问题，并提供简洁明了的答案。\
        请确保向用户提出相关的后续问题。
    """# 插入 message
    messages = [{'role': 'system', 'content': system_message},{'role': 'user', 'content': f"{delimiter}{user_input}{delimiter}"},{'role': 'assistant', 'content': f"相关商品信息:\n{product_information}"}]# 获取 GPT3.5 的回答# 通过附加 all_messages 实现多轮对话
    final_response = get_completion_from_messages(all_messages + messages)if debug:print("第四步：生成用户回答")# 将该轮信息加入到历史信息中
    all_messages = all_messages + messages[1:]# 第五步：基于 Moderation API 检查输出是否合规
    response = openai.Moderation.create(input=final_response)
    moderation_output = response["results"][0]# 输出不合规if moderation_output["flagged"]:if debug: print("第五步：输出被 Moderation 拒绝")return "抱歉，我们不能提供该信息"if debug: print("第五步：输出经过 Moderation 检查")# 第六步：模型检查是否很好地回答了用户问题
    user_message = f"""
    用户信息: {delimiter}{user_input}{delimiter}
    代理回复: {delimiter}{final_response}{delimiter}

    回复是否足够回答问题
    如果足够，回答 Y
    如果不足够，回答 N
    仅回答上述字母即可
    """# print(final_response)
    messages = [{'role': 'system', 'content': system_message},{'role': 'user', 'content': user_message}]# 要求模型评估回答
    evaluation_response = get_completion_from_messages(messages)# print(evaluation_response)if debug: print("第六步：模型评估该回答")# 第七步：如果评估为 Y，输出回答；如果评估为 N，反馈将由人工修正答案if "Y" in evaluation_response:  # 使用 in 来避免模型可能生成 Yesif debug: print("第七步：模型赞同了该回答.")return final_response, all_messages
    else:if debug: print("第七步：模型不赞成该回答.")
        neg_str = "很抱歉，我无法提供您所需的信息。我将为您转接到一位人工客服代表以获取进一步帮助。"return neg_str, all_messages

user_input = "请告诉我关于 smartx pro phone 和 the fotosnap camera 的信息。另外，请告诉我关于你们的tvs的情况。"
response,_ = process_user_message_ch(user_input,[])print(response)
```

## 评估

  

在上一章中，我们探索了如何评估 LLM 模型在 有明确正确答案 的情况下的性能，并且我们学会了编写一个函数来验证 LLM 是否正确地进行了分类列出产品。

然而，如果我们想要使用 LLM 来生成文本，而不仅仅是用于解决分类问题，我们又应该如何评估其回答准确率呢？在本章，我们将讨论如何评估LLM在这种应用场景中的输出的质量。

  

希望您从本章中学到两个设计模式。

1. 即使没有专家提供的理想答案，只要能制定一个评估标准，就可以使用一个 LLM 来评估另一个 LLM 的输出。
    
2. 如果您可以提供一个专家提供的理想答案，那么可以帮助您的 LLM 更好地比较特定助手输出是否与专家提供的理想答案相似。
    

希望这可以帮助您评估 LLM 系统的输出，以便在开发期间持续监测系统的性能，并使用这些工具不断评估和改进系统的性能。


# Reference
https://github.com/Steven-Luo/MasteringRAG/tree/main