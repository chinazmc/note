# 什么是关联分析？

示例1：如下是一个超市几名顾客的交易信息。
![[Pasted image 20230228160432.png]]
TID代表交易流水号，Items代表一次交易的商品。

我们对这个数据集进行关联分析，可以找出关联规则{Diaper}→{Beer}。
它代表的意义是：购买了Diaper的顾客会购买Beer。这个关系不是必然的，但是可能性很大，这就已经足够用来辅助商家调整Diaper和Beer的摆放位置了，例如摆放在相近的位置，进行捆绑促销来提高销售量。

所以，关联分析的任务就是从数据集中挖掘出频繁项集，然后从频繁项集中提取出事物之间的强关联规则，辅助决策。

各种定义：
1、事务：每一条交易称为一个事务，例如示例1中的数据集就包含四个事务。
2、项：交易的每一个物品称为一个项，例如Cola、Egg等。
3、项集：包含零个或多个项的集合叫做项集，例如{Cola, Egg, Ham}。
4、k−项集：包含k个项的项集叫做k-项集，例如{Cola}叫做1-项集，{Cola, Egg}叫做2-项集。
5、支持度计数：一个项集出现在几个事务当中，它的支持度计数就是几。例如{Diaper, Beer}出现在事务002、003和004中，所以它的支持度计数是3。
6、支持度：支持度计数除于总的事务数。例如上例中总的事务数为4，{Diaper, Beer}的支持度计数为3，所以它的支持度是3÷4=75%，说明有75%的人同时买了Diaper和Beer。
7、频繁项集：支持度大于或等于某个阈值的项集就叫做频繁项集。例如阈值设为50%时，因为{Diaper, Beer}的支持度是75%，所以它是频繁项集。
8、前件和后件：对于规则{Diaper}→{Beer}，{Diaper}叫做前件，{Beer}叫做后件。
9、置信度：对于规则{Diaper}→{Beer}，{Diaper, Beer}的支持度计数除于{Diaper}的支持度计数，为这个规则的置信度。例如规则{Diaper}→{Beer}的置信度为3÷3=100%。说明买了Diaper的人100%也买了Beer。
10、强关联规则：大于或等于最小支持度阈值和最小置信度阈值的规则叫做强关联规则。关联分析的最终目标就是要找出强关联规则。

我们容易发现，如果一个项集是频繁项集，则它的子项集也都是频繁项集。如果一个项集是非频繁项集，则它的超集也一定是非频繁项集。（可用反证法证明，此处略）
例如{Diaper, Beer}是频繁项集，则{Diaper}、{Beer}也都是频繁项集。
例如{Egg}是非频繁项集，则{Cola, Egg}也是非频繁项集。


# 关联分析：
关联分析分为两个步骤：
<1> 利用支持度找出数据集中的频繁项集。
<2> 利用置信度从频繁项集中提取出强关联规则。

频繁项集的挖掘：
Apriori算法：
Apriori算法的思路是先找出候选项集，然后根据最小支持度阈值筛选出频繁项集。
例如先找出所有1-项集，然后筛选出里面的频繁1-项集； 根据频繁1-项集生成候选2-项集，然后筛选出里面的频繁2-项集； 再根据频繁2-项集生成候选3-项集，从里面筛选出频繁3-项集；·······

Apriori算法的缺点是需要不断扫描数据集，不断地求候选项集的支持度从而判断它是否是频繁项集。当数据集很大的时候，这种算法的效率将会非常低。
更多关于Apriori，请见Apriori算法的介绍。

FP-Growth算法：
FP-Growth算法只需要扫描两次数据集。它的思想是把构造一棵FP-Tree，把数据集中的数据映射到树上，再根据这棵FP-Tree找出所有频繁项集。
更多关于FP-Growth，请见FP-Growth算法的介绍、FP_Growth算法python实现。

关联规则的生成：
从步骤一已经得到了频繁项集，而此时的任务就是在频繁项集里面挖掘出大于最小置信度阈值的关联规则。
怎么挖呢？把频繁项集分成前件和后件两部分，然后求规则前件→后件的置信度，如果大于最小置信度阈值，则它就是一条强关联规则。
但是把频繁项集分成前件和后件的情况有很多，我们可以对其进行一些优化。


附加：
此处是针对购物篮示例来介绍关联分析，购物篮信息属于布尔型的，而现实生活中许多事物都是数值量化的，例如{购买1个时钟}→{购买2块电池}。
另外，对于产生的强关联规则，并不是全部都是有价值的，还需要对关联规则进行评价。
更多内容后续再补上。

# # Apriori算法的介绍
前言：
数据挖掘中的关联分析可以分成频繁项集的挖掘和关联规则的生成两个步骤，而Apriori算法是找频繁项集最常用到的一种算法。
关于关联分析和频繁项集请见：什么是关联分析？

中言：
我们还是利用购物篮的例子来讲述Apriori算法的思路。

购物篮信息如下：
![[Pasted image 20230228160518.png]]
TID代表交易流水号，Items代表一次交易的商品。

我们Apriori算法的最终目的就是要找出数据集中的频繁项集，把最小支持度阈值设为50%，则最终挖掘结果如下（后面的数字表示该项集的支持度计数）：

频繁1-项集：
{Cola} 3
{Diaper} 3
{Beer} 3
{Ham} 2

频繁2-项集：
{Cola, Diaper} 2
{Cola, Beer} 2
{Cola,Ham} 2
{Diaper, Beer} 3

频繁3-项集：
{Cola, Diaper, Beer} 2

Apriori算法的思路是由频繁(k-1)-项集生成候选k-项集，然后根据最小支持度判断该候选k-项集是否是频繁k-项集。
例如先找出所有1-项集，然后筛选出里面的频繁1-项集； 根据频繁1-项集生成候选2-项集，然后筛选出里面的频繁2-项集； 再根据频繁2-项集生成候选3-项集，从里面筛选出频繁3-项集；·······

那么问题来了，如何从频繁(k-1)-项集生成候选k-项集呢？
答案是利用Apriori性质：`一个频繁项集的任一子集也应该是频繁子集（用反证法容易证明，略）。所以如果一个项集是非频繁项集，那么它的超集也应该是非频繁项集。`
例如{Cola, Diaper}是频繁项集，所以{Cola}和{Diaper}也应该是频繁项集。因为{Egg}是非频繁项集，所以{Cola, Egg}也是非频繁项集。

从频繁1-项集生成候选2-项集的步骤是：把频繁1-项集和频繁1-项集排列组合成2-项集，把含有非频繁子项集的2-项集去掉，就是候选2-项集了。

从频繁2-项集生成候选三项集的步骤是：把频繁2-项集和频繁1-项集排列组合成3-项集：{Cola, Diaper, Beer}、{Cola, Diaper, Ham}、{Cola, Beer, Ham}、{Diaper, Beer, Ham}。
因为{Diaper, Ham}不是频繁2-项集，所以含有{Diaper, Ham}的{Cola, Diaper, Ham}不是候选3-项集，去掉。因为{Beer, Ham}不是频繁2-项集，所以含有{Beer, Ham}的{Cola, Beer, Ham}、{Diaper, Beer, Ham}不是候选3-项集，去掉。
所以候选3-项集只有{Cola, Diaper, Beer}。

购物篮频繁项集的挖掘过程如下：
![[Pasted image 20230228160535.png]]
Apriori算法描述如下（代码源自《数据挖掘原理与实践》）：
![[Pasted image 20230228160553.png]]


# FP-Growth算法的介绍
在关联分析中，频繁项集的挖掘最常用到的就是Apriori算法。Apriori算法是一种先产生候选项集再检验是否频繁的“产生-测试”的方法。这种方法有种弊端：当数据集很大的时候，需要不断扫描数据集造成运行效率很低。
而FP-Growth算法就很好地解决了这个问题。它的思路是把数据集中的事务映射到一棵FP-Tree上面，再根据这棵树找出频繁项集。FP-Tree的构建过程只需要扫描两次数据集。
更多关联分析和Apriori算法的信息请见：什么是关联分析、Apriori算法的介绍。
![[Pasted image 20230228155026.png]]

首先，FP-Growth算法的任务是找出数据集中的频繁项集。
然后，FP-Growth算法的步骤，大体上可以分成两步：(1)FP-Tree的构建； (2)FP-Tree上频繁项集的挖掘。

![[Pasted image 20230228155051.png]]

构造FP-Tree的伪代码如下：

算法：FP-Tree构造算法
输入：事务数据集 D，最小支持度阈值 min_sup
输出：FP-Tree
(1) 　　扫描事务数据集 D 一次，获得频繁项的集合 F 和其中每个频繁项的支持度。对 F 中的所有频繁项按其支持度进行降序排序，结果为频繁项表 L ;
(2) 　　创建一个 FP-Tree 的根节点 T，标记为“null”;
(3) 　　for 事务数据集 D 中每个事务 Trans do
(4) 　　　　对 Trans 中的所有频繁项按照 L 中的次序排序;
(5) 　　　　对排序后的频繁项表以 [p|P] 格式表示，其中 p 是第一个元素，而 P 是频繁项表中除去 p 后剩余元素组成的项表;
(6) 　　　　调用函数 insert_tree( [p|P], T );
(7) 　　end for

insert_tree( [p|P], root)
(1) 　　if root 有孩子节点 N and N.item-name=p.item-name then
(2) 　　　　N.count++;
(3) 　　Else
(4) 　　　　创建新节点 N;
(5) 　　　　N.item-name=p.item-name;
(6) 　　　　N.count++;
(7) 　　　　p.parent=root;
(8) 　　　　将 N.node-link 指向树中与它同项目名的节点;
(9) 　　end if
(10)　　if P 非空 then
(11)　　　　把 P 的第一项目赋值给 p，并把它从 P 中删除;
(12)　　　　递归调用 insert_tree( [p|P], N);
(13)　　end if

从FP-Tree提取频繁项集：
相对而言，FP-Tree的构造比较简单，而从FP-Tree提取频繁项集比较难理解。其中出现了几个新名词，下面直接针对购物篮的FP-Tree进行讲解吧。

求以“Ham”为后缀的频繁项集：
![[Pasted image 20230228155139.png]]

求以“Beer”为后缀的频繁项集：

“Beer”的条件模式基有{(Cola Diaper:2), (Diaper:1)}。
“Beer”的条件FP-树如下。
“Beer”为后缀的频繁项集为 {Cola Diaper Beer:2}、{Diaper Beer:2}、{Cola Beer:2}
![[Pasted image 20230228155201.png]]
求以“Diaper”为后缀的频繁项集：
　　条件模式基为{(Cola:2)}，最后求得频繁项集为{Cola Diaper:2}。

综上，得到的频繁项集有：{Cola Ham:2}、{Cola Beer:2}、{Diaper Beer:3}、{Cola Diaper:2}、{Cola Diaper Beer:2}。


从FP-Tree提取频繁项集的主要步骤是：

>对于每个频繁项，通过以下步骤求它的条件频繁项集：
- 找出它的条件模式基
- 把条件模式基当做事务集去建造一棵树，这棵树不叫FP-Tree，而叫做该频繁项的条件FP-Tree。
- 对这棵条件FP-Tree递归以上操作，即找这棵条件FP-Tree上的子条件频繁项集。

以上找到的都是该频繁项的条件频繁项集而已，所以每次递归都需要把条件频繁项集和该频繁项拼接起来才是我们最终要求的频繁项集

伪代码如下：
![[Pasted image 20230228155243.png]]

golang代码在：
 https://github.com/baiyyang/FP-growth.git 

# APRIORI 与 FP-GROWTH：不得不再说一遍啤酒与尿布的故事

这一课时，我们进入第四种数据挖掘算法——关联分析的学习。关联分析是一种无监督学习，它的目标就是从大数据中找出那些经常一起出现的东西，不管是商品还是其他什么 item，然后靠这些结果总结出关联规则以用于后续的商业目的或者其他项目需求。

一个例子  
不管你在哪一个数据挖掘课堂上，几乎都会听到这样一个“都市传说”：在一个大型超市中，数据分析人员整理了一整年的购物篮数据，来分析大家都买过什么样的东西。就在对购物篮的数据进行分析的时候，分析人员惊奇地发现，与“尿不湿”出现在一个购物小票上最频繁的商品竟然是啤酒。

这个结果背后的原因，是女人嘱托丈夫去超市给孩子买尿不湿，而丈夫通常会顺便买上一些自己喜欢的啤酒。超市发现了这个神奇的组合，于是毫不犹豫地把尿布和啤酒摆在了一起进行销售，以起到互相促进的作用。

关于这个故事是否真实发生过，我们保持怀疑的态度，但是这个故事确实反映了数据挖掘在商业运作中的价值。同时，购物篮分析也早已经是大型商超必备的技术手段。那么，如何发现这些潜在的价值，就用到了我们这一课时要讲的关联分析算法。

**算法原理**  
要了解算法原理，首先我们需要对关联分析中的一些概念进行一下解释。

为了方便说明，我在这里编造了十条购物小票数据（如有雷同，纯属巧合），如下表所示。

表1 购物小票数据
![[Pasted image 20230228155919.png]]

**项集（Item Set）**： 第一个要介绍的概念叫项集，中文名称有点拗口，还是看英文比较容易理解。项集可以是单个的项，也可以是一系列项目的合集。在我们的例子中，项目就是啤酒、尿布等商品，一个小票上的内容就可以看作一个项集，通过关联分析得到的经常一起出现的啤酒和尿布可以称为一个“频繁项集”。

**关联规则**： 根据频繁项集挖掘出的结果，例如 {尿布}→{啤酒}，规则的左侧称为先导，右侧称为后继。

**支持度**： 支持度就是一个项集在数据中出现的比例。在我们的 10 条数据中，{尿布} 出现了 9 次，那它的支持度就是 0.9；{啤酒} 出现了 8 次，啤酒的支持度就是 0.8。{尿布，啤酒} 的支持度是 8/10=0.8，以此类推。支持度还可以用来判定一条规则是否还需要继续进行挖掘，如果支持度已经很低，再加入新的项肯定会更低，挖掘的意义不大。

![2021-07-27T07:19:35.png](http://blog.ayla1688.cool/usr/uploads/2021/07/2986318602.png "2021-07-27T07:19:35.png")  
**置信度**： 置信度指的是在一条规则中，出现先导也出现后继的比例。我们可以用公式来看一下，

置信度表示的是一条规则的可靠程度。

![2021-07-27T07:19:42.png](http://blog.ayla1688.cool/usr/uploads/2021/07/3228390481.png "2021-07-27T07:19:42.png")

置信度（尿布-》啤酒） = 0.8/0.9 = 0.89

**提升度**： 在置信度中，只考虑了规则中的先导与后继同时发生的情况，而对于后继单独发生的情况没有加以考虑。所以又有人提出了一个提升度，用来衡量先导和后继的独立性。比如在前面我们算出“尿布→啤酒”的置信度为 0.89，这说明买了“尿布”的人里有 89% 会买“啤酒”，这看起来已经很高了。但是如果在没有买“尿布”的购物小票中，购买“啤酒”的概率仍然为 0.89，那其实购买“尿布”和购买“啤酒”并没有什么关系。所以，提升度的计算公式如下：  
![2021-07-27T07:19:49.png](http://blog.ayla1688.cool/usr/uploads/2021/07/1216207422.png "2021-07-27T07:19:49.png")

提升度（尿布-》啤酒） = 0.8 /（0.9*0.8） = 1.1

如果提升度大于 1，说明是有提升的。

**确信度**： 最后一个指标是确信度，确信度指的是对于一条规则，不发生先导而发生后继的概率与这条规则错误的概率比值。公式如下：

![2021-07-27T07:20:05.png](http://blog.ayla1688.cool/usr/uploads/2021/07/3420016669.png "2021-07-27T07:20:05.png")

确信度（尿布-》 啤酒） = （1-0.8）/（1-0.89） = 1.82

上面的结果为 1.82，这说明该条规则是真的概率比它只是偶然发生的概率高 82%。

---

## 这里概率高82%是为什么？

一口气看了这么多的概念和指标，下面终于进入我们的正题环节，看看在关联挖掘中的算法原理。

**Apriori**  
关联挖掘的目标已经很明确了，而关联挖掘的步骤也就只有两个：第一步是找出频繁项集，第二步是从频繁项集中提取规则。A**priori 算法的核心就是：如果某个项集是频繁项集，那么它的全部子集也都是频繁项集。**

为了方便了解算法的效果，我预先画出了数据集中所有可能存在的项集关系，就如下图显示：

![2021-07-27T07:20:13.png](http://blog.ayla1688.cool/usr/uploads/2021/07/3467994219.png "2021-07-27T07:20:13.png")

首先，需要设定一个最小支持度阈值，假设我们设定为 0.5，那么高于 0.5 的就认为是频繁项集。然后，我们计算出所有单个商品的支持度，如下表所示：

表2 一阶项集支持度
![[Pasted image 20230228155940.png]]

从这里可以看出，“苹果”的支持度达不到阈值，于是把它删掉。因此，所有跟“苹果”相关的父集也都是低频的，在后续的计算也不会涉及了，就是下图标绿色的这些。

![2021-07-27T07:21:27.png](http://blog.ayla1688.cool/usr/uploads/2021/07/4156329915.png "2021-07-27T07:21:27.png")

接下来我们再计算二阶的项集支持度。

表3 二阶项集支持度

![[Pasted image 20230228155954.png]]

到了这一步，后四个项集的支持度已经达不到我们的阈值 0.5，于是也删掉。

接下来再计算三阶的项集支持度，这个时候发现已经没有可用的三阶项集了，所以算法计算结束。这时候我们得到了两组频繁项集 {尿布，啤酒}、{尿布，奶粉}。接下来从这两个频繁项集，我们可以得到两个关联关系：尿布→啤酒，尿布→奶粉。根据前面的公式，分别计算这两个关系的置信度、提升度和确信度。

表4 三个关联关系的置信度、提升度和确信度
![[Pasted image 20230228160006.png]]
到了这里，再根据我们的需求设定阈值来筛选最终需要留下来的规则。

**FP-Growth（Frequent Pattern Growth）**  
根据上面的 Apriori 计算过程，我们可以知道 Apriori 计算的过程中，会使用排列组合的方式列举出所有可能的项集，每一次计算都需要重新读取整个数据集，从而计算本轮次的项集支持度。所以 Apriori 会耗费大量的计算资源，这时候就有了一个更高效的算法——FP-Growth 算法。

Apriori 算法一开始需要对所有的规则进行枚举，然后再进行计算，而 FP-Growth 则是首先使用数据生成一棵 FP-Growth 树，然后再根据这棵树来生成频繁项集。下面我们来看一下如何构建 FP 树。

仍然使用上面编的购物小票数据，并对每一个小票里的项按统一的顺序进行排序，设置一个空集作为根节点。我们把第一条数据录入这个树中，每一个单项作为上一个节点的叶子节点，旁边的数据代表该路径访问的次数。下图就是我们录入第一条数据后的结果：

![2021-07-27T07:23:52.png](http://blog.ayla1688.cool/usr/uploads/2021/07/777867781.png "2021-07-27T07:23:52.png")  
第二条数据与第一条一样，所以只有数字的变化，节点不会发生变化。

接着输入第三条数据。

输入完第二、三条数据之后的结果如下，其中由第一个洋葱和第二个洋葱直接画了一条虚线，标识它们是同一项。  
![2021-07-27T07:23:59.png](http://blog.ayla1688.cool/usr/uploads/2021/07/4273700235.png "2021-07-27T07:23:59.png")

接下来我们把所有的数据都插入到这棵树上。  
![2021-07-27T07:24:05.png](http://blog.ayla1688.cool/usr/uploads/2021/07/3996799675.png "2021-07-27T07:24:05.png")

这个时候我们得到的是完整的 FP 树，接下来我们要从这里面去寻找频繁项集。当然首先也要设定一个最小频度的阈值，然后从叶子节点开始，也就是最低频的节点。如果频度高于阈值那么就收录所有以该项结尾的项集，否则就向上继续检索。

至于后面的计算关联规则的方法跟上面就没有什么区别了，这里不再赘述。下面我们尝试使用代码来实现关联规则的发现。

**尝试动手**  
在我们之前使用的 sklearn 算法工具包中没有 apriori 算法，所以这次我们需要先安装一个 efficient-apriori 算法包。而我们这次所使用的数据集，也就是我在上面提到的这个“啤酒尿布”数据。这个算法包使用起来也极其简单。

```
复制代码
'''记得安装包
pip install efficient-apriori
'''
from efficient_apriori import apriori
# 设置数据集
data = [('尿布', '啤酒', '奶粉','洋葱'),
('尿布', '啤酒', '奶粉','洋葱'),
('尿布', '啤酒', '苹果','洋葱'),
('尿布', '啤酒', '苹果'),
('尿布', '啤酒', '奶粉'),
('尿布', '啤酒', '奶粉'),
('尿布', '啤酒', '苹果'),
('尿布', '啤酒', '苹果'),
('尿布', '奶粉', '洋葱'),
('奶粉', '洋葱')
]
# 挖掘频繁项集和规则
itemsets, rules = apriori(data, min_support=0.4, min_confidence=1)
print(itemsets)
print(rules)
#输出结果
{1: {('奶粉',): 6, ('洋葱',): 5, ('尿布',): 9, ('啤酒',): 8, ('苹果',): 4}, 2: {('啤酒', '奶粉'): 4, ('啤酒', '尿布'): 8, ('奶粉', '尿布'): 5, ('奶粉', '洋葱'): 4, ('尿布', '洋葱'): 4, ('啤酒', '苹果'): 4, ('尿布', '苹果'): 4}, 3: {('啤酒', '奶粉', '尿布'): 4, ('啤酒', '尿布', '苹果'): 4}}
[{啤酒} -> {尿布}, {苹果} -> {啤酒}, {苹果} -> {尿布}, {啤酒, 奶粉} -> {尿布}, {尿布, 苹果} -> {啤酒}, {啤酒, 苹果} -> {尿布}, {苹果} -> {啤酒, 尿布}]
#把min_support设置成0.5输出结果
{1: {('尿布',): 9, ('奶粉',): 6, ('啤酒',): 8, ('洋葱',): 5}, 2: {('啤酒', '尿布'): 8, ('奶粉', '尿布'): 5}}
[{啤酒} -> {尿布}]
```

通过上面的代码，我们就成功使用了 apriori 算法，对我们自己编造的数据集完成了关联关系挖掘，当在设置最小支持度为 0.4 的时候，我们找到了 7 条规则；而我们把最小支持度改为 0.5 以后，只剩下 {啤酒} -> {尿布} 这一条规则了。你学会了吗？

**总结**  
这节课里，我们介绍了两种关联关系挖掘的方法，其中 Apriori 使用了穷举的方式，而 FP-Growth 使用了树形结构来提高速度。关联关系挖掘通常使用的算法都非常简单，或者我们可以把关联关系挖掘转化成分类问题、聚类问题来解决都是可以的。

在这节课中，我们还介绍了关联关系的评估指标，不管是用什么算法来挖掘的关联关系，都可以使用这些指标来进行评估。

下一课时，我们会进入关联关系挖掘的实践课程，看看如何使用关联关系挖掘来解决业务中的问题。

# Reference
https://blog.csdn.net/Bone_ACE/article/details/46669699
http://blog.ayla1688.cool/archives/709.html
https://blog.csdn.net/bone_ace/article/details/46648965
https://blog.csdn.net/bone_ace/article/details/46660819
