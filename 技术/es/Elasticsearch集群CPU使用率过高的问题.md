#es 
# 说明

本文描述问题及解决方法同样适用于 **腾讯云 Elasticsearch Service（ES）**。

# 背景

ES集群在某些情况下会出现CPU使用率高的现象，具体有两种表现：

1.  个别节点CPU使用率远高于其他节点；
2.  集群中所有节点CPU使用率都很高。

下面我们来详细分析这两种可能出现的情况。

# 出现集群负载不均的问题如何解决？

## 问题现象

集群在某些情况下会个别节点CPU使用率远高于其他节点的现象，具体表现为：  
ES集群UI控制台节点监控上可以明显看到某些节点CPU使用率很高。

## 原因

可能存在以下几种原因：

-   索引分片设计不合理
-   Segment大小不均
-   存在典型的冷热数据需求场景

## 问题分析及解决方案

### 一、Shard设置不合理

1. 登录Kibana控制台，在开发工具中执行以下命令，查看索引的shard信息，确认索引的shard在负载高的节点上呈现的数量较多，说明shard分配不均；

GET \_cat/shards?v

2. 登录Kibana控制台，在开发工具中执行以下命令，查看索引信息。结合集群配置，确认存在节点shard分配不均的现象；

GET \_cat/indices?v

**解决方案**

-   重新分配分片，合理规划shard，确保主shard数与副shard数之和是集群数据节点的整数倍；
-   由于Shard大小和数量是影响Elasticsearch集群稳定性和性能的重要因素之一。Elasticsearch集群中任何一个索引都需要有一个合理的shard规划。合理的shard规划能够防止因业务不明确，导致分片庞大消耗Elasticsearch本身性能的问题。以下是shard规划时的几个建议：

1.  尽量遵循索引单分片20g~50g的设计原则；
2.  索引尽量增加时间后缀，按时间滚动，方便管理；
3.  在遵循单分片设计原则的前提下，预测出索引最终大小，并根据集群节点数设计索引分片数量，使分片尽量平均分布在各个节点。

**特别注意**

-   主分片不是越多越好，因为主分片越多，Elasticsearch性能开销也会越大。建议单节点shard总数按照单节点内存*/30进行评估，如果shard数量太多，极易引起文件句柄耗尽，导致集群故障。
-   Elasticsearch在检索过程中也会检索 `.del` 文件，然后过滤标记有 `.del` 的文档，这会降低检索效率，耗费规格资源，建议在业务低峰期进行强制合并操作，具体请参见force merge。

### 二、Segment大小不均

1.  在查询body中添加 `"profile": true` ，检查test索引是否存在某个shard查询时间比其他shard长。
2.  查询中分别指定 `preference=_primary` 和 `preference=_replica` ，在body中添加 `"profile": true` ，分别查看主副shard查询消耗的时间。检查较耗时的shard主要体现在主shard上还是副shard上。
3.  登录Kibana控制台，在开发工具中执行以下命令，查看shard，并根据其中segment信息分析问题所在，确认负载不均与segment大小不均有关。

GET \_cat/segments/index?v&h=shard,segment,size,size.momery,ipGET _cat/shards?v

**解决方案**  
参考以下两种方法其中一种解决问题：

-   在业务低峰期进行强制合并操作，具体请参见force merge，将缓存中的delete.doc彻底删除，将小segment合并成大segment。
-   重启主shard所在节点，触发副shard升级为主shard。并且重新生成副shard，副shard复制新的主shard中的数据，保持主副shard的segment一致。

### 三、存在典型的冷热数据需求场景

如果查询中添加了routing或查询频率较高的热点数据，则必然导致数据出现负载不均。

# 出现ES集群整体CPU使用率都很高时该如何排查？

## 问题现象

集群所有节点CPU都很高，但读写都不是很高。  
具体表现可以从kibana端Stack Monitoring监控页面看到：

## 原因

出现这种情况，由于表面上看集群读写都不高，导致很难快速从监控上找到根因。所以需要细心观察，从细节中找答案，下面我们介绍几种可能出现的场景以及排查思路。

## 问题分析及解决方案

### 原因一：比较大的查询请求导致CPU飙高

这种情况比较常见，细心一点的话可以从监控上找到线索：

从监控上可以发现，查询请求量的波动与集群最大CPU使用率是基本吻合的。  
发现了问题所在，进一步确认则需要开启集群的慢日志收集，可以参考官方文档：集群日志说明。从慢日志中，我们可以得到更多信息。比如引起慢查询的索引、查询参数以及内容。

**解决方案**  
针对这种情况，我们一般建议：  
1、尽量避免大段文本搜索，优化查询；  
2、通过慢日志确认查询慢的索引，对于一些数据量不大的索引，设置少量分片多副本，比如1分片多副本，以此来提高查询性能。

### 原因二：写入请求导致CPU飙高

同理，首先通过监控来观察到CPU飙高是与写入相关，然后开启集群的慢日志收集，确认写入慢的请求，进行优化。也可以通过获取`hot_threads`信息来确认什么线程在消耗CPU：

curl http://9.15.49.78:9200/_nodes/hot_threads

比如这里发现的是有大量ingest pipeline操作，ingest操作是十分消耗资源的。

**解决方案**  
如遇到上面这种问题，则需要业务方根据实际情况来优化。

# 小结

排查该类问题的关键点，还是在于善用集群的监控指标来快速判断问题的方向，再配合集群日志来定位问题的根因，才能快速地解决问题。
