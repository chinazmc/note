本文起源于之前去面试的一道面试题，面试题大致上是这样的：消费者去Kafka里拉去消息，但是目前Kafka中又没有新的消息可以提供，那么Kafka会如何处理？

如下图所示，两个follower副本都已经拉取到了leader副本的最新位置，此时又向leader副本发送拉取请求，而leader副本并没有新的消息写入，那么此时leader副本该如何处理呢？可以直接返回空的拉取结果给follower副本，不过在leader副本一直没有新消息写入的情况下，follower副本会一直发送拉取请求，并且总收到空的拉取结果，这样徒耗资源，显然不太合理。

![](https://cdn.nlark.com/yuque/0/2021/png/2725209/1634521805493-ed503f9a-b722-4c53-945b-7fb53f226145.png)

假设某个分区有3个副本：leader、follower1和follower2，它们都在分区的ISR集合中。为了简化说明，这里我们不考虑ISR集合伸缩的情况。Kafka在收到客户端的生产请求后，将消息3和消息4写入leader副本的本地日志文件，如上图所示。

这里就涉及到了Kafka延迟操作的概念。Kafka在处理拉取请求时，会先读取一次日志文件，如果收集不到足够多（fetchMinBytes，由参数fetch.min.bytes配置，默认值为1）的消息，那么就会创建一个延时拉取操作（DelayedFetch）以等待拉取到足够数量的消息。当延时拉取操作执行时，会再读取一次日志文件，然后将拉取结果返回给follower副本。

延迟操作不只是拉取消息时的特有操作，在Kafka中有多种延时操作，比如延时数据删除、延时生产等。

对于延时生产（消息）而言，如果在使用生产者客户端发送消息的时候将acks参数设置为-1，那么就意味着需要等待ISR集合中的所有副本都确认收到消息之后才能正确地收到响应的结果，或者捕获超时异常。

![](https://cdn.nlark.com/yuque/0/2021/png/2725209/1634521799383-bd20f197-9768-44a2-b7df-fc40f0b7bc40.png)

由于客户端设置了acks为-1，那么需要等到follower1和follower2两个副本都收到消息3和消息4后才能告知客户端正确地接收了所发送的消息。如果在一定的时间内，follower1副本或follower2副本没能够完全拉取到消息3和消息4，那么就需要返回超时异常给客户端。生产请求的超时时间由参数request.timeout.ms配置，默认值为30000，即30s。

![](https://cdn.nlark.com/yuque/0/2021/png/2725209/1634521793307-7c8e6e8d-ff87-4422-93e1-5331fd8809ec.png)

![](https://cdn.nlark.com/yuque/0/2021/png/2725209/1634521785087-a74f4d60-55c8-4b4c-a515-3f2e056b155b.png)

那么这里等待消息3和消息4写入follower1副本和follower2副本，并返回相应的响应结果给客户端的动作是由谁来执行的呢？在将消息写入leader副本的本地日志文件之后，Kafka会创建一个延时的生产操作（DelayedProduce），用来处理消息正常写入所有副本或超时的情况，以返回相应的响应结果给客户端。

延时操作需要延时返回响应的结果，首先它必须有一个超时时间（delayMs），如果在这个超时时间内没有完成既定的任务，那么就需要强制完成以返回响应结果给客户端。其次，延时操作不同于定时操作，定时操作是指在特定时间之后执行的操作，而延时操作可以在所设定的超时时间之前完成，所以延时操作能够支持外部事件的触发。

就延时生产操作而言，它的外部事件是所要写入消息的某个分区的HW（高水位）发生增长。也就是说，随着follower副本不断地与leader副本进行消息同步，进而促使HW进一步增长，HW每增长一次都会检测是否能够完成此次延时生产操作，如果可以就执行以此返回响应结果给客户端；如果在超时时间内始终无法完成，则强制执行。

回顾一下文中开头的延时拉取操作，它也同样如此，也是由超时触发或外部事件触发而被执行的。超时触发很好理解，就是等到超时时间之后触发第二次读取日志文件的操作。外部事件触发就稍复杂了一些，因为拉取请求不单单由follower副本发起，也可以由消费者客户端发起，两种情况所对应的外部事件也是不同的。如果是follower副本的延时拉取，它的外部事件就是消息追加到了leader副本的本地日志文件中；如果是消费者客户端的延时拉取，它的外部事件可以简单地理解为HW的增长。

延迟操作背后还有一些更深层次的内容，比如对于“炼狱”、“收割机”的理解


# 选举
[https://juejin.cn/post/6844903846297206797](https://juejin.cn/post/6844903846297206797)

面试官在考查你Kafka知识的时候很可能会故弄玄虚的问你一下：Kafka中的选举时怎么回事？除非问你具体的哪种选举，否则问这种问题的面试官多半也是对Kafka一知半解，这个时候就是“弄死”他的时候。当然如果你没有一定的知识储备，那么就是你被“弄死”的时候。

一般问这个问题，那么他肯定知道其中的一种，比如分区leader的选举。所谓分区leader的选举就是当ISR中的leader副本歇菜了，再重新选举一个的过程。对于这个问题就是你反客为主的机会，因为Kafka中的选举有多处，可不止分区leader的选举这一处，就算指明问分区leader的选举，那么也需要分4种情况具体分析。而这里面的细节是大多数人不甚明了的。

Kafka中的选举大致可以分为三大类：控制器的选举、分区leader的选举以及消费者相关的选举，这里还可以具体细分为7个小类。我们一一来过一下，本文只是简单罗列下大致的内容，至于内部具体的细节逻辑就需要靠读者自己去探索啦。虐人还是被虐就靠你的自驱力了。

## 控制器的选举

在Kafka集群中会有一个或多个broker，其中有一个broker会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态等工作。比如当某个分区的leader副本出现故障时，由控制器负责为该分区选举新的leader副本。再比如当检测到某个分区的ISR集合发生变化时，由控制器负责通知所有broker更新其元数据信息。

Kafka Controller的选举是依赖Zookeeper来实现的，在Kafka集群中哪个broker能够成功创建/controller这个临时（EPHEMERAL）节点他就可以成为Kafka Controller。

这里需要说明一下的是Kafka Controller的实现还是相当复杂的，涉及到各个方面的内容，如果你掌握了Kafka Controller，你就掌握了Kafka的“半壁江山”。篇幅所限，这里就不一一展开了，有兴趣的读者可以查阅一下《深入理解Kafka》中第6章的相关内容。

## 分区leader的选举

这里不说什么一致性协议（PacificA）相关的内容，只讲述具体的选举内容。

分区leader副本的选举由Kafka Controller 负责具体实施。当创建分区（创建主题或增加分区都有创建分区的动作）或分区上线（比如分区中原先的leader副本下线，此时分区需要选举一个新的leader上线来对外提供服务）的时候都需要执行leader的选举动作。

基本思路是按照AR集合中副本的顺序查找第一个存活的副本，并且这个副本在ISR集合中。**一个分区的AR集合在分配的时候就被指定，并且只要不发生重分配的情况，集合内部副本的顺序是保持不变的，而分区的ISR集合中副本的顺序可能会改变。**注意这里是根据AR的顺序而不是ISR的顺序进行选举的。这个说起来比较抽象，有兴趣的读者可以手动关闭/开启某个集群中的broker来观察一下具体的变化。

还有一些情况也会发生分区leader的选举，比如当分区进行重分配（reassign）的时候也需要执行leader的选举动作。这个思路比较简单：**从重分配的AR列表中找到第一个存活的副本，且这个副本在目前的ISR列表中。**

**再比如当发生优先副本（preferred replica partition leader election）的选举时，直接将优先副本设置为leader即可，AR集合中的第一个副本即为优先副本。**

Kafka中有很多XX副本的称呼，如果不是很了解，可以关注本系列的下一篇《Kafka科普系列 | Kafka中到底有多少种副本？》

还有一种情况就是当某节点被优雅地关闭（也就是执行ControlledShutdown）时，位于这个节点上的leader副本都会下线，所以与此对应的分区需要执行leader的选举。这里的具体思路为：从AR列表中找到第一个存活的副本，且这个副本在目前的ISR列表中，与此同时还要确保这个副本不处于正在被关闭的节点上。

## 消费者相关的选举

对于这部分内容的理解，额。。如果你对消费者、消费组、消费者协调器以及组协调器不甚理解的话，那么。。。职能毛遂自荐《深入理解Kafka》一书了，嘿嘿。

组协调器GroupCoordinator需要为消费组内的消费者选举出一个消费组的leader，这个选举的算法也很简单，分两种情况分析。如果消费组内还没有leader，那么第一个加入消费组的消费者即为消费组的leader。如果某一时刻leader消费者由于某些原因退出了消费组，那么会重新选举一个新的leader，这个重新选举leader的过程又更“随意”了，相关代码如下：

//scala code. private val members = new mutable.HashMap[String, MemberMetadata] var leaderId = members.keys.head 复制代码

解释一下这2行代码：在GroupCoordinator中消费者的信息是以HashMap的形式存储的，其中key为消费者的member_id，而value是消费者相关的元数据信息。leaderId表示leader消费者的member_id，它的取值为HashMap中的第一个键值对的key，这种选举的方式基本上和随机无异。总体上来说，消费组的leader选举过程是很随意的。


# **1.1、Kafka Reactor模型架构**

Kafka客户端和服务端通信采取的是NIO的reactor模式，它是一种事件驱动模式。那么一个常见的单线程Reactor模式下，NIO线程的职责都有哪些呢？我们整理了如下几点：  
1、作为NIO服务端，接收客户端的TCP连接  
2、作为NIO客户端，向服务端发起TCP连接  
3、读取通信对端的请求或者应答消息  
4、向通信对端发送消息请求或者应答消息  
以上四点对应的一个Reactor模式的架构图如下：  
![](https://cdn.nlark.com/yuque/0/2021/png/2725209/1634803950005-473ddf0f-f647-4cc7-ba16-738d239d8513.png)

对于一些小容量的业务场景，这种单线程的模式基本够用。但是对于高负载、大并发的应用场景却并不适合，主要原因如下：  
性能问题1：一个NIO线程同时处理数十万甚至百万级的链路性能是无法支撑的  
性能问题2：如果超时发生重试会加重服务端处理负荷，导致大量处理积压  
可靠性问题：单个线程出现故障，整个系统无法使用，造成单点故障  
所以一个高并发的处理服务需要对以上架构进行优化改造，例如处理采取多线程模式，将接收线程尽量简化，相当于将接收线程作为一个接入层。那么我们回到主题kafka的reactor模式架构是怎样的？  
![](https://cdn.nlark.com/yuque/0/2021/png/2725209/1634803943544-9159e743-6b01-4020-990b-f2dd44a71233.png)

在上面这个kafka的架构图中可以看出，它包含以下几个流程：  
1、客户端请求NIO的连接器Acceptor，同时它还具备事件的转发功能，转发到Processor处理  
2、服务端网络事件处理器Processor  
3、请求队列RequestChannel，存储了所有待处理的请求信息  
4、请求处理线程池(RequestHandler Pool)作为守护线程轮训RequestChannel的请求处理信息，并将其转发给API层对应的处理器处理  
5、API层处理器将请求处理完成之后放入到Response Queue中，并由Processor从Response Queue取出发送到对应的Client端  
需要注意的一点是虽然Broker层包含多个Acceptor，但是kafka的reactor模式里面还是单线程Acceptor多线程handler的模式，这里的多个Acceptor是针对一个服务器下多网卡场景的，每个EndPoint就是一个网卡它对应于一个ip和port的组合，而每个Endpoint只有一个Acceptor。

acceptor是通过轮询找Processor的！！

## **1.2、Kafka Reactor模型源码详解**

按照上面架构图阐述的几个流程，它分别对应着kafka里面的事件接收、处理、响应等几个阶段，我们下面从具体实现这几个阶段的源码层面来分析。  
_**1.2.1、SocketServer**_  
SocketServer是一个标准的NIO服务端实现，它主要包含以下变量：  
1、RequestChannel：Processor和KafkaRequestHandler 之间数据交换的队列  
2、Processors：processor的容器，存放的是processor的id和processor对象的映射  
3、Acceptors：acceptor的容器，存放的是EndPoint和acceptor的映射  
4、ConnectionQuotas：链接限制器，针对每个IP的链接数进行限制  
SocketServer的启动流程如下：  
![](https://cdn.nlark.com/yuque/0/2021/png/2725209/1634803925203-06c4b9a6-d3b8-4330-885b-86d7fe0e8043.png)

![](https://cdn.nlark.com/yuque/0/2021/png/2725209/1634804362521-1eaae76d-20af-4209-a3e9-d9b2a1857d44.png)

整个流程可以归纳为如下：

1、对kafka集群元素信息的获取及更新  
2、Key和value的序列化  
3、如果有指定分区则采用指定分区，否则计算目标分区  
4、缓存消息压入到RecordAccumulator 中  
5、有条件的唤醒发送线程

**集群信息获取**

集群信息的更新从上面介绍我们知道它是在消息发送的时候实施的而且是阻塞等待更新，因为信息随时可能会发生变化，我们获得的集群信息一定要是最新的，所以异步更新没有任何意义，只能采取主动等待更新。那我们先看下消息更新的一个流程图：  
![](https://cdn.nlark.com/yuque/0/2021/png/2725209/1634804437160-1ba0596e-491b-49ae-92e2-54306dffe413.png)

消息更新是一个标准的I/O通信过程，分为两个线程，metadata主线程等待信息获取，Sender线程进行网络I/O通信获取，并更新到metadata当中，下面我们会重点介绍metada的主线程触发更新逻辑和部分的Sender线程和metada相关的逻辑，其它Sender逻辑我们放到消息发送过程中讲解。

**1.2.4、缓存消息收集器（RecordAccumulator ）**

RecordAccumulator 在消息发送中的一个重要作用可以认为是个蓄水池，我们先看一张消息缓存收集的架构图：  
![](https://cdn.nlark.com/yuque/0/2021/png/2725209/1634804463479-5ef638c0-54dd-4b7a-9ea1-06cc0bf47cae.png)

所有消息的收集过程从这个图可以很明显的看出，每条消息先从MetaData里面获取分区信息，再申请一段buffer空间形成一个批接收空间，RecordAccumulator 会将收到的每条消息append到这个buffer中，最后将每个批次压入到队列当中，等待Sender线程来获取发送。

以上内容来着[https://segmentfault.com/a/1190000020379573](https://segmentfault.com/a/1190000020379573)


很多同学私信问我Kafka在性能优化方面做了哪些举措，对于相关问题的答案其实我早就写过了，就是没有系统的整理一篇，最近思考着花点时间来整理一下，下次再有同学问我相关的问题我就可以潇洒的甩个链接了。这个问题也是Kafka面试的时候的常见问题，面试官问你这个问题也不算刁难你。在网上也有很多相关的文章开讲解这个问题，比如之前各大公众号转载的《为什么Kafka这么快？》，这些文章我看了，写的不错，问题在于只是罗列了部分的要领，没有全部的详述出来。本文所罗列的要领会比你们网上搜寻到的都多，如果你在看完本篇文章之后，在面试的时候遇到相关问题，相信你一定能让面试官眼前一亮。

PS: 本文章的所有要领在《深入理解Kafka》一书中都有详细的描述，如果对相关知识点生疏的话，可以再去翻看一下。

具体解析参考《[Linux IO磁盘篇整理小记](https://link.juejin.cn?target=https%3A%2F%2Fblog.csdn.net%2Fu013256816%2Farticle%2Fdetails%2F78945085)》

## 批量处理

传统消息中间件的消息发送和消费整体上是针对单条的。对于生产者而言，它先发一条消息，然后broker返回ACK表示已接收，这里产生2次rpc；对于消费者而言，它先请求接受消息，然后broker返回消息，最后发送ACK表示已消费，这里产生了3次rpc（有些消息中间件会优化一下，broker返回的时候返回多条消息）。而Kafka采用了批量处理：生产者聚合了一批消息，然后再做2次rpc将消息存入broker，这原本是需要很多次的rpc才能完成的操作。假设需要发送1000条消息，每条消息大小1KB，那么传统的消息中间件需要2000次rpc，而Kafka可能会把这1000条消息包装成1个1MB的消息，采用2次rpc就完成了任务。这一改进举措一度被认为是一种“作弊”的行为，然而在微批次理念盛行的今日，其它消息中间件也开始纷纷效仿。

## 客户端优化

这里接着批量处理的概念继续来说，新版生产者客户端摒弃了以往的单线程，而采用了双线程：主线程和Sender线程。主线程负责将消息置入客户端缓存，Sender线程负责从缓存中发送消息，而这个缓存会聚合多个消息为一个批次。有些消息中间件会把消息直接扔到broker。

## 日志格式

Kafka从0.8版本开始日志格式历经了三次变革：v0、v1、v2。在之前发过的一篇文章《一文看懂Kafka消息格式的演变》中详细介绍了Kafka日志格式，Kafka的日志格式越来越利于批量消息的处理，有兴趣的同学可以阅读一下这篇文章以作了解。

![](https://cdn.nlark.com/yuque/0/2021/png/2725209/1634521889151-6d1997c0-d9a9-4afe-bbcc-d6db309ce7b1.png)

## 日志编码

如果了解了Kafka具体的日志格式（可以参考上图），那么你应该了解日志（Record，或者称之为消息）本身除了基本的key和value之外，还有一些其它的字段，原本这些附加字段按照固定的大小占用一定的篇幅（参考上图左），而Kafka最新的版本中采用了变成字段Varints和ZigZag编码，有效地降低了这些附加字段的占用大小。日志（消息）尽可能变小了，那么网络传输的效率也会变高，日志存盘的效率也会提升，从而整理的性能也会有所提升。

## 消息压缩

Kafka支持多种消息压缩方式（gzip、snappy、lz4）。对消息进行压缩可以极大地减少网络传输 量、降低网络 I/O，从而提高整体的性能。消息压缩是一种使用时间换空间的优化方式，如果对 时延有一定的要求，则不推荐对消息进行压缩。

## 建立索引，方便快速定位查询

每个日志分段文件对应了两个索引文件，主要用来提高查找消息的效率，这也是提升性能的一种方式。（具体的内容在书中的第5章有详细的讲解，公众号里好像忘记发表了，找了一圈没找到）

## 分区

很多人会忽略掉这个因素，其实分区也是提升性能的一种非常有效的方式，这种方式所带来的效果会比前面所说的日志编码、消息压缩等更加的明显。分区在其他分布式组件中也有大量涉及，至于为什么分区能够提升性能这种基本知识在这里就不在赘述了。不过需要注意，一昧地增加分区并不能一直带来性能的提升，有兴趣的同学可以看一下这篇《Kafka主题中的分区数越多吞吐量就越高？》。

## 一致性

绝大多数的资料在讲述Kafka性能优化的举措之时是不会提及一致性的东西的。我们所了解的通用的一致性协议如Paxos、Raft、Gossip等，而Kafka另辟蹊径采用类似PacificA的做法不是“拍大腿”拍出来的，采用这种模型会提升整理的效率。具体的细节后面会整理一篇，类似《在Kafka中使用Raft替换PacificA的可行性分析及优缺点》。

## 顺序写盘

操作系统可以针对线性读写做深层次的优化，比如预读(read-ahead，提前将一个比较大的磁盘块读入内存) 和后写(write-behind，将很多小的逻辑写操作合并起来组成一个大的物理写操作)技术。Kafka 在设计时采用了文件追加的方式来写入消息，即只能在日志文件的尾部追加新的消 息，并且也不允许修改已写入的消息，这种方式属于典型的顺序写盘的操作，所以就算 Kafka 使用磁盘作为存储介质，它所能承载的吞吐量也不容小觑。

## 页缓存

为什么Kafka性能这么高？当遇到这个问题的时候很多人都会想到上面的顺序写盘这一点。其实在顺序斜盘前面还有页缓存（PageCache）这一层的优化。

页缓存是操作系统实现的一种主要的磁盘缓存，以此用来减少对磁盘 I/O 的操作。具体 来说，就是把磁盘中的数据缓存到内存中，把对磁盘的访问变为对内存的访问。为了弥补性 能上的差异，现代操作系统越来越“激进地”将内存作为磁盘缓存，甚至会非常乐意将所有 可用的内存用作磁盘缓存，这样当内存回收时也几乎没有性能损失，所有对于磁盘的读写也 将经由统一的缓存。

当一个进程准备读取磁盘上的文件内容时，操作系统会先查看待读取的数据所在的页 (page)是否在页缓存(pagecache)中，如果存在(命中)则直接返回数据，从而避免了对物 理磁盘的 I/O 操作;如果没有命中，则操作系统会向磁盘发起读取请求并将读取的数据页存入 页缓存，之后再将数据返回给进程。同样，如果一个进程需要将数据写入磁盘，那么操作系统也会检测数据对应的页是否在页缓存中，如果不存在，则会先在页缓存中添加相应的页，最后将数据写入对应的页。被修改过后的页也就变成了脏页，操作系统会在合适的时间把脏页中的 数据写入磁盘，以保持数据的一致性。

对一个进程而言，它会在进程内部缓存处理所需的数据，然而这些数据有可能还缓存在操 作系统的页缓存中，因此同一份数据有可能被缓存了两次。并且，除非使用 Direct I/O 的方式， 否则页缓存很难被禁止。**此外，用过 Java 的人一般都知道两点事实:对象的内存开销非常大， 通常会是真实数据大小的几倍甚至更多，空间使用率低下;Java 的垃圾回收会随着堆内数据的 增多而变得越来越慢。基于这些因素，使用文件系统并依赖于页缓存的做法明显要优于维护一 个进程内缓存或其他结构，至少我们可以省去了一份进程内部的缓存消耗，同时还可以通过结构紧凑的字节码来替代使用对象的方式以节省更多的空间。如此，我们可以在 32GB 的机器上使用 28GB 至 30GB 的内存而不用担心 GC 所带来的性能问题。此外，即使 Kafka 服务重启， 页缓存还是会保持有效，然而进程内的缓存却需要重建。这样也极大地简化了代码逻辑，因为 维护页缓存和文件之间的一致性交由操作系统来负责，这样会比进程内维护更加安全有效。**

Kafka 中大量使用了页缓存，这是 Kafka 实现高吞吐的重要因素之一。虽然消息都是先被写入页缓存，然后由操作系统负责具体的刷盘任务的。

## 零拷贝

我在很久之前就之前就发过一篇《[什么是Zero Copy](https://link.juejin.cn?target=https%3A%2F%2Fblog.csdn.net%2Fu013256816%2Farticle%2Fdetails%2F52589524)》,如果对Zero Copy不了解的同学可以翻阅一下。Kafka使用了Zero Copy技术提升了消费的效率。前面所说的Kafka将消息先写入页缓存，如果消费者在读取消息的时候如果在页缓存中可以命中，那么可以直接从页缓存中读取，这样又节省了一次从磁盘到页缓存的copy开销。另外对于读写的概念可以进一步了解一下什么是写放大和读放大。

## 附

一个磁盘IO流程可以参考下图：

![](https://cdn.nlark.com/yuque/0/2021/png/2725209/1634521869712-d75e2ef6-2958-4feb-bf96-5bc35947a9c7.png)