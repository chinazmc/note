#nsq 

# nsq 启动流程讲解

这篇文章我们就正式的开始分析nsq的代码了，上一篇给大家介绍了下nsq的特性和功能。再分析代码的同时，大家可以比对着我写的nsq精注版代码一遍看一遍调试。这样的效果更佳。

nsq精注版地址：[nsq精注版](https://github.com/gwyy/nsq-learn)

下面进入正题，nsqd的主函数位于apps/nsqd.go中的main函数。 在初始化的时候，它使用了第三方进程管理包 go-svc 来托管进程，go-svc有三个方法进行管理：

```go
func main() {
	if err := svc.Run(prg, syscall.SIGINT, syscall.SIGTERM); err != nil {
		logFatal("%s", err)
	}
}
//svc 的init 方法 初始化方法
func (p *program) Init(env svc.Environment) error {
	// 检查是否是windows 服务。。。目测一般时候也用不到，基本上可以直接过
	if env.IsWindowsService() {
		dir := filepath.Dir(os.Args[0])
		return os.Chdir(dir)
	}
	return nil
}
//真正的启动方法
func (p *program) Start() error {
	nsqd, err := nsqd.New(opts)
	if err != nil {
		logFatal("failed to instantiate nsqd - %s", err)
	}
	return nil
}

func (p *program) Stop() error {
	p.once.Do(func() {
		p.nsqd.Exit()
	})
	return nil
}
```

可以看到，man方法实例化了 program 结构体，该结构体从nsq启动一直到销毁贯穿了整个流程，然后实例化了svc包调用了run方法。通过看svc包源码可以发现。svc包内部依次调用了 Init、Start、和接收到指定信号后调用Stop方法。

svc.Init方法主要是判断了在windows下面目录一些特殊处理，可以直接略过。 而svc.Stop方法 主要是调用了program的Exit方法做了一些销毁。我们重点看下 svc.Start方法：

```go
/* 实例化并初始一些配置和默认值 */
opts := nsqd.NewOptions()
```

首先实例化了 Options结构体，该方法内部先获取到了主机名md5,并且作为默认的当前机器id, 然后return了 Options 结构体指针。设置了一些基本的必要的默认参数。

接下来就是设置和解析用户传来的参数，并且中间穿插了打印版本号，如果是打印版本号，打印后就直接退出。用户配置这里也可以通过配置文件进行读取，最后检测配置文件合法性，并且按照优先级依次设置配置文件：

```go
flagSet := nsqdFlagSet(opts)
flagSet.Parse(os.Args[1:])  //解析用户传参
// 初始化load 随机数种子 time.Now().UnixNano()  单位纳秒
rand.Seed(time.Now().UTC().UnixNano())
// 打印版本号,接收命令行参数version  默认值：false 然后直接结束
if flagSet.Lookup("version").Value.(flag.Getter).Get().(bool) {
  fmt.Println(version.String("nsqd"))
  os.Exit(0)
}
// 获取外部的配置文件，解析toml文件格式
var cfg config
configFile := flagSet.Lookup("config").Value.String()
if configFile != "" {
  _, err := toml.DecodeFile(configFile, &cfg)
  if err != nil {
    logFatal("failed to load config file %s - %s", configFile, err)
  }
}
// 检查配置文件
cfg.Validate()
// 采用优先级从高到低依次进行解析，最终
options.Resolve(opts, flagSet, cfg)
```

然后就就是New了一个nsq的实例 ，并且把nsqd对象加入到 progrem中的nsqd属性中去：

```go
nsqd, err := nsqd.New(opts)
if err != nil {
  logFatal("failed to instantiate nsqd - %s", err)
}
//加入到program类里面
p.nsqd = nsqd
```

我们来到 nsqd.New 方法，可以看到该方法做了很多事情，一开始设置了默认路径，并且 设置了默认 log 等一些操作。并且实例化了 NSQD结构体指针。

```go
var err error
// 设置数据缓存路径,主要就是 .dat 文件,记录了 topic 和 channel 的信息
dataPath := opts.DataPath
...
//默认  logger 是否设置,如果没设置 用系统的log
opts.Logger = log.New(os.Stderr, opts.LogPrefix, log.Ldate|log.Ltime|log.Lmicroseconds)
//实例化主类 也是结构体指针
n := &NSQD{
...
}
```

接下里实例化了http客户端，并且实例化了clusterinfo结构体，并且做了一系列的其他的初始化。这里就不一一说明了，直接看代码我有写注释：

```go
//实例化 http_client 结构体，简单包了一层http 	 创建一个 HTTP 客户端，用来从 lookupd 中获取 topic 数据
httpcli := http_api.NewClient(nil, opts.HTTPClientConnectTimeout, opts.HTTPClientRequestTimeout)
//实例化clusterinfo
n.ci = clusterinfo.New(n.logf, httpcli)
...
//给数据目录加锁
err = n.dl.Lock()
// 设置前缀先把统计前缀拼出来存到 opts.StatsdPrefix
if opts.StatsdPrefix != "" {
  opts.StatsdPrefix = prefixWithHost
}
...
// 设置 TLS config
tlsConfig, err := buildTLSConfig(opts)
n.tlsConfig = tlsConfig
...
//初始化tcp server
n.tcpServer = &tcpServer{}
//监听tcp端口
n.tcpListener, err = net.Listen("tcp", opts.TCPAddress)
//监听http端口
n.httpListener, err = net.Listen("tcp", opts.HTTPAddress)
...
//如果开了https
if n.tlsConfig != nil && opts.HTTPSAddress != "" {
  //监听 https
  n.httpsListener, err = tls.Listen("tcp", opts.HTTPSAddress, n.tlsConfig)
}
//这里注意下，端口监听并不会阻塞程序，会直接返回，accept才会阻塞程序
```

到这里，nsqd.New 方法结束，总结下：

1.  开启了主协程，监听了退出信号
2.  初始化并且合并了配置项
3.  实例化的nsq主实例。
4.  给数据目录加锁，监听了tcp和http接口（有https会多个监听https）

我们继续看svc.Start方法。接下来就是加载历史数据到内存：
```go
//加入到program类里面
p.nsqd = nsqd
err = p.nsqd.LoadMetadata()
```

这一步相对来说比较复杂，我们进入到 LoadMetadata函数里面看看：

```go
//使用atomic包中的方法来保证方法执行前和执行后isLoading值的改变
atomic.StoreInt32(&n.isLoading, 1)
defer atomic.StoreInt32(&n.isLoading, 0)
//得到文件路径 nsqd.dat
fn := newMetadataFile(n.getOpts())
//打开文件 读取所有数据
data, err := readOrEmpty(fn)
...
//循环所有topic
for _, t := range m.Topics {
    //使用GetTopic函数通过名字获得topic对象
    topic := n.GetTopic(t.Name)
    //获取当前topic下所有的channel，并且遍历channel，执行的操作与topic基本一致
    for _, c := range t.Channels {
        channel := topic.GetChannel(c.Name)
    }
    //topic启动
    topic.Start()
}
```

首先，我们看到nsq使用了 atomic.StoreInt32(&n.isLoading, 1) 这样的赋值方式来保证原子性的写入 n.isLoading,为什么不直接赋值呢，比如说 n.isLoading = 1 ,其实是因为这样无法保证原子性，使用StoreInt32赋值的时候，任何cpu都不会进行针对进行同一个值的读或写操作。如果我们把所有针对此值的写操作都改为原子操作，那么就不会出现针对此值的读操作读操作因被并发的进行而读到修改了一半的情况。 相同的。你也应该就能理解为什么它在defer的时候在通过原子性写入，把isLoading 改成0了吧。

接下来nsq读取本地文件 nsqd.dat 里面的内容，作为初始化数据加载进来，循环所有Topic, 初始化所有的Topic,和每一个Topic下面的Channel 。最后启动这个Topic。这一块逻辑比较复杂，后面我们单独开一篇来讲。

继续回到main文件。LoadMetadata后是 PresistMetadata 。从文件中load进来初始化后马上写入到文件一遍。这一步操作应该为了更新 nsqd.dat 文件中的信息,因为在加载的过程中可能会对原有信息做一些改变。比如说版本号等。

最后就是开个子协程执行 Main方法了。这里多说一下，为什么要开个子协程呢？大家可以暂停思考下。其实大家是不是还记得文章一开始的地方说过的nsq是需要第三方进程管理包 go-svc 来托管进程。go-svc在执行完Start方法后，会阻塞监听信号量来判断是否要关闭进程。但是Main方法里面也有业务逻辑要阻塞监听ErrorChannel 来退出进程。所以这里需要2个协程阻塞监听channel. 不知道大家有没有猜到呢？


```go
go func() {
		err := p.nsqd.Main()
		if err != nil {
			//有问题直接停止掉结束进程
			p.Stop()
			os.Exit(1)
		}
	}()
```

下面我们就进入Main函数里面看下他会做什么逻辑，我把不重要的代码删了一些：

```go
func (n *NSQD) Main() error {
    //实例化上下文对下，传入 NSQD 对象作为全局变量
    ctx := &context{n}
    ...
    //waitGroup开个协程 监听tcp连接 一直接受请求 accpet
    n.waitGroup.Wrap(func() {
        exitFunc(protocol.TCPServer(n.tcpListener, n.tcpServer, n.logf))
    })
    //监听http连接 初始化所有的路由 roter
    httpServer := newHTTPServer(ctx, false, n.getOpts().TLSRequired == TLSRequired)
    //开了个协程  监听http
    n.waitGroup.Wrap(func() {
        //调用 http_api.Serve 开始启动 HTTPServer 并在 4151 端口进行 HTTP 通信.
        exitFunc(http_api.Serve(n.httpListener, httpServer, "HTTP", n.logf))
    })
    //循环监控队列信息
    n.waitGroup.Wrap(n.queueScanLoop)    // 处理消息的优先队列
    //开了个协程  节点信息管理
    n.waitGroup.Wrap(n.lookupLoop)      // 如果 nsqd 发生变化，同步至 nsqloopdup，函数定义在 lookup 中
    ...
    //阻塞监听 exitCh  有问题直接返回
    err := <-exitCh
    return err
}
```

nsq 自己实现了一个工具函数 n.waitGroup.Wrap。使用该函数每次会开一个新的协程。 可以看到整个流程下来，阻塞实现http的Accept，阻塞tcp的Accept，（如果有https也会监听https).都是外面包了一个 waitGroup .每个都是一个单独的协程。最后Main函数执行了 n.queueScanLoop 和 n.lookupLoop。这两块都是比较复杂的流程。我们后面会单独开一章来讲。我们可以先简单的了解下。

n.queueScanLoop 函数维护并管理 goroutine 池的数量（默认4个），这些 goroutine 主要用于处理 channel 中 延时优先级队列和等待消费确认优先级队列。同时 queueScanLoop 循环随机选择 channel （默认20个）并交给工作线程池进行处理。

对于等待消费确认的队列，如果超过最大等待时间。nsq将会尝试重新发送消息。

对于延迟消息，每次从最小堆里拿到到底的消息并且发送。

n.lookupLook 函数是用于和lookupd 交户使用的事件处理模块。例如Topic 增加或者删除， channel 增加或者删除 需要对所有 nslookupd 模块做消息广播等处理逻辑，均在此处实现。 主要的事件:

定时心跳操作 每隔 15s 发送 PING 到 所有 nslookupd 的节点上

topic,channel新增删除操作 发送消息到所有 nslookupd 的节点上

配置修改的操作 如果配置修改，会重新从配置中刷新一次 nslookupd 节点

最后，本来打算自己画个启动流程图，不过网上有别人画好的还不错的，我就直接粘过来了。 ![流转](https://img1.liangtian.me/myblog/imgs/nsq21.jpg?x-oss-process=style/small)
# nsq Topic

与Topic相关的代码主要位于nsqd/topic.go中。

上一篇文字我们讲解了下nsq的启动流程。对nsq的整体框架有了一个大概的了解。本篇文章就是由大到小。对于topic这一部分进行详尽的讲解。

topic 管理着多个 channel 通过从 client 中获取消息，然后将消息发送到 channel 中传递给客户端.在 channel 初始化时会加载原有的 topic 并在最后统一执行 topic.Start(),新创建的 topic 会同步给 lookupd 后开始运行. nsqd 中通过创建创建多个 topic 来管理不同类别的频道.

### topic结构体：

```go
type Topic struct {
  // 64bit atomic vars need to be first for proper alignment on 32bit platforms
  // 这两个字段仅作统计信息,保证 32 位对其操作
  messageCount uint64  // 累计消息数
  messageBytes uint64// 累计消息体的字节数

  sync.RWMutex  // 加锁，包括 putMessage

  name              string // topic名，生产和消费时需要指定此名称
  channelMap        map[string]*Channel  // 保存每个channel name和channel指针的映射
  backend           BackendQueue    // 磁盘队列，当内存memoryMsgChan满时，写入硬盘队列
  memoryMsgChan     chan *Message    // 消息优先存入这个内存chan
  startChan         chan int    // 接收开始信号的 channel，调用 start 开始 topic 消息循环

  exitChan          chan int    // 判断 topic 是否退出

  // 在 select 的地方都要添加 exitChan
  // 除非使用 default 或者保证程序不会永远阻塞在 select 处,即可以退出循环
  // channel 更新时用来通知并更新消息循环中的 chan 数组
  channelUpdateChan chan int
  // 用来等待所有的子 goroutine
  waitGroup         util.WaitGroupWrapper
  exitFlag          int32     // topic 退出标识符
  idFactory         *guidFactory    // 生成 guid 的工厂方法

  ephemeral      bool  // 该 topic 是否是临时 topic
  deleteCallback func(*Topic)   // topic 删除时的回调函数
  deleter        sync.Once   // 确保 deleteCallback 仅执行一次

  paused    int32   // topic 是否暂停
  pauseChan chan int   // 改变 topic 暂停/运行状态的通道
  ctx *context  // topic 的上下文
}
```

可以看到。topic 采用了 map + *Channel 来管理所有的channel. 并且也有 memoryMsgChan 和 backend 2个队列。

### 实例化Topic :

下面就是 topic 的创建流程,传入的参数参数包括,topicName,上下文环境,删除回调函数:

```go
func NewTopic(topicName string,ctx *context,deleteCallback func(*Topic)) *Topic {
  t := &Topic{
    name:              topicName, //topic名称
    channelMap:        make(map[string]*Channel),
    memoryMsgChan:     nil,
    startChan:         make(chan int, 1),
    exitChan:          make(chan int),
    channelUpdateChan: make(chan int),
    ctx:               ctx, //上下文指针
    paused:            0,
    pauseChan:         make(chan int),
    deleteCallback:    deleteCallback, //删除callback函数
    // 所有 topic 使用同一个 guidFactory，因为都是用的 nsqd 的 ctx.nsqd.getOpts().ID 为基础生成的
    idFactory:         NewGUIDFactory(ctx.nsqd.getOpts().ID),
  }
  // create mem-queue only if size > 0 (do not use unbuffered chan)
  //  // 根据消息队列生成消息 chan,default size = 10000
  if ctx.nsqd.getOpts().MemQueueSize > 0 {
    // 初始化一个消息队列
    t.memoryMsgChan = make(chan *Message, ctx.nsqd.getOpts().MemQueueSize)
  }
  // 判断这个 topic 是不是暂时的，暂时的 topic 消息仅仅存储在内存中
  // DummyBackendQueue 和 diskqueue 均实现了 backend 接口
  if strings.HasSuffix(topicName, "#ephemeral") {
    // 临时的 topic，设置标志并使用 newDummyBackendQueue 初始化 backend
    t.ephemeral = true
    t.backend = newDummyBackendQueue()   // 实现了 backend 但是并没有逻辑，所有操作仅仅返回 nil
  } else {
    dqLogf := func(level diskqueue.LogLevel, f string, args ...interface{}) {
      opts := ctx.nsqd.getOpts()
      lg.Logf(opts.Logger, opts.LogLevel, lg.LogLevel(level), f, args...)
    }
    // 使用 diskqueue 初始化 backend 队列
    t.backend = diskqueue.New(
      topicName,
      ctx.nsqd.getOpts().DataPath,
      ctx.nsqd.getOpts().MaxBytesPerFile,
      int32(minValidMsgLength),
      int32(ctx.nsqd.getOpts().MaxMsgSize)+minValidMsgLength,
      ctx.nsqd.getOpts().SyncEvery,
      ctx.nsqd.getOpts().SyncTimeout,
      dqLogf,
    )
  }
  // 使用一个新的协程来执行 messagePump
  //startChan 就发送给了它,messagePump 函数负责分发整个 topic 接收到的消息给该 topic 下的 channels.
  t.waitGroup.Wrap(t.messagePump)
  // 调用 Notify
  t.ctx.nsqd.Notify(t)

  return t
}
```

可以看到先实例化了一个Topic指针对象。初始化`memoryMsgChan队列`， 默认1000个。并且判断topicName是否是临时topic,如果是的话，`BackendQueue`（这是个接口）实现了一个空的内存Queue. 否则使用 `diskqueue`来初始化 backend队列。

随后，NewTopic函数开启一个新的goroutine来执行messagePump函数，该函数负责消息循环，将进入topic中的消息投递到channel中。

最后，NewTopic函数执行`t.ctx.nsqd.Notify(t)`，该函数在topic和channel创建、停止的时候调用， Notify函数通过执行`PersistMetadata`函数，将topic和channel的信息写到文件中。

```go
func (n *NSQD) Notify(v interface{}) {
  persist := atomic.LoadInt32(&n.isLoading) == 0
  n.waitGroup.Wrap(func() {
    // by selecting on exitChan we guarantee that
    // we do not block exit, see issue #123
    select {
    //如果执行那一刻 有exitChan 那么就走exit
    case <-n.exitChan:
      //否则就走正常逻辑 往notifyChan 里发个消息
    case n.notifyChan <- v:
      if !persist {
        return
      }
      n.Lock()
      err := n.PersistMetadata()
      if err != nil {
        n.logf(LOG_ERROR, "failed to persist metadata - %s", err)
      }
      n.Unlock()
    }
  })
}
```

在`Notify`函数的实现时，首先考虑了数据持久化的时机，如果当前nsqd尚在初始化，则不需要立即持久化数据，因为nsqd在初始化后会进行一次统一的持久化工作，

`Notify`在进行数据持久化的时候采用了异步的方式。使得topic和channel能以同步的方式来调用Nofity而不阻塞。在异步运行的过程中， 通过waitGroup和监听exitChan的使用保证了结束程序时goroutine能正常退出。

在执行持久化之前，`case n.notifyChan <- v:`语句向notifyChan传递消息，触发`lookupLoop`函数（nsqd/lookup.go中）接收`notifyChan`消息的部分， 从而实现向loopupd注册/取消注册响应的topic或channel。

### 消息写入Topic

客户端通过nsqd的HTTP API或TCP API向特定topic发送消息，nsqd的HTTP或TCP模块通过调用对应topic的`PutMessage`或`PutMessages`函数， 将消息投递到topic中。`PutMessage`或`PutMessages`函数都通过topic的私有函数put进行消息的投递，两个函数的区别仅在`PutMessage`只调用一次put， `PutMessages`遍历所有要投递的消息，对每条消息使用put函数进行投递。默认topic会优先往`memoryMsgChan` 队列内投递，如果内存队列已满，才会往磁盘队列写入，（临时的topic磁盘队列不做任何存储，数据直接丢弃）

```go
func (t *Topic) put(m *Message) error {
    select {
    case t.memoryMsgChan <- m:
    default:
        //写入磁盘队列
    }
    return nil
}
```

### Start && messagePump 操作

topic的Start方法就是发送了个 startChan ，这里有个小技巧，nsq使用了select来发送这个消息，这样做的目的是如果start被并发调用了，第二个start会直接走到default里，什么都不做.

那么这个Start函数都有哪里调用的呢。

1.  nsqd启动的时候，触发 LoadMetadata 会把文件里的topic加载到内存里，这时候会调用Start方法
    
2.  用户通过请求获取topic的时候会通过 getTopic 来获取或者创建topic
    

```go
func (t *Topic) Start() {
  select {
  case t.startChan <- 1:
  default:
  }
}
```

接下来我们看下 `messagePump`, 刚才的 startChan 就是发给了这个函数，该函数在创建新的topic时通过waitGroup在新的goroutine中运行。该函数仅在触发 startChan 开始运行，否则会阻塞住，直到退出。

```go
for {
    select {
    case <-t.channelUpdateChan:
      continue
    case <-t.pauseChan:
      continue
    case <-t.exitChan:
      goto exit
    case <-t.startChan:
    }
    break
  }
```

`messagePump`函数初始化时先获取当前存在的channel数组，设置`memoryMsgChan`和`backendChan`，随后进入消息循环， 在循环中主要处理四种消息：

接收来自`memoryMsgChan`和`backendChan`两个go channel进入的消息，并向当前的channal数组中的channel进行投递

处理当前topic下channel的更新

处理当前topic的暂停和恢复

监听当前topic的删除

### 消息投递

```go
case msg = <-memoryMsgChan:
case buf = <-backendChan:
    msg, err = decodeMessage(buf)
    if err != nil {
        t.ctx.nsqd.logf("ERROR: failed to decode message - %s", err)
        continue
    }
```

这两个case语句处理进入topic的消息，关于两个go channel的区别会在后续的博客中分析。 从`memoryMsgChanbackendChan`读取到的消息是_Message类型，而从`backendChan`读取到的消息是byte数组的。 因此取出`backendChan`的消息后海需要调用`decodeMessage`函数对byte数组进行解码，返回_Message类型的消息。 二者都保存在msg变量中。

```go
for i, channel := range chans {
    chanMsg := msg
    if i > 0 {
        chanMsg = NewMessage(msg.ID, msg.Body)
        chanMsg.Timestamp = msg.Timestamp
        chanMsg.deferred = msg.deferred
    }
    if chanMsg.deferred != 0 {
        channel.StartDeferredTimeout(chanMsg, chanMsg.deferred)
        continue
    }
    err := channel.PutMessage(chanMsg)
    if err != nil {
        t.ctx.nsqd.logf(
            "TOPIC(%s) ERROR: failed to put msg(%s) to channel(%s) - %s",
            t.name, msg.ID, channel.name, err)
    }
}
```

随后是将消息投到每个channel中，首先先对消息进行复制操作，这里有个优化，对于第一次循环， 直接使用原消息进行发送以减少复制对象的开销，此后的循环将对消息进行复制。对于即时的消息， 直接调用channel的PutMessage函数进行投递，对于延迟的消息， 调用channel的`StartDeferredTimeout`函数进行投递。对于这两个函数的投递细节，后续博文中会详细分析。

### Topic下Channel的更新

```go
case <-t.channelUpdateChan:
    chans = chans[:0]
    t.RLock()
    for _, c := range t.channelMap {
        chans = append(chans, c)
    }
    t.RUnlock()
    if len(chans) == 0 || t.IsPaused() {
        memoryMsgChan = nil
        backendChan = nil
    } else {
        memoryMsgChan = t.memoryMsgChan
        backendChan = t.backend.ReadChan()
    }
    continue
```

Channel的更新比较简单，从`channelMap`中取出每个channel，构成channel的数组以便后续进行消息的投递。 并且根据当前是否有channel以及该topic是否处于暂停状态来决定`memoryMsgChan和backendChan`是否为空。

### Topic的暂停和恢复

```go
case pause := <-t.pauseChan:
    if pause || len(chans) == 0 {
        memoryMsgChan = nil
        backendChan = nil
    } else {
        memoryMsgChan = t.memoryMsgChan
        backendChan = t.backend.ReadChan()
    }
    continue
```

这个case既处理topic的暂停也处理topic的恢复，pause变量决定其究竟是哪一种操作。 Topic的暂停和恢复其实和topic的更新很像，根据是否暂停以及是否有channel来决定是否分配memoryMsgChan和backendChan。

### messagePump函数的退出

```go
case <-t.exitChan:
    goto exit

// ...
exit:
    t.ctx.nsqd.logf("TOPIC(%s): closing ... messagePump", t.name)
}
// End of messagePump
```

`messagePump`通过监听exitChan来获知topic是否被删除，当topic的删除时，跳转到函数的最后，输出日志后退出消息循环。

### Topic的关闭和删除

```go
// Delete empties the topic and all its channels and closes
func (t *Topic) Delete() error {
    return t.exit(true)
}

// Close persists all outstanding topic data and closes all its channels
func (t *Topic) Close() error {
    return t.exit(false)
}

func (t *Topic) exit(deleted bool) error {
    if !atomic.CompareAndSwapInt32(&t.exitFlag, 0, 1) {
        return errors.New("exiting")
    }

    if deleted {
        t.ctx.nsqd.logf("TOPIC(%s): deleting", t.name)

        // since we are explicitly deleting a topic (not just at system exit time)
        // de-register this from the lookupd
        t.ctx.nsqd.Notify(t)
    } else {
        t.ctx.nsqd.logf("TOPIC(%s): closing", t.name)
    }

    close(t.exitChan)

    // synchronize the close of messagePump()
    t.waitGroup.Wait()

    if deleted {
        t.Lock()
        for _, channel := range t.channelMap {
            delete(t.channelMap, channel.name)
            channel.Delete()
        }
        t.Unlock()

        // empty the queue (deletes the backend files, too)
        t.Empty()
        return t.backend.Delete()
    }

    // close all the channels
    for _, channel := range t.channelMap {
        err := channel.Close()
        if err != nil {
            // we need to continue regardless of error to close all the channels
            t.ctx.nsqd.logf("ERROR: channel(%s) close - %s", channel.name, err)
        }
    }

    // write anything leftover to disk
    t.flush()
    return t.backend.Close()
}
// Exiting returns a boolean indicating if this topic is closed/exiting
func (t *Topic) Exiting() bool {
    return atomic.LoadInt32(&t.exitFlag) == 1
}
```

Topic关闭和删除的实现都是调用exit函数，只是传递的参数不同，删除时调用exit(true)，关闭时调用exit(false)。 exit函数进入时通过`atomic.CompareAndSwapInt32`函数判断当前是否正在退出，如果不是，则设置退出标记，对于已经在退出的topic，不再重复执行退出函数。 接着对于关闭操作，使用Notify函数通知lookupd以便其他nsqd获知该消息。

随后，exit函数调用`close(t.exitChan)`和`t.waitGroup.Wait()`通知其他正在运行goroutine当前topic已经停止，并等待`waitGroup`中的goroutine结束运行。

最后，对于删除和关闭两种操作，执行不同的逻辑来完成最后的清理工作：

1.  对于删除操作，需要清空`channelMap`并删除所有channel，然后删除内存和磁盘中所有未投递的消息。最后关闭backend管理的的磁盘文件。
    
2.  对于关闭操作，不清空channelMap，只是关闭所有的channel，使用flush函数将所有`memoryMsgChan`中未投递的消息用`writeMessageToBackend`保存到磁盘中。最后关闭backend管理的的磁盘文件。
    

```go
func (t *Topic) flush() error {
    //...
    for {
        select {
        case msg := <-t.memoryMsgChan:
            err := writeMessageToBackend(&msgBuf, msg, t.backend)
            if err != nil {
                t.ctx.nsqd.logf(
                    "ERROR: failed to write message to backend - %s", err)
            }
        default:
            goto finish
        }
    }
    
finish:
    return nil
}
```

`flush`函数也使用到了default分支来检测是否已经处理完全部消息。 由于此时已经没有生产者向memoryMsgChan提供消息，因此如果出现阻塞就表示消息已经处理完毕。

```go
func (t *Topic) Empty() error {
    for {
        select {
        case <-t.memoryMsgChan:
        default:
            goto finish
        }
    }
finish:
    return t.backend.Empty()
}
```

在删除topic时用到的`Empty`函数跟`flush`处理逻辑类似，只不过Empty只释放`memoryMsgChan`消息，而不保存它们。

topic 下的源码基本就看完了，虽然还没有别的部分完整的完整的串联起来，但是也可以了解到，多个 topic 在初始化时就开启了消息循环 goroutine，执行完 Start 后开始消息分发，如果是正常的Topic,除了默认10000的内存队列，还会有个硬盘队列。topic将收到的消息分发到管理的 channel 中.每个 topic 运行的 goroutine 比较简单，只有一个消息分发 `goroutine: messagePump`

# nsq 源码 diskQueue 讲解

`diskQueue是backendQueue`接口的一个实现。`backendQueue`的作用是在实现在内存go channel缓冲区满的情况下对消息的处理的对象。 除了diskQueue外还有`dummyBackendQueue`实现了`backendQueue`接口。

对于临时（#ephemeral结尾）Topic/Channel，在创建时会使用`dummyBackendQueue`初始化backend， `dummyBackendQueue`只是为了统一临时和非临时Topic/Channel而写的，它只是实现了接口，不做任何实质上的操作， 因此在内存缓冲区满时直接丢弃消息。这也是临时Topic/Channel和非临时的一个比较大的差别。 每个非临时Topic/Channel，创建的时候使用`diskQueue`初始化`backend，diskQueue`的功能是将消息写入磁盘进行持久化， 并在需要时从中取出消息重新向客户端投递。

`diskQueue`的实现在nsqd/disk_queue.go中。需要注意一点，查找`diskQueue`中的函数的调用可能不会返回正确的结果， 因为`diskQueue`对外是以`backendQueue`形式存在，因此查找`diskQueue`的函数的调用情况时应当查找`backendQueue`中相应函数的调用。

### 创建和初始化

```go
// New instantiates an instance of diskQueue, retrieving metadata
// from the filesystem and starting the read ahead goroutine
func New(name string, dataPath string, maxBytesPerFile int64,
  minMsgSize int32, maxMsgSize int32,
  syncEvery int64, syncTimeout time.Duration, logf AppLogFunc) Interface {
  d := diskQueue{
    name:              name,
    dataPath:          dataPath,
    maxBytesPerFile:   maxBytesPerFile,
    minMsgSize:        minMsgSize,
    maxMsgSize:        maxMsgSize,
    readChan:          make(chan []byte),
    writeChan:         make(chan []byte),
    writeResponseChan: make(chan error),
    emptyChan:         make(chan int),
    emptyResponseChan: make(chan error),
    exitChan:          make(chan int),
    exitSyncChan:      make(chan int),
    syncEvery:         syncEvery,
    syncTimeout:       syncTimeout,
    logf:              logf,
  }
​
  // no need to lock here, nothing else could possibly be touching this instance
  err := d.retrieveMetaData()
  if err != nil && !os.IsNotExist(err) {
    d.logf(ERROR, "DISKQUEUE(%s) failed to retrieveMetaData - %s", d.name, err)
  }
​
  go d.ioLoop()
  return &d
}
```

`diskQueue`的获得是通过`newDiskQueue`，该函数比较简单，通过传入的参数创建一个`dispQueue`， 然后通过`retrieveMetaData`函数获取之前与该`diskQueue`相关联的`Topic/Channel`已经持久化的信息。最后启动ioLoop循环处理消息。

```go
// retrieveMetaData initializes state from the filesystem
func (d *diskQueue) retrieveMetaData() error {
  var f *os.File
  var err error
​
  fileName := d.metaDataFileName()
  f, err = os.OpenFile(fileName, os.O_RDONLY, 0600)
  if err != nil {
    return err
  }
  defer f.Close()
​
  var depth int64 
   //多少数据，读数量，读指针，写数量，写指针
  _, err = fmt.Fscanf(f, "%d\n%d,%d\n%d,%d\n",
    &depth,
    &d.readFileNum, &d.readPos,
    &d.writeFileNum, &d.writePos)
  if err != nil {
    return err
  }
  atomic.StoreInt64(&d.depth, depth)
  d.nextReadFileNum = d.readFileNum
  d.nextReadPos = d.readPos
​
  return nil
}
```

`retrieveMetaData`函数从磁盘中恢复`diskQueue`的状态。`diskQueue`会定时将自己的状态备份到文件中， 文件名由`metaDataFileName`函数确定。`retrieveMetaData`函数同样通过`metaDataFileName`函数获得保存状态的文件名并打开。 该文件只有三行，格式为`%d\n%d,%d\n%d,%d\n`，第一行保存着该`diskQueue`中消息的数量（depth）， 第二行保存`readFileNum和readPos`，第三行保存`writeFileNum和writePos`。

```go
// persistMetaData atomically writes state to the filesystem
func (d *diskQueue) persistMetaData() error {
  var f *os.File
  var err error
​
  fileName := d.metaDataFileName()
  tmpFileName := fmt.Sprintf("%s.%d.tmp", fileName, rand.Int())
​
  // write to tmp file
  f, err = os.OpenFile(tmpFileName, os.O_RDWR|os.O_CREATE, 0600)
  if err != nil {
    return err
  }
​  //多少数据，读数量，读指针，写数量，写指针 
  _, err = fmt.Fprintf(f, "%d\n%d,%d\n%d,%d\n",
    atomic.LoadInt64(&d.depth),
    d.readFileNum, d.readPos,
    d.writeFileNum, d.writePos)
  if err != nil {
    f.Close()
    return err
  }
  f.Sync()
  f.Close()
​
  // atomically rename
  return os.Rename(tmpFileName, fileName)
}
```

与`retrieveMetaData`相对应的是`persistMetaData`函数，这个函数将运行时的元数据保存到文件用于下次重新构建`diskQueue`时的恢复。 逻辑基本与`retrieveMetaData`，此处不再赘述。

### diskQueue的消息循环

```go
func (d *diskQueue) ioLoop() {
  var dataRead []byte
  var err error
  var count int64
  var r chan []byte
​
  syncTicker := time.NewTicker(d.syncTimeout)
​
  for {
    // dont sync all the time :)
    if count == d.syncEvery {
      d.needSync = true
    }
​
    if d.needSync {
      err = d.sync()
      if err != nil {
        d.logf(ERROR, "DISKQUEUE(%s) failed to sync - %s", d.name, err)
      }
      count = 0
    }
​
    if (d.readFileNum < d.writeFileNum) || (d.readPos < d.writePos) {
      if d.nextReadPos == d.readPos {
        dataRead, err = d.readOne()
        if err != nil {
          d.logf(ERROR, "DISKQUEUE(%s) reading at %d of %s - %s",
            d.name, d.readPos, d.fileName(d.readFileNum), err)
          d.handleReadError()
          continue
        }
      }
      r = d.readChan
    } else {
      r = nil
    }
​
    select {
    // the Go channel spec dictates that nil channel operations (read or write)
    // in a select are skipped, we set r to d.readChan only when there is data to read
    case r <- dataRead:
      count++
      // moveForward sets needSync flag if a file is removed
      d.moveForward()
    case <-d.emptyChan:
      d.emptyResponseChan <- d.deleteAllFiles()
      count = 0
    case dataWrite := <-d.writeChan:
      count++
      d.writeResponseChan <- d.writeOne(dataWrite)
    case <-syncTicker.C:
      if count == 0 {
        // avoid sync when there's no activity
        continue
      }
      d.needSync = true
    case <-d.exitChan:
      goto exit
    }
  }
​
exit:
  d.logf(INFO, "DISKQUEUE(%s): closing ... ioLoop", d.name)
  syncTicker.Stop()
  d.exitSyncChan <- 1
}
```

`ioLoop`函数实现了`diskQueue`的消息循环，`diskQueue`的定时操作和读写操作的核心都在这个函数中完成。

函数首先使用`time.NewTicker(d.syncTimeout)`定义了`syncTicker`变量，`syncTicker`的类型是`time.Ticker`， 每隔`d.syncTimeout`时间就会在`syncTicker.C`这个go channel产生一个消息。 通过`select syncTicker.C`能实现至多`d.syncTimeout`时间就跳出select块一次，这种方式相当于一个延时的default子句。 在ioLoop中，通过这种方式，就能在一个goroutine中既实现消息的接收又实现定时任务（跳出select后执行定时任务，然后在进入select）。 有点类似于定时的轮询。

`ioLoop`的定时任务是调用sync函数刷新文件，防止突然结束程序后内存中的内容未被提交到磁盘，导致内容丢失。 控制是否需要同步的变量是`d.needSync`，该变量在一次sync后会被置为false，在许多需要刷新文件的地方会被置为true。 在ioLoop中，d.needSync变量还跟刷新计数器count变量有关，count值的变化规则如下：

1.  如果一次消息循环中，有写入操作，那么count就会被自增。
    
2.  当count达到d.syncEvery时，会将count重置为0并且将`d.needSync`置为true，随后进行文件的刷新。
    
3.  在`emptyChan`收到消息时，count会被重置为0，因为文件已经被删除了，所有要重置刷新计数器。
    
4.  在`syncTicker.C`收到消息后，会将count重置为0，并且将d.needSync置为true。也就是至多d.syncTimeout时间刷新一次文件。
    

`ioLoop`还定时检测当前是否有数据需要被读取，如果`(d.readFileNum < d.writeFileNum) || (d.readPos < d.writePos)` 和`d.nextReadPos == d.readPos`这两个条件成立，则执行`d.readOne()`并将结果放入`dataRead`中，然后设置`r`为`d.readChan`。 如果条件不成立，则将r置为空值nil。随后的select语句中有case r <- `dataRead:`这样一个分支，在注释中作者写了这是一个Golang的特性， 即：如果r不为空，则会将`dataRead`送入go channel。进入d.readChan的消息通过ReadChan函数向外暴露，最终被Topic/Channel的消息循环读取。 而如果r为空，则这个分支会被跳过。这个特性的使用统一了select的逻辑，简化了当数据为空时的判断。

### diskQueue的写操作

```go
// Put writes a []byte to the queue
func (d *diskQueue) Put(data []byte) error {
    d.RLock()
    defer d.RUnlock()
​
    if d.exitFlag == 1 {
        return errors.New("exiting")
    }
​
    d.writeChan <- data
    return <-d.writeResponseChan
}
```

写操作的对外接口是Put函数，该函数比较简单，加锁，并且将数据放入`d.writeChan`，等待`d.writeResponseChan`的结果后返回。 `d.writeChan`的接收在`ioLoop`中`select`的一个分支，处理时调用`writeOne`函数，并将处理结果放入`d.writeResponseChan`。

```go
// writeOne performs a low level filesystem write for a single []byte
// while advancing write positions and rolling files, if necessary
func (d *diskQueue) writeOne(data []byte) error {
    var err error
​
    if d.writeFile == nil {
        curFileName := d.fileName(d.writeFileNum)
        d.writeFile, err = os.OpenFile(curFileName, os.O_RDWR|os.O_CREATE, 0600)
        if err != nil {
            return err
        }
​
        d.logf("DISKQUEUE(%s): writeOne() opened %s", d.name, curFileName)
​
        if d.writePos > 0 {
            _, err = d.writeFile.Seek(d.writePos, 0)
            if err != nil {
                d.writeFile.Close()
                d.writeFile = nil
                return err
            }
        }
    }
​
    dataLen := int32(len(data))
​
    if dataLen < d.minMsgSize || dataLen > d.maxMsgSize {
        return fmt.Errorf("invalid message write size (%d) maxMsgSize=%d", dataLen, d.maxMsgSize)
    }
​
    d.writeBuf.Reset()
    //先在d里写入4个字节，标记长度 4个字节转成二进制
    err = binary.Write(&d.writeBuf, binary.BigEndian, dataLen)
    if err != nil {
        return err
    }
​    //再往d里写入数据
    _, err = d.writeBuf.Write(data)
    if err != nil {
        return err
    }
​
    // only write to the file once
   // 最终 长度 + 数据 一起写入文件
    _, err = d.writeFile.Write(d.writeBuf.Bytes())
    if err != nil {
        d.writeFile.Close()
        d.writeFile = nil
        return err
    }
​
    totalBytes := int64(4 + dataLen)
    d.writePos += totalBytes
    atomic.AddInt64(&d.depth, 1)
​
    if d.writePos > d.maxBytesPerFile {
        d.writeFileNum++
        d.writePos = 0
​
        // sync every time we start writing to a new file
        err = d.sync()
        if err != nil {
            d.logf("ERROR: diskqueue(%s) failed to sync - %s", d.name, err)
        }
​
        if d.writeFile != nil {
            d.writeFile.Close()
            d.writeFile = nil
        }
    }
​
    return err
}
```

`writeOne`函数是写操作的最终执行部分，负责将消息写入磁盘。函数逻辑比较简单。消息写入步骤如下：

1.  若当前要写的文件不存在，则通过d.fileName(d.writeFileNum)获得文件名，并创建文件
    
2.  根据d.writePos定位本次写的位置
    
3.  从要写入的内容得到要写入的长度
    
4.  先写入3中计算出的消息长度（4字节），然后写入消息本身
    
5.  将d.writePos后移4 + 消息长度作为下次写入位置。加4是因为消息长度本身也占4字节。
    
6.  判断d.writePos是否大于每个文件的最大字节数d.maxBytesPerFile，如果是，则将d.writeFileNum加1， 并重置d.writePos。这个操作的目的是为了防止单个文件过大。
    
7.  如果下次要写入新的文件，那么需要调用sync函数对当前文件进行同步。
    

### diskQueue的读操作

```go
// readOne performs a low level filesystem read for a single []byte
// while advancing read positions and rolling files, if necessary
func (d *diskQueue) readOne() ([]byte, error) {
    var err error
    var msgSize int32
​
    if d.readFile == nil {
        curFileName := d.fileName(d.readFileNum)
        d.readFile, err = os.OpenFile(curFileName, os.O_RDONLY, 0600)
        if err != nil {
            return nil, err
        }
​
        d.logf("DISKQUEUE(%s): readOne() opened %s", d.name, curFileName)
​
        if d.readPos > 0 {
            _, err = d.readFile.Seek(d.readPos, 0)
            if err != nil {
                d.readFile.Close()
                d.readFile = nil
                return nil, err
            }
        }
​
        d.reader = bufio.NewReader(d.readFile)
    }
​    //先读4个字节 int32
    err = binary.Read(d.reader, binary.BigEndian, &msgSize)
    if err != nil {
        d.readFile.Close()
        d.readFile = nil
        return nil, err
    }
​
    if msgSize < d.minMsgSize || msgSize > d.maxMsgSize {
        // this file is corrupt and we have no reasonable guarantee on
        // where a new message should begin
        d.readFile.Close()
        d.readFile = nil
        return nil, fmt.Errorf("invalid message read size (%d)", msgSize)
    }
​    //得到刚才读到的长度，申请一个固定长度的[]byte数组，比如说长度122
    readBuf := make([]byte, msgSize)   
    //一次性全部读完
    _, err = io.ReadFull(d.reader, readBuf)
    if err != nil {
        d.readFile.Close()
        d.readFile = nil
        return nil, err
    }
​
    totalBytes := int64(4 + msgSize)
​
    // we only advance next* because we have not yet sent this to consumers
    // (where readFileNum, readPos will actually be advanced)
    d.nextReadPos = d.readPos + totalBytes
    d.nextReadFileNum = d.readFileNum
​
    // TODO: each data file should embed the maxBytesPerFile
    // as the first 8 bytes (at creation time) ensuring that
    // the value can change without affecting runtime
    if d.nextReadPos > d.maxBytesPerFile {
        if d.readFile != nil {
            d.readFile.Close()
            d.readFile = nil
        }
​
        d.nextReadFileNum++
        d.nextReadPos = 0
    }
​
    return readBuf, nil
}
​
```

消息读取对外暴露的是一个go channel，而数据的最终来源是ioLoop中调用的readOne函数。readOne函数逻辑跟writeOne类似， 只是把写操作换成了读操作，唯一差异较大的地方是`d.nextReadPos`和`d.nextReadFileNum`这两个变量的使用。

在写操作时，如果写入成功，则可以直接将写入位置和写入文件更新。但是对于读操作来说，由于读取的目的是为了向客户端投递， 因此无法保证一定能投递成功。因此需要使用next开头的两个变量来保存成功后需要读的位置，如果投递没有成功， 则继续使用当前的读取位置将再一次尝试将消息投递给客户端。

```go
func (d *diskQueue) moveForward() {
    oldReadFileNum := d.readFileNum
    d.readFileNum = d.nextReadFileNum
    d.readPos = d.nextReadPos
    depth := atomic.AddInt64(&d.depth, -1)
​
    // see if we need to clean up the old file
    if oldReadFileNum != d.nextReadFileNum {
        // sync every time we start reading from a new file
        d.needSync = true
​
        fn := d.fileName(oldReadFileNum)
        err := os.Remove(fn)
        if err != nil {
            d.logf("ERROR: failed to Remove(%s) - %s", fn, err)
        }
    }
​
    d.checkTailCorruption(depth)
}
```

当消息投递成功后，则使用`moveForward`函数将保存在`d.nextReadPos`和`d.nextReadFileNum`中的值取出， 赋值给`d.readPos`和`d.readFileNum`，`moveForward`函数还负责清理已经读完的旧文件。最后，调用`checkTailCorruption`函数检查文件是否有错， 如果出现错误，则调用`skipToNextRWFile`重置读取和写入的文件编号和位置。

### diskQueue的其他函数

diskQueue中还有与错误处理相关的`handleReadError`，与关闭diskQueue相关的`Close`，`Delete`，`exit`，`Empty`和`deleteAllFiles`等， 函数，逻辑较简单，不再专门分析。

### diskQueue总结

diskQueue主要逻辑是对磁盘的读写操作，较为琐碎但没有复杂的架构。 其中消息循环的思路和读写过程周全的考虑都值得学习的。


# nsq - 一条消息的生命周期（一)

本篇我们带着大家一起走完一遍nsq的生命周期。

经过前面几篇的学习，相信大家对nsq已经有了一个大概的了解，我在写这篇文章的时候也看了很多其他人写的教程，发现大家对于分析系统每个点写的很不错，但是都很少有整体串起来一起走一遍，所以，我打算分成2-3章来带着大家从nsq启动到创建一个topic,然后发一条消息，最后再开个消费者接收消息，中间的所有流程都带大家一起走一遍，从而让大家能够深入地理解nsq的整体运行机制。

今天，这篇文章是整个 《一条消息的生命周期》第一章，我会从nsq的启动，nsqlookupd连接等方面开始讲起。

### 启动nsq

相信看了nsq这个系列的童鞋应该都知道nsq的启动脚本在哪里了吧，没错。就是在`apps/nsqd/main.go` 文件。我们可以切到当前目录，不过在这之前我们要先启动位于 `apps/nsqlookupd/`目录下的 `nsqlookupd`

```go
#启动 nsqlookupd
输出--------------------------------------------------------
➜  nsqlookupd git:(master) ✗ go run main.go 
[nsqlookupd] 2021/10/13 15:27:57.828505 INFO: nsqlookupd v1.2.1-alpha (built w/go1.15.15)
[nsqlookupd] 2021/10/13 15:27:57.828996 INFO: TCP: listening on [::]:4160
[nsqlookupd] 2021/10/13 15:27:57.828996 INFO: HTTP: listening on [::]:4161
[nsqlookupd] 2021/10/13 15:31:20.121567 INFO: TCP: new client(127.0.0.1:54011)
[nsqlookupd] 2021/10/13 15:31:20.121852 INFO: CLIENT(127.0.0.1:54011): desired protocol magic '  V1'
[nsqlookupd] 2021/10/13 15:31:20.122590 INFO: CLIENT(127.0.0.1:54011): IDENTIFY Address:liangtiandeMacBook-Pro.local TCP:4150 HTTP:4151 Version:1.2.1-alpha
[nsqlookupd] 2021/10/13 15:31:20.122661 INFO: DB: client(127.0.0.1:54011) REGISTER category:client key: subkey:
[nsqlookupd] 2021/10/13 15:31:35.121527 INFO: CLIENT(127.0.0.1:54011): pinged (last ping 14.998981s)
[nsqlookupd] 2021/10/13 15:31:50.120787 INFO: CLIENT(127.0.0.1:54011): pinged (last ping 14.99928s)
​
​
#接着我们启动nsq 
go run main.go options.go  --lookupd-tcp-address=127.0.0.1:4160
输出------------------------------------------------------------------------
[nsqd] 2021/10/13 15:31:20.095882 INFO: nsqd v1.2.1-alpha (built w/go1.15.15)
[nsqd] 2021/10/13 15:31:20.096040 INFO: ID: 933
[nsqd] 2021/10/13 15:31:20.096421 INFO: NSQ: persisting topic/channel metadata to nsqd.dat
[nsqd] 2021/10/13 15:31:20.120544 INFO: TCP: listening on [::]:4150
[nsqd] 2021/10/13 15:31:20.120655 INFO: LOOKUP(127.0.0.1:4160): adding peer
[nsqd] 2021/10/13 15:31:20.120685 INFO: LOOKUP connecting to 127.0.0.1:4160
[nsqd] 2021/10/13 15:31:20.120686 INFO: HTTP: listening on [::]:4151
[nsqd] 2021/10/13 15:31:20.123026 INFO: LOOKUPD(127.0.0.1:4160): peer info {TCPPort:4160 HTTPPort:4161 Version:1.2.1-alpha BroadcastAddress:liangtiandeMacBook-Pro.local}
```

可以看到，会输出默认的TCP和HTTP监听的端口，并且会把数据文件写入到当前目录的 nsqd.dat 内。并且连上了nsqlookupd 这个时候我们就算启动了nsqd。

### nsq和nsqlookupd 的链接

虽然启动成功了，但是我们还不知道nsq是怎么和nsqlookup链接上的，并且定期心跳的。其实nsqd主函数Main中启动与nsqlookupd服务通讯的工作线程lookupLoop

```go
ticker := time.Tick(15 * time.Second)
  for {
    if connect {
      //循环所有的 nsqlookupd 地址 我们这边就一个 127.0.0.1:4160
      for _, host := range n.getOpts().NSQLookupdTCPAddresses {
        if in(host, lookupAddrs) {
          continue
        }
        //LOOKUP(127.0.0.1:4160): adding peer
        n.logf(LOG_INFO, "LOOKUP(%s): adding peer", host)
        //实例化newLookupPeer数据结构，并且拿到链接callback方法
        lookupPeer := newLookupPeer(host, n.getOpts().MaxBodySize, n.logf,
          connectCallback(n, hostname))
        //尝试链接
        lookupPeer.Command(nil) // start the connection
        //把nsqlookupd 写入lookupPeers
        lookupPeers = append(lookupPeers, lookupPeer)
        lookupAddrs = append(lookupAddrs, host)
      }
      n.lookupPeers.Store(lookupPeers)
      connect = false
    }
​
```

第一次循环的时候，变量connect = true，nsq会尝试链接n`sqlookupd`. 其实就是连接上nsqlookupd的tcp端口，并且发送 V1信息。

```go
//标记链接成功
lp.state = stateConnected
//发送  V1
_, err = lp.Write(nsq.MagicV1)
```

发送后 V1后，如果没有失败，紧接着nsq会执行connectCallback，在connectCallback里面我们可以看下代码：

```go
func connectCallback(n *NSQD, hostname string) func(*lookupPeer) {
  return func(lp *lookupPeer) {
    //鉴权
    ci := make(map[string]interface{})
    ci["version"] = version.Binary
    ci["tcp_port"] = n.RealTCPAddr().Port
    ci["http_port"] = n.RealHTTPAddr().Port
    ci["hostname"] = hostname
    ci["broadcast_address"] = n.getOpts().BroadcastAddress
​
    cmd, err := nsq.Identify(ci)
    if err != nil {
      lp.Close()
      return
    }
    //发送鉴权
    resp, err := lp.Command(cmd)
    if err != nil {
      n.logf(LOG_ERROR, "LOOKUPD(%s): %s - %s", lp, cmd, err)
      return
    } else if bytes.Equal(resp, []byte("E_INVALID")) {
      n.logf(LOG_INFO, "LOOKUPD(%s): lookupd returned %s", lp, resp)
      lp.Close()
      return
    } else {
      err = json.Unmarshal(resp, &lp.Info)
      if err != nil {
        n.logf(LOG_ERROR, "LOOKUPD(%s): parsing response - %s", lp, resp)
        lp.Close()
        return
      } else {
        //鉴权成功
        // LOOKUPD(127.0.0.1:4160): peer info {TCPPort:4160 HTTPPort:4161 Version:1.2.1-alpha BroadcastAddress:liangtiandeMacBook-Pro.local}
        n.logf(LOG_INFO, "LOOKUPD(%s): peer info %+v", lp, lp.Info)
        if lp.Info.BroadcastAddress == "" {
          n.logf(LOG_ERROR, "LOOKUPD(%s): no broadcast address", lp)
        }
      }
    }
​
    // build all the commands first so we exit the lock(s) as fast as possible
    var commands []*nsq.Command
    n.RLock()
    for _, topic := range n.topicMap {
      topic.RLock()
      if len(topic.channelMap) == 0 {
        commands = append(commands, nsq.Register(topic.name, ""))
      } else {
        for _, channel := range topic.channelMap {
          commands = append(commands, nsq.Register(channel.topicName, channel.name))
        }
      }
      topic.RUnlock()
    }
    n.RUnlock()
    for _, cmd := range commands {
      n.logf(LOG_INFO, "LOOKUPD(%s): %s", lp, cmd)
      _, err := lp.Command(cmd)
      if err != nil {
        n.logf(LOG_ERROR, "LOOKUPD(%s): %s - %s", lp, cmd, err)
        return
      }
    }
  }
}
```

`connectCallback`函数没有任何逻辑，直接return了一个匿名函数，该匿名函数里首先会组装一个map,把自己的信息写入到内，包括版本，tcp端口，http端口，hostname, 主机名等包装到一个结构体内发送。当`nsqlookupd`接收到信息后，其实不会做任何校验，只是单纯的拿到数据后放入到`nsqlookupd`的全局DB-map中的client内。

```go
// body is a json structure with producer information
  peerInfo := PeerInfo{id: client.RemoteAddr().String()}
  //unmarshal 成功就表示注册成功
  err = json.Unmarshal(body, &peerInfo)
  if err != nil {
    return nil, protocol.NewFatalClientErr(err, "E_BAD_BODY", "IDENTIFY failed to decode JSON body")
  }
​
  peerInfo.RemoteAddress = client.RemoteAddr().String()
​
  // require all fields
  if peerInfo.BroadcastAddress == "" || peerInfo.TCPPort == 0 || peerInfo.HTTPPort == 0 || peerInfo.Version == "" {
    return nil, protocol.NewFatalClientErr(nil, "E_BAD_BODY", "IDENTIFY missing fields")
  }
​
  atomic.StoreInt64(&peerInfo.lastUpdate, time.Now().UnixNano())
​
  p.ctx.nsqlookupd.logf(LOG_INFO, "CLIENT(%s): IDENTIFY Address:%s TCP:%d HTTP:%d Version:%s",
    client, peerInfo.BroadcastAddress, peerInfo.TCPPort, peerInfo.HTTPPort, peerInfo.Version)
​
  //写入到 client DB里
  client.peerInfo = &peerInfo
  if p.ctx.nsqlookupd.DB.AddProducer(Registration{"client", "", ""}, &Producer{peerInfo: client.peerInfo}) {
    p.ctx.nsqlookupd.logf(LOG_INFO, "DB: client(%s) REGISTER category:%s key:%s subkey:%s", client, "client", "", "")
  }
​
  // build a response  返回response
  data := make(map[string]interface{})
  data["tcp_port"] = p.ctx.nsqlookupd.RealTCPAddr().Port
  data["http_port"] = p.ctx.nsqlookupd.RealHTTPAddr().Port
  data["version"] = version.Binary
  hostname, err := os.Hostname()
  if err != nil {
    log.Fatalf("ERROR: unable to get hostname %s", err)
  }
  data["broadcast_address"] = p.ctx.nsqlookupd.opts.BroadcastAddress
  data["hostname"] = hostname
​
  response, err := json.Marshal(data)
  if err != nil {
    p.ctx.nsqlookupd.logf(LOG_ERROR, "marshaling %v", data)
    return []byte("OK"), nil
  }
  return response, nil
```

鉴权成功后，会触发当前nsq所有的topic和channel注册到`nsqlookupd`内，由于我们是新启动的服务所以这一步直接跳过。紧接着connect变量就会设置成false。表示链接成功了。后面的每次for循环基本上都是触发了15秒的ticker做了一次ping操作。nsq 发送ping 到 nsqlookup后，nsqlookupd做的唯一一步就是把lastUpdate更新掉。其他没做任何操作。

```go
//每次ping后，修改lastUpdate 时间
atomic.StoreInt64(&client.peerInfo.lastUpdate, now.UnixNano())
```

### 客户端创建topic

nsq客户端可以使用官方封装的go-nsq包。

```go
go get -u github.com/nsqio/go-nsq
```

我们简单实现一个发送者程序：

```go
addr := "127.0.0.1:4150"
topic := "first_topic"
channel := "first_channel"
defaultConfig := nsq.NewConfig()
//新建生产者
p, err := nsq.NewProducer(addr, defaultConfig)
if err != nil {
panic(err)
}
//创建一个topic
p.Publish(topic, []byte("Hello Pibigstar"))
```

我们可以看到，先实例化了 NewConfig. 它会获得`nsqclient`的Config对象，并且通过Config结构体的默认配置注入配置。设置Config对象里的`initialized`属性为true. 表示初始化成功。这里有个注意点，nsq的golang客户端中，`consumer`实现了从nsqlookupd中动态拉取服务列表，并进行消费，但是`producer`中没有实现这个。所以发送消息需要填写nsq的地址。

接下来调用`NewProducer`方法 实例化一个 `Producer`对象：

```go
func NewProducer(addr string, config *Config) (*Producer, error) {
  //检查配置文件，是否初始化，验证是否成功
  config.assertInitialized()
  err := config.Validate()
  if err != nil {
    return nil, err
  }
  //实例化Producer
  p := &Producer{
    //id 自增1
    id: atomic.AddInt64(&instCount, 1),
​
    addr:   addr,//nsqlookupd address
    config: *config,  //配置文件
​
    logger: log.New(os.Stderr, "", log.Flags()),
    logLvl: LogLevelInfo,
​
    transactionChan: make(chan *ProducerTransaction),
    exitChan:        make(chan int),
    responseChan:    make(chan []byte),
    errorChan:       make(chan []byte),
  }
  return p, nil
}
```

该函数比较简单，首先检查了下配置是否初始化（initialized）。接着对配置项进行 min/max 范围校验。成功后就直接实例化Producer对象，Producer对象会保存config的引用和nsq 的地址信息。

最后我们看下最核心的函数 Publish 基本上所有的逻辑都是在Publish里面实现的。, Publish函数本身没有内容，它直接调用了 `w.sendCommand(Publish(topic, body))` 我们转到 sendCommand 函数看下：

```go
func (w *Producer) sendCommand(cmd *Command) error {
  //提前设置了一个接受返回参数的Chan, 这里有伏笔，埋伏它一手
  doneChan := make(chan *ProducerTransaction)
  //调用了sendCommandAsync 并且把doneChan 传进去了
  err := w.sendCommandAsync(cmd, doneChan, nil)
  if err != nil {
    close(doneChan)
    return err
  }
  //上面函数结束后，在这里苦苦的等待 doneChan的返回值，所以我们可以大胆的推测 sendCommandAsync 方法并不返回真实的值
  t := <-doneChan
  return t.Error
}
```

这个方法里面大家不要漏了 doneChan , nsq通过这个channel实现了一个高效的ioLoop模型。虽然说`sendCommandAsync`函数名里有个async，但是它并不是同步返回的。而是等待 doneChan这个channel 的返回。并且最后返回内部的Error属性。我们继续看下去。

```go
func (w *Producer) sendCommandAsync(cmd *Command, doneChan chan *ProducerTransaction,
  args []interface{}) error {
  // keep track of how many outstanding producers we're dealing with
  // in order to later ensure that we clean them all up...
  atomic.AddInt32(&w.concurrentProducers, 1)
  defer atomic.AddInt32(&w.concurrentProducers, -1)
  if atomic.LoadInt32(&w.state) != StateConnected {
    err := w.connect()
    if err != nil {
      return err
    }
  }
  t := &ProducerTransaction{
    cmd:      cmd,
    doneChan: doneChan,
    Args:     args,
  }
  select {
  case w.transactionChan <- t:
  case <-w.exitChan:
    return ErrStopped
  }
  return nil
}
```

该函数比较简单，判断是否连接，如果没有连接调用 `connect()` 方法，接着包一个 `ProducerTransaction`结构体。记录要发送的信息和刚才传过来的`doneChan`，发送到 `w.transactionChan`内。到这里发送者代码就全部完了。但是这就全部看完了吗。其实我们只是看了冰山一角。接下里我们要看下 connect() 方法。

```go
func (w *Producer) connect() error {
  ...
  w.conn = NewConn(w.addr, &w.config, &producerConnDelegate{w})
  w.conn.SetLogger(logger, logLvl, fmt.Sprintf("%3d (%%s)", w.id))
​
  _, err := w.conn.Connect()
  atomic.StoreInt32(&w.state, StateConnected)
  w.closeChan = make(chan int)
  w.wg.Add(1)
  go w.router()
  return nil
}
```

NewConn传入配置文件，初始化Conn结构体。调用`w.conn.Connect()` 连接。我们先简单看下 `w.conn.Connect()`方法:

```go
func (c *Conn) Connect() (*IdentifyResponse, error) {
  dialer := &net.Dialer{
    LocalAddr: c.config.LocalAddr,
    Timeout:   c.config.DialTimeout,
  }
  //打开tcp端口
  conn, err := dialer.Dial("tcp", c.addr)
  c.conn = conn.(*net.TCPConn)
  c.r = conn
  c.w = conn
  //发送[]byte("  V2")
  _, err = c.Write(MagicV2)
  //身份校验
  resp, err := c.identify()
  if err != nil {
    return nil, err
  }
​
  if resp != nil && resp.AuthRequired {
    if c.config.AuthSecret == "" {
      c.log(LogLevelError, "Auth Required")
      return nil, errors.New("Auth Required")
    }
    err := c.auth(c.config.AuthSecret)
    if err != nil {
      c.log(LogLevelError, "Auth Failed %s", err)
      return nil, err
    }
  }
​
  c.wg.Add(2)
  atomic.StoreInt32(&c.readLoopRunning, 1)
  go c.readLoop()
  go c.writeLoop()
  return resp, nil
}
```

该方法连接到nsq，并进行身份校验。核心是最后几行代码。开了2个协程。`readLoop()` 和 `writeLoop()` 用来接收消息和写入消息。

这里先停顿下，我们先跳回去继续看producer，当它连接上nsq之后，会开个 go w.router() 协程，我们看下内部实现：

```go
func (w *Producer) router() {
  for {
    select {
    case t := <-w.transactionChan:
      w.transactions = append(w.transactions, t)
      err := w.conn.WriteCommand(t.cmd)
      if err != nil {
        w.log(LogLevelError, "(%s) sending command - %s", w.conn.String(), err)
        w.close()
      }
    case data := <-w.responseChan:
      w.popTransaction(FrameTypeResponse, data)
    case data := <-w.errorChan:
      w.popTransaction(FrameTypeError, data)
    case <-w.closeChan:
      goto exit
    case <-w.exitChan:
      goto exit
    }
  }
​
exit:
  w.transactionCleanup()
  w.wg.Done()
  w.log(LogLevelInfo, "exiting router")
}
```

在这个方法里就是监听多个chan，分别是：是否有需要发送的消息，是否有收到的响应，是否有错误，是否有退出消息。刚才我们包装的 `transactionChan` 就是通过这里发出去了。到这里整个发送流程就已经全部完成了。那么我们怎么知道到底是发送成功还是失败了呢。这个时候就回到了刚才我们看到的在`connect`的时候开的2个协程`readLoop()` 和 `writeLoop()` 了。在这之前我们先了解下nsq的三种消息类型：

```go
frame typesconst (  
  FrameTypeResponse int32 = 0   //响应  
  FrameTypeError    int32 = 1   //错误   
  FrameTypeMessage  int32 = 2   //消息
)
```

然后我们看下readLoop

```go
func (c *Conn) readLoop() {
  delegate := &connMessageDelegate{c}
  for { 
  ...
    frameType, data, err := ReadUnpackedResponse(c)
  ...
    switch frameType {
    case FrameTypeResponse:
      c.delegate.OnResponse(c, data)
    case FrameTypeMessage:
      msg, err := DecodeMessage(data)
      if err != nil {
        c.log(LogLevelError, "IO error - %s", err)
        c.delegate.OnIOError(c, err)
        goto exit
      }
      msg.Delegate = delegate
      msg.NSQDAddress = c.String()
​
      atomic.AddInt64(&c.messagesInFlight, 1)
      atomic.StoreInt64(&c.lastMsgTimestamp, time.Now().UnixNano())
​
      c.delegate.OnMessage(c, msg)
    case FrameTypeError:
      c.log(LogLevelError, "protocol error - %s", data)
      c.delegate.OnError(c, data)
    default:
      c.log(LogLevelError, "IO error - %s", err)
      c.delegate.OnIOError(c, fmt.Errorf("unknown frame type %d", frameType))
    }
  }
​
exit:
...
}
```

readLoop 核心的代码接收到消息后判断消息类型，如果是响应`(FrameTypeResponse)`。调用 `c.delegate.OnResponse(c, data)` ,如果是消息`(FrameTypeMessage)`，那么就增加`messageInFlight` 数，并且更新`lastMsgTimestamp` 时间，最后调用 `c.delegate.OnMessage(c, msg)`。 但是 `c.delegate` 这个接口是哪里实现的呢。我们可以看到 producer 在connect的时候传入过：

```go
w.conn = NewConn(w.addr, &w.config, &producerConnDelegate{w})
```

所以最终就是调用producerConnDelegate结构体的 OnResponse 和OnMessage。 转到producerConnDelegate结构体的方法非常简单：

```go
func (d *producerConnDelegate) OnResponse(c *Conn, data []byte)       { d.w.onConnResponse(c, data) }
func (d *producerConnDelegate) OnError(c *Conn, data []byte)          { d.w.onConnError(c, data) }
func (d *producerConnDelegate) OnMessage(c *Conn, m *Message)         {}
```

它只处理了`OnResponse` 和 onError, 针对`OnMessage`不做任何处理。`OnResponse`也很简单。`d.w.onConnResponse(c, data)` 我们转到 `onConnResponse` 发现也是只有一句代码：

```go
func (w *Producer) onConnResponse(c *Conn, data []byte) { w.responseChan <- data }
```

这里就和之前的 producer的router方法对应上了。router接受 `responseChan`或者 errorChan 执行 `popTransaction` 方法。

```go
func (w *Producer) popTransaction(frameType int32, data []byte) {
  t := w.transactions[0]
  w.transactions = w.transactions[1:]
  if frameType == FrameTypeError {
    t.Error = ErrProtocol{string(data)}
  }
  t.finish()
}
```

首先获取第一个transactions中的元素，如果是错误的响应，那么给他的Error上设置错误信息，最后调用finish方法

```go
func (t *ProducerTransaction) finish() {
  if t.doneChan != nil {
    t.doneChan <- t
  }
}
```

这时候发送的就我们刚才创建的doneChan中传入发送结果，那么用户就可以通过doneChan知道消息是否发送成功了。最后我画了一幅简单的图大家可以参考下：

[![流转](https://img1.liangtian.me/myblog/imgs/nsq22.png?x-oss-process=style/small)](https://img1.liangtian.me/myblog/imgs/nsq22.png?x-oss-process=style/small)

# 3. 常见问题

文章中只介绍了nsqd启动，nsqd接收和发送消息，没有具体到其他细节，比如nsqd如何接收客户端确认，收到确认后做些什么。但是，已经涵盖了nsqd中比较重要的部分。接下来会对一些使用中常见的问题做一下分析。

### 3.1 ID not in flight

之前提过，每个channel有两个队列用来接收topic分发的消息，分别是memoryMsgChan和backend。其中memoryMsgChan是一个带缓冲的golang channel，其长度是有限的；backend是一个基于文件的队列。当memoryMsgChan满了之后，消息会被存储入backend中。nsqd首先会从这两个队列中获取消息，给消息设置本次投递的消费端ID，投递的次数，超时时间等信息之后移到InFlight队列中，最后推送给消费端。当接收到消费端确认之后，将消息从InFlight中移除。消费端确认的方式有以下几种：`touch`,`finish`,`requeue`。touch指更新某消息在InFlight队列中的超时时间，nsqd的做法是先将此消息移出InFlight，更新超时时间之后重新放入InFlight。finish指消费端正常接收并处理了消息，告知nsqd可以从InFlight中移除此消息。requeue指消费端接收到此消息，但是可能因为处理失败或者其他什么原因需要再次处理，希望nsqd将此消息从InFlight中重新移到memoryMsgChan或者backend中排队。从InFlight中移除消息的函数如下：
```go
// popInFlightMessage atomically removes a message from the in-flight dictionary
func (c *Channel) popInFlightMessage(clientID int64, id MessageID) (*Message, error) {
	c.inFlightMutex.Lock()
	msg, ok := c.inFlightMessages[id]
	if !ok {
		c.inFlightMutex.Unlock()
		// 需要移除的消息已经不在InFlight中 
		return nil, errors.New("ID not in flight")
	}
	if msg.clientID != clientID {
		c.inFlightMutex.Unlock()
		// 消息在InFlight中，但是当前投递的客户端已经跟上一次不同
		return nil, errors.New("client does not own message")
	}
	delete(c.inFlightMessages, id)
	c.inFlightMutex.Unlock()
	return msg, nil
}

```

当消息已经不在InFlight队列，但是又收到客户端的确认消息时，nsqd会出现ID not in flight报警提示。我们来分析一下在什么场景中可能出现这种情况。假设当前有个消息M被推送给消费端C1，nsqd给M设置推送次数(M.Attempts++)，消费端id(M.clientID=C1.id)，投递时间(M.deliveryTS=now)，超时时间(M.pri=now.Add(timeout),UnixNano())之后将消息移到InFlight。如果因为网络，计算量大等因素，导致C1迟迟没有给nsqd发送确认消息，一直到当前时刻大于M.pri，M被认为超时，由nsqd将消息移出InFlight队列，重新排队。在M重新排队期间，nsqd接收到C1的确认，由于此时M已经不在InFlight中，nsqd给消费端返回ID not in flight。所以我们知道，如果受外因影响严重的应用环境或者本身的计算量就很大，这种情况会被放大叠加，消息一直处于重新排队，重新投递的循环中，最终消息会不断堆积，即使增加消费能力也于事无补。解决方法除了提高程序的效率，应用环境的可靠性外，从nsqd的角度来说，可以通过配置增加每个消息在InFlight队列中的超时时间，或者在程序运行中间对消息进行touch，更新其在InFlight中的超时时间。

### 3.2 client does not own message

这个问题的本质跟ID not in flight一样。场景是这样：nsqd将M推送给C1之后，一直没有收到C1确认，超时时间到了之后将M移出InFlight重新排队。之后M被重新投递给C2，因此被移到InFlight中。此时C1往nsqd发送确认，这时候M确实在InFlight中，但是投递的客户端已经是C2了，所以nsqd给C1返回client does not own message。所以我们知道，如果M正在排队还没投递，出现的提示是ID not in flight，如果已经重新投递则是client does not own message。解决方法跟之前一样，也是增加消息待在InFlight队列的时间。

### 3.3 消费端从nsq获取消息的速度快吗

之前说过，nsq的消息并不是消费端主动获取，而是nsq主动推送。消费端代码需要有自己的一套机制去更新RDY值，告知nsqd最多可以接收多少的消息。以python的客户端代码pynsq为例，nsq.Reader中有一个参数max_in_flight，用于指定nsqd一次推送的消息数量，以便客户端可以一次获取较多消息提高传输效率。如果消费端与nsqd建立了多个连接，所有连接的RDY值之和不能超过max_in_flight。

比如客户端C更新其RDY值为100，并告知nsqd。nsqd负责收集100条消息准备发送至客户端，并且在收集第一条消息的时候就会开启一个定时器。如果在此定时器时间到来之前准备好100条数据，则直接发送出去，如果时间到了还没准备好数据，也会发出去。所以，对于消息生产很快的场景，nsqd收集到固定消息之后就会直接发送，延迟基本可以认为是网络延迟。消息生产较慢的场景，则依赖此定时器的时间，默认值是1s。当然，在客户端代码中增加output_buffer_timeout配置，可以修改此时间。

# Reference
https://liangtian.me/series_courses/