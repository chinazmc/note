## 1、Raft分为哪几个部分？

**主要是分为leader选举、日志复制、日志压缩、成员变更等**。

## 2、Raft中任何节点都可以发起选举吗？

Raft发起选举的情况有如下几种：

-   刚启动时，所有节点都是follower，这个时候发起选举，选出一个leader；
-   当leader挂掉后，**时钟最先跑完的follower发起重新选举操作**，选出一个新的leader。
-   成员变更的时候会发起选举操作。

## 3、Raft中选举中给候选人投票的前提？

**Raft确保新当选的Leader包含所有已提交（集群中大多数成员中已提交）的日志条目**。这个保证是在RequestVoteRPC阶段做的，candidate在发送RequestVoteRPC时，会带上自己的**last log entry的term_id和index**，follower在接收到RequestVoteRPC消息时，**如果发现自己的日志比RPC中的更新，就拒绝投票**。日志比较的原则是，如果本地的最后一条log entry的term id更大，则更新，如果term id一样大，则日志更多的更大(index更大)。

## 4、Raft网络分区下的数据一致性怎么解决？

发生了网络分区或者网络通信故障，**使得Leader不能访问大多数Follwer了，那么Leader只能正常更新它能访问的那些Follower，而大多数的Follower因为没有了Leader，他们重新选出一个Leader**，然后这个 Leader来接受客户端的请求，如果客户端要求其添加新的日志，这个新的Leader会通知大多数Follower。**如果这时网络故障修复 了，那么原先的Leader就变成Follower，在失联阶段这个老Leader的任何更新都不能算commit，都回滚，接受新的Leader的新的更新（递减查询匹配日志）**。

![](https://ask.qcloudimg.com/http-save/yehe-5760343/lwmd8waidw.png?imageView2/2/w/1200)

## 5、Raft数据一致性如何实现？

**主要是通过日志复制实现数据一致性，leader将请求指令作为一条新的日志条目添加到日志中，然后发起RPC 给所有的follower，进行日志复制，进而同步数据**。

## 6、Raft的日志有什么特点？

**日志由有序编号（log index）的日志条目组成，每个日志条目包含它被创建时的任期号（term）和用于状态机执行的命令**。

## 7、Raft和Paxos的区别和优缺点？

-   Raft的leader有限制，**拥有最新日志的节点才能成为leader**，multi-paxos中对成为Leader的限制比较低，**任何节点都可以成为leader**。
-   **Raft中Leader在每一个任期都有Term**号。

## 8、Raft prevote机制？

![](https://ask.qcloudimg.com/http-save/yehe-5760343/xjwqhnajfu.png?imageView2/2/w/1200)

**Prevote（预投票）是一个类似于两阶段提交的协议**，**第一阶段先征求其他节点是否同意选举，如果同意选举则发起真正的选举操作，否则降为Follower角色**。这样就**避免了网络分区节点重新加入集群，触发不必要的选举操作**。

## 9、Raft里面怎么保证数据被commit，leader宕机了会怎样，之前的没提交的数据会怎样？

**leader会通过RPC向follower发出日志复制，等待所有的follower复制完成，这个过程是阻塞的**。

**老的leader里面没提交的数据会回滚，然后同步新leader的数据**。

## 10、Raft日志压缩是怎么实现的？增加或删除节点呢？？

在实际的系统中，**不能让日志无限增长**，否则**系统重启时需要花很长的时间进行回放**，从而影响可用性。**Raft采用对整个系统进行snapshot来解决，snapshot之前的日志都可以丢弃（以前的数据已经落盘了）**。

**snapshot里面主要记录的是日志元数据，即最后一条已提交的 log entry的 log index和term**。

## 11、Raft里面的lease机制是什么，有什么作用？

**租约机制确保了一个时刻最多只有一个leader，避免只使用心跳机制产生双主的问题**。**中心思想是每次租约时长内只有一个节点获得租约、到期后必须重新颁发租约**。

![](https://ask.qcloudimg.com/http-save/yehe-5760343/wm811fgc2r.png?imageView2/2/w/1200)

在这里插入图片描述

## 12、Raft协议的leader选举，正常情况下，网络抖动造成follower发起leader选举，且该follower的Term比现有leader高，集群中所有结点的日志信息当前一致，这种情况下会选举成功吗？

**参考网络分区的情况**。

### **Q: 节点故障后为何不重置为Candidate，而是Follower?**

正常来说，一个节点在挂了之后重新加入集群，是以Follower的身份，需要等待一定时间才能开始选举。但是为什么不设计成挂了之后以Candidate的身份，一加入就开始选举投票呢？从正确性的角度上来说，这不会影响Raft的正确性：如果新加入的节点立刻开始选举，选举成功即为Leader，失败即为Follower。同时假设Leader挂了，那么这个时候相当于群龙无首，就算立刻把Leader拉起来，也要等待一定的选举时间才能选举出新Leader，开始处理业务。如果拉起来就能立刻选举的话，能有效减少等待选举的时间。既然这种设计不会带来错误，同时又在某些Corner Case上表现不错，为什么工业上不这么设计呢？

A: 主要是从工业实际运行的稳定性上考虑的。虽然分布式系统，crash是常态，但是正常运行的时间始终是占大多数的。如果挂了的节点拉起来之后重置为Candidate，那么会比原来多出很多Leader无缘无故切换的情况，扰乱系统稳定性。因此不推荐这种做法。（小声：那对于原来是Leader的节点，crash之后重置为Candidate立刻开始投票可不可以呢？）

  

### **Q：多个节点同时选举，能否成功？ 选举时不首先投给自己呢？**

在Raft中，虽然采取了随机时间选举，但是假设一下现在多个节点同时开始选举，那么选举是不可能成功的，因为每个节点都会首先把票投给了自己。那么我们突发奇想一下，假设所有节点发起投票时，不首先把票投给自己呢？这样的话，还是有可能选举失败（三节点：A投给B，B投给C，C投给A，每个节点各一票）。同时在其他场景中，如果出现网络分区（分为多数派和少数派），假设多数派节点数为３，少数派为２。那么理论上来说多数派节点可以选举出Leader对外提供服务，但如果选举的时候不投给自己，可能就因为这一票永远无法选举出Leader，因此这种做法不可选。

  

### **Q：Ｍaster在准备Commit某条Log的时候挂了，Raft如何保证一致性？**

因为Master在Commit某条Log的时候，这个Log已经被复制到集群中半数以上节点了。根据Raft的选举性质，那么之后再选举出来的Leader，一定会包含这条Log。对于这条Log，是上一个Term的，那么当前Leader不能直接commit，而是要发送AppendEntries RPC来commit这条log。具体原因见论文图８。

  

### **Q: 假设某个Follower节点出现网络分区，由于接收不到Leader的心跳包，所以会不断选举，Term会一直增加。加入原集群后会把原Leader降级为Follower，导致重新选举。但实际上它并不能成为Leader(没有最新日志)，造成disruption。如何解决？**

Raft作者博士论文《CONSENSUS: BRIDGING THEORY AND PRACTICE》的第9.6节 "Preventing disruptions when a server rejoins the cluster"提到了PreVote算法的大概实现思路。

主要就是Prevote的思想。把选举也拆成一个两阶段提交的方式，先进行一轮prevote，这一轮prevote并不会更改自己的Term＋１，但是Vote请求里的Term是＋１的。如果收到大多数的赞成票，那么发起真正选举。否则继续等待下一轮。（好奇的是Etcd的Raft应该对Prevote票和Vote票做区分？如果Prevote投过这个Term就不能再投了，那就非常奇怪了。所以应该是区别对待的）

  

### **Q: 假设Ａ, B, C三节点，A为Leader。A和B之间出现分区，但是A, C和B, C之间连接不受影响，会导致Leader频繁切换，有什么方法优化？**

A, B, C三节点，只有A和B之间的网络不通，而C是可以和A, B 连通的，有什么办法优化？由于Leader A和B无法连通，B会选举成为新Leader (C 会投给它)。同时Leader A的心跳包会被拒绝，降级为Follower(因为Term的原因)。过一段时间后，A也会经历一样的过程，选举成为Leader，B降级为Follower，如此反复。

应该办法是在系统中去detect这种网络分区，如果发现了的话，要及时修复连接。以及C其实可以观测到频繁的Leader切换，从而报警，或者自己选举成为新Leader。好奇其他算法是怎么做的。

raft能保证一个请求，成功了就是成功了。但是raft怎么保证失败？如果Leader刚刚把请求X分发到A节点，Leader就挂了（客户端那边发现自己请求失败了），A成为新的Leader，然后一直正常工作，并分发请求X。此时客户端再发送请求Y（请求内容和X一摸一样）。那么客户端视角，我一次请求失败，一次请求成功，按理说只会执行一次操作。但是事实上却执行了两次操作。在例如转账这种场景下，这种事情怎么避免？

例如，如果领导者在提交日志条目后但在响应客户端之前崩溃，客户端将用一个新的领导者重试该命令，导致它被第二次执行。解决方案是让客户端为每个命令分配唯一的序列号。然后，状态机跟踪为每个客户处理的最新序列号，以及相关的响应。如果它收到一个序列号已经被执行的命令，它会立即响应，而不重新执行该请求

-   问题1：新加进来的节点开始时没有存储任何日志条目，当它们加入到集群中，需要一段时间来更新日志才能赶上其他节点，这段时间内它们无法提交新的日志条目。为了避免上述问题而造成的系统短时间的不可用，raft在配置变更前引入了一个额外的阶段，在该阶段，新的节点以没有投票权身份加入到集群中，leader也日志给它们，但考虑过半的时候不用考虑它们，一旦新节点追赶上了集群中的其他机器，配置变更可以按照上面描述的方式进行。
-   问题2：集群的leader可能不在新配置Cnew中，这种情况，leader一旦提交了Cnew日志条目就会退位到followr状态。这意味着有一个时间段（leader提交Cnew期间），leader管理的是一个不包括字节的集群，它日志但不把自己算在过半里面。leader转换发生在Cnew被提交的时候，这个时候是新配置可以独立做决定的最早时刻，在此之前只能从Cold中选出leader.
-   问题3：被移除的节点即不在Cnew配置中的节点可能会扰乱集群。这些节点将不会再接收到心跳，当选举超时时候，它们会进行新的选举过程。它们会发送带有新任期号的requestVote RPC,这会导致当前的leader回到follower状态。新的leader最终会被选出来，但被移除的节点会再次超时，导致系统可用性很差。处理方法是，当服务器认为当前leader存在时，会忽略requestVote PRC。特别当服务器在最小选举超时时间内收到一个requestVote RPC，它不会更新任期号或投票。这不会影响正常的选举，因为每个节点在开始一次选举之前，至少等待最小选举超时时间。

# raft 算法中的集群成员变更问题

在上一篇文章[《分布式一致性算法之 raft 图解》](https://juejin.cn/post/6973502080350683149 "https://juejin.cn/post/6973502080350683149")中我们讲解了 raft 算法的领导者选举以及日志复制的问题，同时通过一个具体实例讲解了 raft 是如何通过“一切以领导者”为准来解决日志不一致的情况的。同时在文章结尾笔者也讲到 raft 算法包含的内容远不止这么多，甚至上述的一些问题都是 raft 中的 base（基础）问题。接下来，我们将会用一篇文章来继续讲解 raft 需要解决的另外一个难题 -- 成员变更问题。

## 为什么会有成员变更

首先我们要有一个常识：一台服务器不可能永远无故障地运行下去，即使服务器不会发生问题，那么也许是因为网络问题、亦或是集群本身的 bug，都有可能导致某个节点不可用。在这个时候，我们往往会选择新增一个或多个节点来替换掉不可用的节点，从而产生了成员变更。同时，一家公司的发展不可能永远不变，我们的业务规模也就不可能永远不变，这个时候，集群规模的变更也就是顺理成章的事情了。

也许有的同学会说，想要新增节点的话，那就直接新增好了，反正 raft 会通过日志一致性算法将新节点不存在的日志复制过去。但是事实果真如此吗？

## 成员变更会产生什么问题

不妨我们假设原始集群中有 3 个节点 A、B、C，它们当前的日志状态如下：

![mc0.jpg](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a6c27df3d654400886122ae30d24453b~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

我们要意识到，基于日志复制“大多数”原则，上述的日志情况是完全有可能存在的，因为对于三节点的集群来说，最新的那条日志项已经被成功复制到了大多数集群，那么它便可用被领导者应用到状态机。

假设我们现在想要往集群中新增两个节点 D、E，大家可用想一下，假设我们直接将两个节点添加到集群中，会产生什么问题？

我们都了解到 raft 算法具有领导者唯一性，这是实现数据一致性的首要保证，一旦集群中有两个领导者节点，那么将会产生及其严重的数据不一致，这显然对于保证严格一致性的 raft 算法是无法接受的。而像上述那样直接添加两个节点，由于每个节点新旧配置更新的时间不同，导致在某一时刻可能存在新旧配置两个大多数情况的存在，便很有可能使集群发生“脑裂”，也就是出现两个领导者。比如在进行成员变更的时候，节点 A 和 B、 C 产生了网络分区，如果此时新增的节点 D、E 和 A 在同一个分区，那么对于新配置中的领导者 A 而言，集群中依然有大多数节点在正常运行，它依然是领导者，而对于维护了旧配置的节点 B、C 来说，由于接收不到领导者的心跳请求，那么通过领导者选举算法，节点 B 会变成此分区的领导者，此时，整个集群中便产生了两个领导者，分别是节点 A 和节点 B：

![脑裂.jpg](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c1eed5eb34634b4e867c4786083813e0~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

当然，以上是针对往集群中添加节点的情况，其实从集群中同时移除多个节点也同样会发生上述问题，这个大家不妨按照相同的思路来自己推理一下（提示：从 5 节点集群中移除两个日志相对完整的节点）。

因此通过上面的分析我们可以发现，当新增多个新的节点的时候，我们不能直接将所有新节点添加到集群中。

那么有没有什么办法可以解决上述问题呢？

## 解决成员变更问题

raft 解决成员变更问题主要有两个方法，分别是联合共识算法和单节点变更算法。

### 方法一 -- 联合共识（Joint Consensus）

联合共识算法是 raft 作者首先提出的一种方法。此方法允许一次性向集群中插入多个节点而不会出现脑裂等 (safety) 问题，并且整个集群在配置转换的过程中依然能够接收用户请求，从而实现配置切换对集群调用方无感知。

在联合共识阶段，集群工作需要考虑到新旧两种配置，具有如下约束或者约定:

- 约定一：日志会被复制到新老配置的所有节点；
- 约定二：新老配置的节点都可以被选举为领导者；
- 约定三：选举和日志复制阶段需要在新老配置上面都超多半数才能被提交生效。

下图是官方论文中出现的图，代表联合共识阶段配置变更的时间线：

![joint.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/66e81bdc3e274447983432514e710df1~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

其中，虚线代表已创建但是未提交的配置项，实线代表最新的已提交的配置项。领导人会首先创建 Cold_new 日志项，并复制到新旧配置中的大多数，此时所有的日志项都需要被联合共识。然后创建 Cnew 日志项，并复制到 Cnew (新配置) 中的大多数。这样旧配置和新配置就不会存在可以同时做出决策的时间点。

假设之前一共有三个节点 A、B、C，现在要往集群中添加两个节点 D、E。我们一起来看一下联合共识的流程。

1. 首先领导者向集群中发送一条配置变更日志，告诉其他节点集群中要新增两个节点。根据约定一，日志会被复制到新老配置的所有节点。需要注意的是，raft 算法在日志复制过程中会保证日志完整性，所以新节点在复制 Cold_new 之前会同步领导者的数据。

![m1_0.jpg](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cb144e0c8ef14326b5b86b65268b0324~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

2. 根据约定三，这条日志需要在新老配置中都达到超过半数节点复制成功，它才能被成功应用。也就是说需要老配置 (A,B,C) 和新配置 (A,B,C,D,E) 都达到半数以上，如下图所示，老配置的大多数 (A,B) 和新配置的大多数 (A,B,D) 已经复制成功：

![m1_1.jpg](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7b2623a7a970425a997b1ab4393697cf~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

3. Cold_new 被应用成功之后，领导者会生成一条新的日志 Cnew 复制到集群，告诉集群现在可用应用新的配置项了。跟随者节点收到这条日志之后，会马上切换到新集群配置（这里同样要注意 raft 的一致性检查机制，会保证在复制 Cnew 之前，Cold_new 会首先被复制），由于此时领导者已经处于新配置状态，因此它只要被复制到新配置的大多数节点即可提交，而不需要联合共识了。

![m1_2.jpg](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ee35f6754117411a81766dca9ca9e323~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

在联合共识阶段，整个集群依然能够对外提供服务，只是日志需要像上述流程那样被新老配置同时应用才行。对于新的节点 D、E，raft 会通过日志一致性检查来复制领导者的所有日志项，从而保证它们同样能够保持日志完整性。

以上就是往集群中新增节点的流程，我们来看一下按照上述流程为什么不会产生脑裂。我们依然假设集群产生了 2，3 分区，分别是 (A,B) 和 (C,D,E)：

- 如果此时领导者 A 还没有复制任何一条 Cold_new，那么领导者 A 不会应用 Cold_new，(A,B) 分区依然是旧配置，A 是领导者；而 (C,D,E) 分区由于 C 会接收心跳超时而发起选举，但是它不会感知到 D、E 的存在，无法获取到大多数节点的投票。因此两个分区只会有一个领导者，符合预期。
- 如果领导者复制了 Cold_new 之后发生了网络分区。如果 Cold_new 没有被大多数节点确认，那么领导者 A 无法应用 Cold_new，(A,b) 依然处于旧配置状态，对外提供服务，此时 (C,D,E) 分区无论谁发起领导者选举，都无法获取到大多数选票(旧配置状态的 C)或者被联合共识 (新配置状态的 D)。如果 Cold_new 已经被大多数节点复制，那么领导者 A 会应用此日志，然后复制 Cnew，但是 Cnew 日志无法被联合共识，领导者 A 后续不会提交任何日志（在一些实现中会自动退位为跟随者）；对于分区 (C,D,E)，C 无法获取到大多数选票，如果它没有复制 Cold_new，也不会投票给 D / E，此分区也无法选举出领导者。符合预期。
- 如果在 Cnew 阶段产生了分区，由于 raft 算法具有持久性，已经提交的 Cold_new 会永久生效，此时 (A,B) 分区无法获取大多数选票，不会选出新领导者，也就不可能发生脑裂，符合预期。

看到这里，相信很多读者都是云里雾里的（包括笔者本人），因为这种方式确实具有很大的理解难度，因此作者提出了第二种方法。

### 方法二 -- 单节点成员变更

单节点变更是 raft 作者提供的第二种方法，顾名思义，每次变更的时候都只新增一个节点。也就是说假设我们想新增两个节点 D、E，我们可用先将节点 D 添加到集群中，然后再添加节点 E。与方法一不同，单节点变更的方式在集群配置变更的过程中是不能对外提供工作的。

接下来让我们来具体看一下单节点变更是如何解决脑裂的问题的。

还是拿节点 A、B、C 为例，现在我们想要往集群中添加节点 D、E，与上述方法不同的是，我们这次选择一个一个地执行节点变更。

假设我们先添加节点 D，将集群变成 4 节点集群：

单节点变更的具体流程是：

- 节点 D 向领导者申请加入集群；
- 领导者 A 向新节点 D 同步数据；
- 领导者 A 将新配置 [A、B、C、D] 作为一个日志项，复制到配置中的所有节点，然后应用新的配置项（这里需要注意的是，每个节点接收到新的配置项肯定是有时差的）；
- 如果新的日志项应用成功（被大多数节点复制成功），那么新节点添加成功。

然后针对节点 E，同样走一次上述流程，完成新增节点。

我们不妨来看一下只添加一个节点为什么不会出现脑裂问题。假设新增节点 D 的过程中产生了分区，我们以以下几种分区情况分别讨论一下：

- 网络分区成 (A,B) 和 (C,D) 两部分，如果节点 A、B 此时维护的还是旧的配置，那么 A 依旧是领导者，节点 C 因为分区开始发起领导者选举，此时如果 C 维护的是旧的配置 (A,B,C)，那么此时它不会得到节点 D 的投票，无法成为领导者；节点 C 如果维护的是新的配置，那么分区中节点个数不超过一半，它依然不会变成领导者，符合预期。当分区消失之后，节点 D 由于发现自己还没有完成入集群操作，从而会继续向领导者发起“进入集群申请”，领导者便会继续走一遍上述流程。

![sc2.jpg](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1bb050ed6d1d4de0b91c8dcf9364eeb6~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

- 网络分区成 (A,B) 和 (C,D) 两部分，如果节点 A、B 此时维护的是新配置，那么 (A,B) 分区由于无法获取到大多数选票而无法选出领导者，(C,D) 分区同情况 1，这样的话两个集群都不会成功选举出新的领导者。此时便可能需要人工进行介入，但是集群中依然不会存在两个领导者。

![sc3.jpg](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e39291fc89694eb7b30f3074511cebd4~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

- A、B、C 在同一个分区，剩余的一个节点 D 在另外一个分区，此时只有包含三节点的分区能选举出领导者，正常处理请求，符合预期。当分区消失了之后，节点 D 会正常接收自己缺失的日日志项，从而更新自己维护的配置信息（在这里我们可用发现，节点 D 虽然已经在集群中，但是在它自己看来，自己确是被孤立的节点）。同样的当分区消失之后，节点 D 会再次申请“进群”。

![sc1.jpg](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b8facd1aab3e4a30ac5d954f204f2fd6~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

对于新增节点 E 或者从集群中删除一个节点的情况也是一样的，大家可以自己来推导一下看看。

其实单节点变更体现了一个最基本的数学知识：

(�2+1)+(�+12+1)>�+1(2n​+1)+(2n+1​+1)>n+1

就是一个数字肯定小于两个超过数字一半的数字之和。对于 raft 集群来说，旧配置的大多数与新配置的大多数之和一定大于新配置的节点个数。由于 raft 算法的领导者选举需要获得超过大多数选票，而当我们只新增一个节点的时候，旧配置的大多数和新配置的大多数不可能同时存在（否则必定有至少一个节点同时属于两个分区，这显然是不可能存在的），因此两个分区只有一个分区可能选举出领导者。正如下图所示（来自[《CONSENSUS: BRIDGING THEORY AND PRACTICE》](https://link.juejin.cn/?target=http%3A%2F%2Fwcl.cs.rpi.edu%2Fpilots%2Flibrary%2Fpapers%2Fconsensus%2FRAFTOngaroPhD.pdf "http://wcl.cs.rpi.edu/pilots/library/papers/consensus/RAFTOngaroPhD.pdf")）：

![分区图.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bb280903ed8948e4a3d63d171fbea00f~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

当然有的同学可能会问了：还有一种情况是一个分区里面既包含旧配置也包含新配置，这个时候会有什么现象发生呢，会不会存在两个分区选出了两个维护旧配置的领导者节点呢？答案是不会的。原因就是维护旧配置的节点不会向新节点发起投票请求，也不会投票给新的节点，因为它并不知道集群中有新的节点；同时按照“日志完整性原则”，维护了新配置的节点（日志更完整）也不会投票给维护旧配置的节点。所以这种情况反而降低了分区中旧配置节点获取选票的可能性。大家不妨自己按照上面的方式来推导一下看看。

## 结语

从原理和实现上来看，单节点变更的复杂度要比联合共识算法低的多，因此单节点变更的方式也就成了很多 raft 实现的首选算法。

到此为止，我们通过两篇文章以及具体示例大致了解了 raft 算法的领导者选举、日志复制以及节点变更等问题，这是 raft 最基础也是最重要的几个点。其实我们了解 raft 算法并不是为了了解其实现细节，而是管中窥豹，通过 raft 算法本身，来提炼出它所蕴含的丰富的理论基础，这些理论基础才是对我们大有裨益的，因为我们在工作过程中，自己去实现一个 raft 算法基本是不可能的，反而“如何将数据变更同步到集群其他节点”诸如此类的问题我们倒是有可能会遇到。

当然由于笔者能力有限，本文只是笔者在读了论文和其他一些资料之后的总结，有些地方并不是很全面，如果有任何纰漏，欢迎大家评论指正。


# raft 读请求可以读follower吗
会,但是实际上基于 raft 的 etcd 实现了 Linearizable Read 来解决这个问题

读请求到了 follower 后,follower会去向 leader 请求 readindex(也就是当时 leader 的 commitindex), leader 在确认自己还是 leader 之后,就会吧 readindex 发给 follower,follower 会对比自己的 commitindex 和 readindex,只有commitindex 大于等于 readindex 之后,才能读取数据返回.

# raft协议，leader在commit了一条日志后，立刻挂了，那其他节点如何处理这条日志？
  **先明确一点：Leader宕机后，Raft选举算法会保证下一个leader一定包含所有已经提交的日志，除非选举不出leader(大多数节点都宕机的情况)。**

  

"假若leader在commit完一条日志后马上挂了，那其他节点肯定不知道这条日志已经被commit，那后续新的leader也不会把这条日志放进commited log中去。"

你这个理解是不对的。一条log是不是commit是看集群中是不是大部分节点是不是含有这条log，commit index 只是一个位置，标记了该位置之前的log是已经提交了；不存在独立的commit log entries和uncommitted entries,全部都在log[]里面。

leader commit这条log说明这条log已经被复制到了大多数节点上，**Raft这里的的正确性是由选举保证的**，后续选举出来的leader必须包含这条log，能做到应该不难理解正确性了：新leader会把log复制到其到节点上，然后leader会commit，follower commit。

选举是如何保证Raft正确性？

Raft要求选举出来leader的log要更 up-to-date， 其实直觉上也可以理解，含有这条log的节点相对于不含有这条log的节点肯定是更up-to-date。

up-to-date的意思是先判断最后一条log 的term哪个大，term不一样，term大的更up-to-date；term一样大的，log更长的more up-to-date。注意，term是指最后一条log的term，不是currentTerm，因为脑裂的那部分节点可以一直递增自己的term发起选举。

不含有这条log的节点不可能成为leader，它的log最终会被新选举出来的leader更新，最终会成含有这条log。

# raft如何避免惊群效应
使用PreVote

