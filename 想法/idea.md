#idea

web3经历在所有大厂hr秒挂

feed流


秒杀就是单独服务器，缓存预热，用户根据队列来立即拒绝，url动态生成，redis集群化防止宕机，接口根据用户id限流，
下单流程走异步逻辑，全局队列，消息表全局事务。
（redis预减库存这个操作，其实可以内化为每个商品都有一个最大容量为库存的redis队列，每个用户进来就插入到这个商品redis队列中，达到最大容量就拒绝掉其他用户了，只有队列中的用户才能下单这个商品，库存扣减就不需要走redis扣减了）
（防止超卖的话，可以有可用库存，警戒库存，实际库存，下单扣除可用库存，付款就扣除实际库存，所以双重校验来防止超卖。）

秒杀活动的订单按钮需要点击一次就置为灰色来幂等
用答题的方法避免作弊抢购以及延缓下单


===========================================================================
订单幂等性的控制
第一个方面，用户限流，一秒两秒限制用户一次
第二个方面，sql方面要乐观锁控制，状态递增，唯一键，sql加锁
第三个方面，业务分布式锁，比如preOrderId,payOrderId (一方面是分布式锁，一方面是redis 全局set)
用户绑定userid-> preOrderId preOrderId -> payOrderId (多页面打开订单，根据用户id就可以找到之前的记录)


==========================================================================
第三题这种问题其实挺好的 ，说说我的想法。
从 feed 流方向来考虑设计方案，一般而言会有两种模式：
1. 推模式，容易写扩散
2. 拉模式，容易读扩散

微信朋友是双向关系且数量有上限不会无限写扩散可以考虑使用推模式，朋友圈是时间流倒叙。

下面分析一下信息流所有者的操作场景：
1.写，每次发信息流广播给所有的朋友，每个朋友会有一个自己的 message box，读取朋友圈只需查询自己的 messageBox
2.删除，删除信息流即可，messageBox 不用删除，因为查询时无法命中
3.更新，同删除
4.拉黑，查询信息流时校验关系过滤即可
5.查询，结合旁路缓存策略提高性能
6.查询自己的朋友圈，直接查询自己的 message box 即可，详情有缓存机制。

还有一些细节，比如点赞、评论采用常规思路就行了。
----------------------------------------------------------------------
没做过这么大量级的，瞎想了想

1.先定义下要持久化的表：朋友圈：moment,用户的跟随者：user_follower,用户跟随了谁：user_followee
用户：user （无关，外部服务）

2.moment(user_id,data...)表
moment 表 10W QPS 写，一天按 10W 秒算，每天新增 100 亿，每年新增 4W 亿。这个量级需要同时做 sharding 和冷热库了。然后热库存最近一年的，剩下全同步给冷库，应该是够用的。
热库：一年需要存 4W 亿，按单表 1KW 算（其实 SSD 了可以多点），需要 40W 个表，2^19=524288 。按照实体机每台 32 个库，每个库 32 个表分，需要 512 台 mysql 实体机，还可以。
冷库：复制一个同规模的集群，随时同步超过一年的数据。正常业务的冷查询不会很多，做好冷库的防刷是另外一个话题。这边要计算下磁盘够不够用，按照一条 moment 2KB 来算，冷库设计存 10 年，需要存 40W 亿（ 40T ） * 2KB = 80PB，平均每台 160TB，现在密集写的服务器等级的 SSD 主要是 3.84TB 的？ girigiri 够。热库除以 10，16TB 一台物理机，肯定够用了。
sharding 维度：按按发布用户 id 吧，my_moment_ids 的查询直接命中了。

3.follower(user_id,follower_id) & followee(user_id,followee_id)表
user_follower 和 user_followee 是等价量级的，可以认为都是热数据。这个增量还比较可控，每人 400 个好友，按 1W 亿规模计算，上面那个方案够用。两个库分别按 user_id 做 sharding，一个对应我关注的用户列表，一个对应关注我的用户列表，写关系时候同步写 2 个表，主要是方便查方便写缓存。

4.发布&查询朋友圈
增加缓存：moment 实体，user_follower_ids，user_followee_ids 的缓存，我发的朋友圈的 id 索引 my_moment_ids(user_id->[{my_moment_id,time},...])，重点是我看的朋友圈的 id 关系索引 mix_moment_ids(user_id->[{followee_moment_id,time},...])：
全量写到 my_moment_idx 缓存，好办。
读 moment 实体，这块甚至能做到 99%命中缓存。

维护 mix_moment_ids：
全量写：如果 mix_moment_ids 要全量写全量存，量级是 moment 表量级的 400 倍，每年要新增 1600 万亿（ 1.6P ）条数据，按上面的计算，就算放宽 sharding 到单表 1 亿，也需要一个上万台的 mysql 集群，估计 GG 。全量写扩散到 redis 不丢弃，每条关系按 10 字节算，一年 16PB，集群内存估计一个月也存不下，GG 。

部分写：
mix_moment_ids 只写前 100 条的 ID，按 100 亿(10G)用户每条 10 字节计算，10TB 数据，redis 集群内存富裕。总之这里策略合适抗住 95%的 mix_moment_idx 查询，剩下 5W 读 qps 需要计算。命中不够就多缓存点，100TB 的 redis 集群还是有的。全量写到 mix_moment_ids 前 100 条的话，写操作先需要读 my_follower_ids，再写到对应人的 mix_moment_ids，集群需要 4KW 的写 QPS，理论上可以做得到。。。吧？不行就只写热点用户。

剩余 5Wqps 变成了读扩散：这里包含没命中缓存的和冷用户，需要取 user_followee_ids，再取 400 个我关注的人的 my_moment_ids 按时间聚合，这样变成 2KW 读 QPS，95%打到 redis 集群上，剩下 100Wqps 命中 mysql 集群的 moment 表，全热库的话每台 2KQPS，够了。这块应该有很大的做各种 trick 优化的空间。

纸上谈兵的话就是这样了，欢迎做过这个量级的来指点迷津
---
感谢头条群友逆天之剑半夜讨论启发

#### HTTP 

- **SUCCESS:** 状态码 200 StatusOK
- **FAILURE:** 状态码 409 StatusConflict
- **ONGOING:** 状态码 425 StatusTooEarly
- **DB存储引擎：** 采用update ... set owner=..., time=... where ... 这种方式，因为update的原子性，能够保证一条记录只被一个实例更新


saga只能用表的话就相当于异常表了。

dtm默认情况下，重试策略是指数退避算法，可以避免出现故障时，过多的重试导致负载过高。但是这里订票结果不应当采用指数退避算法重试，否则最终用户不能及时收到通知。因此在bookTicket中，返回结果ONGOING，当dtm收到这个结果时，会采用固定间隔重试，这样能及时通知到用户。

#### 部分第三方操作无法回滚 

例如一个订单中的发货，一旦给出了发货指令，那么涉及线下相关操作，那么很难直接回滚。对于涉及这类情况的saga如何处理呢？

我们把一个事务中的操作分为可回滚的操作，以及不可回滚的操作。那么把可回滚的操作放到前面，把不可回滚的操作放在后面执行，那么就可以解决这类问题

```
		saga := dtmcli.NewSaga(DtmServer, shortuuid.New()).
			Add(Busi+"/CanRollback1", Busi+"/CanRollback1Revert", req).
			Add(Busi+"/CanRollback2", Busi+"/CanRollback2Revert", req).
			Add(Busi+"/UnRollback1", "", req).
			Add(Busi+"/UnRollback2", "", req).
			EnableConcurrent().
			AddBranchOrder(2, []int{0, 1}). // 指定step 2，需要在0，1完成后执行
			AddBranchOrder(3, []int{0, 1}) // 指定step 3，需要在0，1完成后执行
```

示例中的代码，指定Step 2，3 中的 UnRollback 操作，必须在Step 0，1 完成后执行。

对于不可回滚的操作，DTM的设计建议是，不可回滚的操作在业务上也不允许返回失败。可以这么思考，如果发货的操作返回了失败，那么这个失败的含义是不够清晰的，调用方不知道这个失败是修改了部分数据的失败，还是修改数据前的业务校验失败，因为这个操作不可回滚，所以调用方收到这个失败，是不知道如何正确处理这个错误的。

另外当你的一个全局事务中，如果出现了两个既不可回滚的又可能返回失败的操作，那么到了实际运行中，一个执行成功，一个执行失败，此时执行成功的那个事务无法回滚，那么这个事务的一致性就不可能保证了。

对于发货操作，如果可能在校验数据上可能发生失败，那么将发货操作拆分为发货校验、发货两个服务则会清晰很多，发货校验可回滚，发货不可回滚同时也不会失败。


#### 超时回滚[#](https://dtm.pub/practice/saga.html#%E8%B6%85%E6%97%B6%E5%9B%9E%E6%BB%9A)

saga属于长事务，因此持续的时间跨度很大，可能是100ms到1天，因此saga没有默认的超时时间。

dtm支持saga事务单独指定超时时间，到了超时时间，全局事务就会回滚。

```
	saga.TimeoutToFail = 1800
```

在saga事务中，设置超时时间一定要注意，这类事务里不能够包含无法回滚的事务分支，因为超时回滚时，已执行的无法回滚的分支，数据就是错的。


现在比较组内的业务使用grpc，如果是多组要调用的话就还是用http让人方便些。

NSQ 使用规范

nsq的消息没有严格的顺序, 消息可以在任何时间以任何顺序进入队列, 如果对消息有顺序要求,避免使用nsq, 或者业务自己控制.
nsq消息可能重复投递, 遇到超时,心跳测试失败等错误情况, nsq会重发消息, 而且可能发送到另一个客户端. 所以业务需要自己确保nsq业务幂等. 最好的每个nsq分配全局唯一id, 确保幂等性.
预生产和正式环境的nsq因为共用一套服务, 所以topic名称最好做区分, 避免线上消息被预生产环境接收处理.


# 为什么我们nsq不用推荐拓扑
因为我们是交给运维去管理nsq集群的，而且跟服务放到一个pod中，容易加大pod的不稳定，我们习惯业务和组件分离，不需要关心这些东西。

轮询算法后面要弄得熟一些。
目前nsq的producer用的是https://github.com/bbdshow/balancer-nsqd-producer 提供的随机方案。连接多个nsqd，然后选一个来发送。


es可以用hbase的counter来做乐观锁更新，redis可以吗？
我们自己指定业务id是为了想要相同的内容要只有一份文档

index/create
　　第一步判断是确定插入的文档是否指定id，如果没有指定id，系统会默认生成一个唯一id。

　　这种情况下，不管index还是create会直接add文档。如果用户指定了id，那么就会走update，update成本比add要高。

　　第二步判断，会检查版本号是否冲突，只有正确的版本号才会执行插入，否则会插入失败。

　　通过源码分析可以了解到，获取版本号的这个过程，是通过id来get文档，但无论是index还是create都不会get整个doc的全部内容，只是get出了版号。这也从一定程度上减少了系统开销。

总结：
　　<1. 自动生成的 id，则 index/create 都采用 add 方式

　　<2. 业务指定的 id，则 index/create 都采用 update 方式

　　<3. 检查版本号是否冲突

update
　　由于 Lucene 中的 update 其实就是覆盖替换，并不支持针对特定 Field 进行修改，Elasticsearch 中的 update 为了实现针对特定字段修改，在 Lucene 的基础上做了一些改动。

　　每次 update 都会调用 InternalEngine 中的 get 方法，来获取整个文档信息，从而实现针对特定字段进行修改，这也就导致了每次更新要获取一遍原始文档，性能上会有很大影响。

　　所以根据使用场景，有时候使用 index 会比 update 好很多。

index和create区别
　　index 和 create 都会检查 _version 版本号。

　　index 插入时分两种情况：

　　　　<1. 没有指定 _version，那对于已有的doc，_version会递增，并对文档覆盖。

　　　　<2. 指定_version，如果与已有的文档 _version 不相等，则插入失败；如果相等则覆盖，_version递增。

　　create 插入时对于已有的文档，不会创建新文档，即插入失败，会抛出一个已经存在的异常。

用途解释：
　　在批量请求的时候最好使用 create 方式进行导入。假如你批量导入一个大小为500MB 的文件，中途突然网络中断，可能其中有5万条数据已经导入，那么第二次尝试导入的时候，如果选用 index 方式，那么前5万条数据又会重复导入，增加了很多额外的开销，如果是 create 的话，elasticsearch 针对 bulk 操作机制是忽略已经存在的（当然在 bulk 完成后会返回哪些数据是重复的），这样就不会重复被导入了


必须对所有消费msg的地方都做监控和降级逻辑！！！！！！！！！对订单库需要做归档来保证活跃订单查询和更新速度快。关键链路是不是要用重试表来保证准确处理。

一般es写入优化就是         "refresh_interval": "30s",
         "index.translog.durability": "async",
        "index.translog.sync_interval": "30s"


 为什么要用hbase，而不是直接查数据库。hbase中的数据是高度业务化的数据，都是从数据库各个表中聚合起来的，数据库的数据是最底层的用于构成其他数据的基础，是应该要保留的。



 可能的解决方案
对于抖动和宕机导致的region服务质量下降，我们可以有两个思路：

优化系统，减少抖动和宕机处理时间：
比如优化GC，系统参数调优，使用更高配置的机器和网络，将HDD更换为SSD
减少宕机检测时间，优化log split和replay，提高速度。
副本(冗余)，一个reigon有问题时，切换到该region的其他副本中
第一条是必须做的，因为抖动问题就像卡在喉咙里的一根鱼刺，没有问题还好，一旦有问题，就让你疼的不行，甚至还会流点血。不小心扎破颈部大血管，就要打110了。减少宕机恢复时间是有上限的，在当前的硬件体系和能力下，软件做的再好通常也不能在几秒的时间里处理完宕机。所以，必须诉诸于副本方案。一个典型的副本方案就是主备集群。两个集群互为主备，一个挂了就切另一个。一般可以做到秒级检测和切换。但双集群部署会增加额外的成本，所以，HBase 1.x系列提供了单集群的冗余策略，region replica方案，即一个region同时在多个RS上打开，有主备，一写多读。原理上跟mysql的一写多读是类似的。

多副本方案要解决的难题就是一致性问题。目前比较常见的是基于paxos或者raft的一致性算法，在多个副本之间进行数据同步，比如tidb。

从长远来看，第一条是每个分布式存储系统的长期任务，第二条是现代系统要做到高可用所必须具备的能力，或者说目前的技术水平下，可以商用的最好方案。原理都差不多，各家拼的就是工程实现的优劣，即成本，一致性和性能。



在探索过程中，其实还发现了一些其他的优化手段，鉴于开发成本和收益，有些我们并没有完全应用于生产环境。这里列出其中几点，希望能给大家一些启发。

- 不获取总数：大部分场景下，不查询总数都能减少开销，提高性能。ES 7.x之后的搜索接口默认不返回总数了，由此可见一斑。
- 自定义routing规则：从上文的查询过程我们可以看到，ES会轮询所有分片以获取想要的数据，如果我们能控制数据的分片落点，那么也能节省不少开销。比如说：如果我们将来如果有大量的场景都是查某个用户的动态，那么可以控制按照用户分片，这样就避免了分片轮询，也能提升搜索效率。
- keyword：不是所有的数字都应该按照数值字段来存，如果你的数字值很少用于范围查询，但是经常被用作term查询，并且对搜索rt很敏感。那么keyword才是最适合的存储方式。
- 数据预处理：就像IndexSoting一样，如果我们能够在写入时预处理好数据，也能节省搜索时的开销。这一点配合_ingest/pipeline 也许能发挥意想不到的效果。

---------------------------------------------------------------
下单扣减库存

支付扣减库存

可用库存和实际库存

使用redis做库存还起到了限流的作用。 做秒杀或者促销，最重要的就是限流，也就是在redis这里拦住。

redis方案：
Lua 脚本执行流程：
批量扣减是对单个扣减的循环调用，所以这里介绍的流程只讲单次扣减的处理步骤。

首先根据订单明细id查询扣减流水，是否已经操作过，做幂等性校验
然后查询sku的剩余库存，并根据下单购买数做校验，只要有一个sku 数量不足，则返回失败
修改所有sku的缓存中的剩余库存数
缓存中插入扣减流水记录

当Redis扣减成功后，应用程序再将此次扣减异步化保存到数据库中，持久化存储，毕竟Redis只是临时性存储，有宕机风险，会丢失数据。
缓存方案利弊分析：

Redis缓存方案，借助了缓存的高性能，承载更高的并发。但是没有数据库的ACID特性，极端情况下，可能出现少卖情况
为了避免少卖情况发生，纯缓存方案需要做大量的对账、异常处理的设计，系统复杂度增加很多。
纯缓存方案适合一些高并发、大流量场景，但对数据准确度要求不是特别苛刻的业务场景。

风险：
上述Lua脚本把多条命令打包在一起，虽然保证了原子性，但不具备事务回滚特性。比如，库存扣减成功了，此时Redis宕机，扣减流水并没有插入成功，应用程序认为本次Redis调用是失败的，前台给用户反馈错误提示，但是已经扣减的数量不会回滚。当Redis故障修复后，再次启动，此时恢复的数据已经存在不一致了。需要结合Redis和数据库做数据核对check，并结合扣减服务的日志，做数据的增量修复。

mysql官方测评：一般单行更新的QPS在500以内！！！！！！！！！！！！！

重建redis需要结合任务库，任务库是跟redis一个事务的，所以不存在redis挂了，然后从库存数据库重放导致不一致的情况。

所以使用redis的本质是限流，精准的限流。不可能花费大量资源让大量请求进来之后再判断。

---------------------
如何补充库存。。。。
每次被使用到就延长超时时间，所有的key都必须是超时的，不然数据库放不下。



更新不能用自动id

商家系统这边所有的查询都要以 （app_id=xx or merchant_info.sign_status=2）
然后编辑接口新增一个字段是否共享，然后插入表，详情接口新增一个字段是否共享用来展示。

------------------------------------------------
1000 * 20万  = 2亿

写入sdyxopen-operator-v3的tps 200 cpu打满


es中一条0.275kb

52.45

262.25gb







10000 条数据 写3秒

一秒写3333条

堆积四十万的话要 120秒。


系统的讲解
需要先讲架构图
读的大概数据流，写的大概数据流

然后再问下是否需要讲业务流程图


下单过程中
0、校验和检测skuid，生成订单id
1、先把所有skuid发到 热点检测服务，热点检测服务返回 疑似热点的skuid和count
2、判断是否有skuid，有的话count是否大于阈值，没有大于阈值的话走正常下单逻辑结束，大于阈值的话标记为异步下单
3、插入流水表，标记下单处理中
4、正常下单的话就进行分布式事务处理，然后标记流水表为成功或者失败，返回给客户然后结束
5、异步下单的话，发送给asynq，然后返回给用户endPayTime,异步下单标记，payAmount,payOrderId

前端收到异步标记后：
0~2秒内随机调用下单结果查询接口，轮询30秒（必须设置最大时间兜底，防止无限查询）

mq处理过程
1、从asynq批量拉取任务，丢弃掉已经超时的任务，标记流水表失败。下面的处理过程中都要进行context的deadline.
2、分布式事务批量扣减库存，批量下单，批量扣减优惠券
3、库存不够的话就选择部分订单来扣减，扣减失败的订单就标记流水表为库存扣减失败。
批量下单的时候要去流水表中过滤出哪些是扣减库存失败的进行去除。
（流水表是一个筛子）


视频里面提到了调优影响力最大的是应用程序侧，我自己实际使用下来的这一年也深有体会。说白了就是要根据自己的业务需求来调，一切以业务需求为主，无论是功能需求还是性能需求，比如业务允许非近实时搜索可以调大refresh_interval，允许极少的情况丢失点数据可以开启aync fsync，调大这个时间间隔，增加translog的大小等，根据自己的数据调整主分片数，复制分片数，如果全文搜索大字段  多数情况下是不需要字段长度归一值参与评分，可以禁用该字段的norms，如果字段不需要参与聚合，排序，可以禁用该字段doc values等 如果非得用还可以优化本地序数和全局序数想关的几个小参等等。说了这么多比较乱，哈哈。还有好多跟查询相关的优化。总之就是在满足业务需求的基础上，尽量从自己应用程序这边优化。

根据公司分，然后多运营商的话是冷热分离和去除

一定要先把asynq的批量操作看懂。

极限科技可能会问如何高可用你，会问zstd,zstd + source_reuse
极限如何防止oom: 限制最大连接数，限制写入速度，限制读速度




es数据的情况：
1、主从复制，单向订阅
2、应用通过双写或者mq异步订阅
3、通过elasticsearch-dump的方式，原理是scoll+bulk


easySearch的方式：
1、将快照备份发送到从集群去恢复
2、check快照发送过去是否元数据完整，是否有缺有误
3、增量复制，通过mq的方式
4、确保是否每个增量都执行完成，唯一性校验
5、停止主集群流量，执行最后的增量同步，校验最后的增量同步是否执行成功
6、切换到从集群进行写流量，但是写流量还是要通过mq的方式同步到旧的主集群



滴滴的方式：
为了保证数据副本的一致性，副本的数据需要恢复到和主分片一致才能正常对外提供服务。ES的副本恢复是分片级别的，分为主分片恢复流程和副分片恢复流程。由于ES的副本恢复流程极为复杂，并且DCDR的数据恢复过程中仅与副分片恢复流程相关，因此这里只简单地介绍下副分片恢复流程。

副本recovery的目标是要将本地数据恢复到和主分片一致，主流程分为两个阶段：

阶段一是主分片给副本发送segment文件（存储的是已经落盘并解析后的具体数据）
阶段二是主分片向副本发送translog日志（未落盘的数据，类似mysql 的WAL Log），两阶段结束后副本的恢复流程就结束了


segment是不可变的，每次的写其实都是在内存中或者在页缓存中，所以merge不会影响读写过程，而且update其实是把数据query出来然后update后写入内存而已，然后形成一个新的segment.

原生版本本身有限流策略，是基于请求数的漏桶策略，通过队列加线程池的方式实现。线程池大小决定的了处理并发度，处理不完放到队列，队列放不下则拒绝请求。但是单纯的基于请求数的限流不能控制资源使用量，而且只作用于分片级子请求的传输层，对于我们前面分析的接入层无法起到有效的保护作用。原生版本也有内存熔断策略，但是在协调节点接入层并没有做限制。


定时任务如何做？
数据库 和 asynq的方式，时间轮

多层级树形结构：
先返回一级类目
在redis中构建多层级类目

百亿级数据迁移基于bitmap数据一致方案

树型结构查询
#parentId

# 通过id域名的方式，比如广东省是000100，广东汕头是000111 我现在用的和国家行政区划类似的代码0 000 000 000 ... 0代表全部没层最多支持1000


#### http
http包会获取空闲连接，获取不到就去新建，但是新建的话有上限
在创建连接的时候会初始化两个 channel ：writech 负责写入请求数据，reqch负责读取响应数据。我们在上面创建连接的时候，也提到了会为连接创建两个异步循环 readLoop 和 writeLoop 来负责处理读写数据。

在获取到连接之后，会调用连接的 roundTrip 方法，它首先会将请求数据写入到 writech 管道中，writeLoop 接收到数据之后就会处理请求。

然后 roundTrip 会将 requestAndChan 结构体写入到 reqch 管道中，然后 roundTrip 会循环等待。readLoop 读取到响应数据之后就会通过 requestAndChan 结构体中保存的管道将数据封装成 responseAndError 结构体回写，这样 roundTrip 就可以接受到响应数据结束循环等待并返回。

所以不调用close方法的话，那么只会泄露一个读协程和一个写协程，（同一个域名！！！！）但是连接是可以复用的，会放回去
但是不读完的话，就会连接就无法复用了
读完是为了讲请求返回连接池，close是为了关闭读写协程

当未读取body就进行close时，会触发earlyCloseFn()回调，看earlyCloseFn的函数定义，在close未见io.EOF时才调用。自定义的earlyCloseFn方法会给readLoop监听的waitForBodyRead传入false, 这样引发alive为false不能继续循环的接收新请求，只能是退出调用注册过的defer方法，关闭连接和清理连接池。



开源项目这里还需要再仔细一下。。。
asynq的批量获取都还没有理解



为什么一定要分库，不按照业务角度来分库？

es上对于一个手机号，如何通过 四个数字就模糊查询出来

es嵌套很深的话如何优化，nested是怎么实现的？

redis epoll如何实现，使用太多epoll会有什么性能损耗？






咖啡教学，入门 区块链 介绍？

羽毛球

量化交易


美哥的很多个供应商，每个供应商有很多个共同的商品，那么方案可以是每个供应商存储自己的productIds。先将每个商品初始化到es中，然后每个供应商通过自己的categoryId和operatorIds去es查询出来，然后放到redis中。
每个binlog不要立即处理，放到一个队列中，做一个批处理，每隔几秒或者多少数据处理一次，处理完放到es和redis。
redis可以做一个冷热处理，缓存预热，lru。
最后通过redis productIds获取商品，这些商品可以通过redis热商品数据获取，或者通过redis中有对相同查询数据的短时缓存来获取，最后再通过db获取或者es获取。



如何做业务限流，限流算法，接口幂等怎么做的？分布式事务、分布式锁



> 老生常谈的项目亮点和遇到的难点是什么，这个需要说到什么粒度？比如我优化了一个本地缓存，本来是全局一个前缀树，解析字符串，根据 xxxx 的业务场景，改成了二级 map ，减小了锁的粒度。这么说是不是过于简单了？  
  
是的简单了，我的思路是：  
1. 实际过程中，在线上业务或者 perf 测试时候发现了问题，如何发现的？这里涉及到了可观测性工具，以及 perf 工具，以及 debug 思路  
2. 找到了问题，那么就要考虑原有的结构， 如何调优，如何测试，有没有更好地思路，是否去做了探查，业界有哪些方法来解决这个问题（方法论问题）  
3. 解决完成后，看收益，要具体到数字指标  
4. 如何持续性优化思路

1. 项目亮点和遇到的难点  
项目亮点主要是说你项目的特殊性 跟普通的 curd 有哪些不同 你可以说业务上有哪些特殊需求 然后技术方案上怎么处理的，难点部分主要针对你自己解决的问题，需求开发或项目运营周期内有哪些棘手比较难解决的问题例如 bug 、接口告警、性能问题等等，最后通过什么方案（不一定是技术方案）解决了，面试官想通过你解决问题的方式了解你的能力水平。  
  
很多人写在简历上的项目就是白开水，就算真的是你也可以有一些自己的思考，可以是你自己没有真的实现但技术能力范围可以实现的，可以介绍一下给他听，比如说你的想法需要协调很多资源最终没能实现，但你自己做过了什么调研可以确认如果按照这个方案是可以解决某些问题的，这也相当于解决了。  
  
2.系统设计相关的题目，当你给出一个可以实际解决问题的回答时如果面试官追问有没有别的方案，你可以说以往的工作中是这样处理的，是不是有哪里不太完善希望面试官给一些提示。  
  
这里有一部分问题是他自己经历过的问题最终解决了，可能你说的没有他想听到的答案或者细节，最好是反问下他到底想听什么，大部分的人都会给一些提示引导一下方向毕竟太开放了也没办法回答，实在答不出也可以邀请对方聊一下，从对方后续的回答中也可以理解下对方想问的方向如果你能继续聊两句 你也可以顺着他的思路再补充一下说刚才没完全理解题目之类的，尽量不要在这种题目把话题聊死说不会答不出等等 尽量沟通上平滑过度 不然很容易快速结束面试。。。 这种题展示的是你的思路和思考的过程，简答题知道吧。。。别空着瞎聊也聊几句  
  
3.平时的工作中应该怎么挖掘和积累这种面试加分的东西  
没办法只能去业务体量足够大的公司，很多这种东西没实际做过写不出来的，需要公司提供一个这样的工作环境才能成长，看视频看书学不到的。比如说 mysql 单表十亿数据就一定有性能问题吗?就一定要分表吗? 这种方案让你给你没做过你给不出来的,所以去到一个体量大的公司，公司会有架构和 DBA 以及运维联合出一个方案，你只要应用体会和学习，简历上就能写出东西了，菜鸟的成长速度除了自己努力之外还要看是否能够站在巨人肩膀上。  
  
后端值钱的地方就在这里，从纯 CURD 的数据库表设计者转为后端方案设计者是真的实实在在上了一个台阶。

