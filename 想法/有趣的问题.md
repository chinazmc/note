# **为什么32bit系统的处理上，waitgroup state1的元素排列和64bit的不同呢 ？**

首先要理解的是**内存对齐**，32 位机和 64 位机的差别在于每次读取的块大小不同，前者一次 读取 4 字节的块，后者一次读取 8 字节的块。 造成这种状态原因是为了保持64 位字段的原子性。 waitgroup 总共12 字节，假设定义变量var wg waitgroup ,起始地址如果为0x12345664，因为 0xc420016240%8=0 ，所以说这个地址无论在32位或者64 位机子上取8字节都可以保证原子性。如果在32位 操作系统上，对于起始地址为0xc420016244这个地址，因为不是按64位对齐的，所有只能将semap 放在前 面，保证后面是原子操作

# 红黑树的性质

红黑树保证了一种平衡，插入、删除、查找的最坏时间复杂度都为 O(logn)。

每个节点要么是红色，要么是黑色；
根节点永远是黑色的；
所有的叶节点都是是黑色的（注意这里说叶子节点其实是上图中的 NIL 节点）；
每个红色节点的两个子节点一定都是黑色；
从任一节点到其子树中每个叶子节点的路径都包含相同数量的黑色节点；
# Elasticsearch 的 Translog 是拿来干什么的？它可以保证数据一定不丢失吗？

es的translog是wal的实现，每个请求先顺序写入磁盘，这样性能消耗不大。
写入的速度会先在内存中，然后通过refresh写入文件系统缓存，如果这个过程中丢失数据，还可以通过translog来恢复数据。
translog会在一定时间间隔或者达到某个数据量的时候，强制把内存中的数据刷入文件系统缓存，并且把文件系统缓存的数据写入磁盘。
#  什么是 Commit Point？用来干什么？
translog日志提供了一个所有还未被flush到磁盘的操作的持久化记录。当ES启动的时候，它会使用最新的commit  
point从磁盘恢复所有已有的segments，然后将重现所有在translog里面的操作来添加更新，这些更新发生在最新的一次commit的记录之后还未被fsync。
# 为什么主分片数不能修改

> 因为es在存储文档时，会根据主分片数和文档id，通过hash算法，计算出文档应该分布在哪个分片上，确保了文档可以均匀的分布在各个分片中。
> 
> 如果修改了主分片数，那计算规则就变了，会导致文档不能均匀的分布在各个分片中。
> 
> Ps:保存文档时，可以传_routing参数，自定义存储到指定的分片上
# 二叉堆的性质
- 节点 i 的左子节点为2 * i + 1，右子节点为 2 * i+2
- 节点 i 的父节点为 (i - 1) /2
- 结构性：二叉堆是一颗完全二叉树，完全二叉树可以用一个数组表示，不需要指针，所以效率更高。当用数组表示时，数组中任一位置i上的元素，其左儿子在位置2i上，右儿子在位置(2i+ 1)上，其父节点在位置(i/2)上。
- 堆序性质：堆的最小值或最大值在根节点上，所以可以快速找到最大值或最小值。

# b树和b+树
一棵 B 树必须满足以下条件：
若根结点不是终端结点，则至少有2棵子树
除根节点以外的所有非叶结点至少有 M/2 棵子树，至多有 M 个子树（关键字数为子树减一）
所有的叶子结点都位于同一层

一棵 B+ 树需要满足以下条件：
节点的子树数和关键字数相同（B 树是关键字数比子树数少一） 
节点的关键字表示的是子树中的最大数，在子树中同样含有这个数据 
叶子节点包含了全部数据，同时符合左小右大的顺序

# 线性结构
线性结构是最常用的数据结构，而其常见的形式有：数组、队列、链表和栈。
线性结构的特点就是：数据元素之间存在着一对一的线性关系。比如说：
有一个数组`a = [1, 3, 2, 5, 6]`，于是`a[3] = 5`，当数组下标为3的时候，就有一个对应的值是5。
同理，`a[1] = 3`，也是这样`1对1`的关系。
而在线性结构中，又存在2种不同的存储结构：**顺序存储结构、链式存储结构**。
- 顺序存储结构：
- 顺序存储结构的线性表称为顺序表，它的存储元素是连续的(内存地址连续，比如数组)。  
    
- 链式存储结构：  
    链式存储结构的线性表称为链表，它的存储元素不一定是连续的，元素节点中存放数据元素以及相邻元素的地址信息，

比如，单链表、双向链表。因为地址不连续，所以可以利用碎片内存。

# 快排和归并
快速排序时间复杂度nlogn，最坏n²，归并最好最坏都是nlogn

# Docker 容器停止过程

对于容器来说，`init` 系统不是必须的，当你通过命令 `docker stop mycontainer` 来停止容器时，docker CLI 会将 `TERM` 信号发送给 mycontainer 的 `PID` 为 1 的进程。

-   **如果 PID 1 是 init 进程** - 那么 PID 1 会将 TERM 信号转发给子进程，然后子进程开始关闭，最后容器终止。
-   **如果没有 init 进程** - 那么容器中的应用进程（Dockerfile 中的 `ENTRYPOINT` 或 `CMD` 指定的应用）就是 PID 1，应用进程直接负责响应 `TERM` 信号。这时又分为两种情况：
    -   **应用不处理 SIGTERM** - 如果应用没有监听 `SIGTERM` 信号，或者应用中没有实现处理 `SIGTERM` 信号的逻辑，应用就不会停止，容器也不会终止。
    -   **容器停止时间很长** - 运行命令 `docker stop mycontainer` 之后，Docker 会等待 `10s`，如果 `10s` 后容器还没有终止，Docker 就会绕过容器应用直接向内核发送 `SIGKILL`，内核会强行杀死应用，从而终止容器。
### 27、简述Kubernetes deployment升级策略

在Deployment的定义中，可以通过spec.strategy指定Pod更新的策略，目前支持两种策略：Recreate（重建）和RollingUpdate（滚动更新），默认值为RollingUpdate。

-   Recreate：设置spec.strategy.type=Recreate，表示Deployment在更新Pod时，会先杀掉所有正在运行的Pod，然后创建新的Pod。
-   RollingUpdate：设置spec.strategy.type=RollingUpdate，表示Deployment会以滚动更新的方式来逐个更新Pod。同时，可以通过设置spec.strategy.rollingUpdate下的两个参数（maxUnavailable和maxSurge）来控制滚动更新的过程。

31、简述Kubernetes Service分发后端的策略

Service负载分发的策略有：RoundRobin和SessionAffinity

-   RoundRobin：默认为轮询模式，即轮询将请求转发到后端的各个Pod上。
-   SessionAffinity：基于客户端IP地址进行会话保持的模式，即第1次将某个客户端发起的请求转发到后端的某个Pod上，之后从相同的客户端发起的请求都将被转发到后端相同的Pod上。

37、简述Kubernetes各模块如何与API Server通信

Kubernetes API Server作为集群的核心，负责集群各功能模块之间的通信。集群内的各个功能模块通过API Server将信息存入etcd，当需要获取和操作这些数据时，则通过API Server提供的REST接口（用GET、LIST或WATCH方法）来实现，从而实现各模块之间的信息交互。

如kubelet进程与API Server的交互：每个Node上的kubelet每隔一个时间周期，就会调用一次API Server的REST接口报告自身状态，API Server在接收到这些信息后，会将节点状态信息更新到etcd中。

如kube-controller-manager进程与API Server的交互：kube-controller-manager中的Node Controller模块通过API Server提供的Watch接口实时监控Node的信息，并做相应处理。

如kube-scheduler进程与API Server的交互：Scheduler通过API Server的Watch接口监听到新建Pod副本的信息后，会检索所有符合该Pod要求的Node列表，开始执行Pod调度逻辑，在调度成功后将Pod绑定到目标节点上。

  39、简述Kubernetes Scheduler使用哪两种算法将Pod绑定到worker节点

Kubernetes Scheduler根据如下两种调度算法将 Pod 绑定到最合适的工作节点：

-   预选（Predicates）：输入是所有节点，输出是满足预选条件的节点。kube-scheduler根据预选策略过滤掉不满足策略的Nodes。如果某节点的资源不足或者不满足预选策略的条件则无法通过预选。如“Node的label必须与Pod的Selector一致”。
-   优选（Priorities）：输入是预选阶段筛选出的节点，优选会根据优先策略为通过预选的Nodes进行打分排名，选择得分最高的Node。例如，资源越富裕、负载越小的Node可能具有越高的排名。

 49、简述Kubernetes网络模型

Kubernetes网络模型中每个Pod都拥有一个独立的IP地址，并假定所有Pod都在一个可以直接连通的、扁平的网络空间中。所以不管它们是否运行在同一个Node（宿主机）中，都要求它们可以直接通过对方的IP进行访问。设计这个原则的原因是，用户不需要额外考虑如何建立Pod之间的连接，也不需要考虑如何将容器端口映射到主机端口等问题。

同时为每个Pod都设置一个IP地址的模型使得同一个Pod内的不同容器会共享同一个网络命名空间，也就是同一个Linux网络协议栈。这就意味着同一个Pod内的容器可以通过localhost来连接对方的端口。

在Kubernetes的集群里，IP是以Pod为单位进行分配的。一个Pod内部的所有容器共享一个网络堆栈（相当于一个网络命名空间，它们的IP地址、网络设备、配置等都是共享的）。


# sync.pool只 Get 不 Put 会内存泄露吗？
1.  已经从 sync.Pool Get 的值，在 poolClean 时虽说将 pool.local 置成了nil，Get 到的值依然是有效的，是被 GC 标记为黑色的，不会被 GC回收，当 Put 后又重新加入到 sync.Pool 中
2.  在第一个 GC 周期内 Put 到 sync.Pool 的数值，在第二个 GC 周期没有被 Get 使用，就会被放在 local.victim 中。如果在 第三个 GC 周期仍然没有被使用就会被 GC 回收。
使用其他的池，如连接池，如果取连接使用后不放回连接池，就会出现连接池泄露，**是不是 sync.Pool 也有这个问题呢？**

通过上面的流程图，可以看出来 Pool.Get 的时候会尝试从当前 private，shared，其他的 p 的 shared 获取或者 victim 获取，如果实在获取不到时，才会调用 New 函数来获取，New 出来的内容本身还是受系统 GC 来控制的。所以如果我们提供的 New 实现不存在内存泄露的话，那么 sync.Pool 是不会内存泄露的。当 New 出来的变量如果不再被使用，就会被系统 GC 给回收掉。

如果不 Put 回 sync.Pool，会造成 Get 的时候每次都调用的 New 来从堆栈申请空间，达不到减轻 GC 压力。

字段 `pad` 主要是防止 `false sharing`，董大的[《什么是 cpu cache》](https://www.jianshu.com/p/dc4b5562aad2)里讲得比较好：

> 现代 cpu 中，cache 都划分成以 cache line (cache block) 为单位，在 x86_64 体系下一般都是 64 字节，cache line 是操作的最小单元。

> 程序即使只想读内存中的 1 个字节数据，也要同时把附近 63 节字加载到 cache 中，如果读取超个 64 字节，那么就要加载到多个 cache line 中。

简单来说，如果没有 pad 字段，那么当需要访问 0 号索引的 poolLocal 时，CPU 同时会把 0 号和 1 号索引同时加载到 cpu cache。在只修改 0 号索引的情况下，会让 1 号索引的 poolLocal 失效。这样，当其他线程想要读取 1 号索引时，发生 cache miss，还得重新再加载，对性能有损。增加一个 `pad`，补齐缓存行，让相关的字段能独立地加载到缓存行就不会出现 `false sharding` 了。

# bloom
可以看到误判率大约 1% 多点。你也许会问这个误判率还是有点高啊，有没有办法降低一点？答案是有的。
我们上面使用的布隆过滤器只是默认参数的布隆过滤器，它在我们第一次 add 的时候自动创建。Redis 其实还提供了自定义参数的布隆过滤器，需要我们在 add 之前使用bf.reserve指令显式创建。如果对应的 key 已经存在，bf.reserve会报错。bf.reserve有三个参数，分别是 key, error_rate和initial_size。错误率越低，需要的空间越大。initial_size参数表示预计放入的元素数量，当实际数量超出这个数值时，误判率会上升。
所以需要提前设置一个较大的数值避免超出导致误判率升高。如果不使用 bf.reserve，默认的error_rate是 0.01，默认的initial_size是 100。

每个布隆过滤器对应到 Redis 的数据结构里面就是一个大型的位数组和几个不一样的无偏 hash 函数。所谓无偏就是能够把元素的 hash 值算得比较均匀。
向布隆过滤器中添加 key 时，会使用多个 hash 函数对 key 进行 hash 算得一个整数索引值然后对位数组长度进行取模运算得到一个位置，每个 hash 函数都会算得一个不同的位置。再把位数组的这几个位置都置为 1 就完成了 add 操作。
向布隆过滤器询问 key 是否存在时，跟 add 一样，也会把 hash 的几个位置都算出来，看看位数组中这几个位置是否都为 1，只要有一个位为 0，那么说明布隆过滤器中这个 key 不存在。如果都是 1，这并不能说明这个 key 就一定存在，只是极有可能存在，因为这些位被置为 1 可能是因为其它的 key 存在所致。如果这个位数组比较稀疏，判断正确的概率就会很大，如果这个位数组比较拥挤，判断正确的概率就会降低。具体的概率计算公式比较复杂，感兴趣可以阅读扩展阅读，非常烧脑，不建议读者细看。
使用时不要让实际元素远大于初始化大小，当实际元素开始超出初始化大小时，应该对布隆过滤器进行重建，重新分配一个 size 更大的过滤器，再将所有的历史元素批量 add 进去 (这就要求我们在其它的存储器中记录所有的历史元素)。因为 error_rate 不会因为数量超出就急剧增加，这就给我们重建过滤器提供了较为宽松的时间。

用过gin是吧？gin是怎么处理请求的？
如果你的系统突然多了10w的访问量，你要怎么处理？
gin如何做这个身份校验

map扩容是桶乘6。5。    溢出桶等于大于桶

kafka如果消费了消息，但是一直不提交，也不崩溃，那么这个消息永远在未提交状态

进程、线程、协程的对比如下：
协程既不是进程也不是线程，协程仅是一个特殊的函数。协程、进程和线程不是一个维度的。
一个进程可以包含多个线程，一个线程可以包含多个协程。虽然一个线程内的多个协程可以切换但是这多个协程是串行执行的，某个时刻只能有一个线程在运行，没法利用CPU的多核能力。
协程与进程一样，也存在上下文切换问题。
进程的切换者是操作系统，切换时机是根据操作系统自己的切换策略来决定的，用户是无感的。进程的切换内容包括页全局目录、内核栈和硬件上下文，切换内容被保存在内存中。 进程切换过程采用的是“从用户态到内核态再到用户态”的方式，切换效率低。
线程的切换者是操作系统，切换时机是根据操作系统自己的切换策略来决定的，用户是无感的。线程的切换内容包括内核栈和硬件上下文。线程切换内容被保存在内核栈中。线程切换过程采用的是“从用户态到内核态再到用户态”的方式，切换效率中等。
协程的切换者是用户(编程者或应用程序),切换时机是用户自己的程序来决定的。协程的切换内容是硬件上下文，切换内存被保存在用自己的变量(用户栈或堆)中。协程的切换过程只有用户态(即没有陷入内核态),因此切换效率高。

通过 GOGC 或者 debug.SetGCPercent 进行控制（他们控制的是同一个变量，即堆的增长率 ）。整个算法的设计考虑的是优化问题：如果设上一次 GC 完成时，内存的数量为 （heap marked），估计需要触发 GC 时的堆大小 （heap trigger），使得完成 GC 时候的目标堆大小 （heap goal） 与实际完成时候的堆大小 （heap actual）最为接近，即： 。
除此之外，步调算法还需要考虑 CPU 利用率的问题，显然我们不应该让垃圾回收器占用过多的 CPU，即不应该让每个负责执行用户 goroutine 的线程都在执行标记过程。理想情况下，在用户代码满载的时候，GC 的 CPU 使用率不应该超过 25%，即另一个优化问题：如果设 为目标 CPU 使用率（goal utilization），而 为实际 CPU 使用率（actual utilization），则 。

那你介绍一下拥塞控制的算法？
拥塞控制一共有四个算法：
慢启动：TCP 在刚建立连接完成后，首先是有个慢启动的过程，这个慢启动的意思就是⼀点⼀点的提高发送数据包的数量。慢启动的算法规则：当发送方每收到⼀个 ACK，拥塞窗⼝ cwnd 的大小就会加 1。
拥塞避免：当拥塞窗口 cwnd「超过」慢启动门限 ssthresh就会进⼊拥塞避免算法。那么进⼊拥塞避免算法后，它的规则是：每当收到⼀个 ACK 时，cwnd 增加 1/cwnd。
快重传：当接收方发现丢了⼀个中间包的时候，发送三次前⼀个包的ACK，于是发送端就会快速地重传，不必等待超时再重传。
快恢复：快重传和快恢复⼀般同时s使用，快速恢复算法是认为，你还能收到 3 个重复的 ACK说明网络也不那么糟糕，所以没有必要像 RTO 超时那么强烈。进⼊快速恢复之前， cwnd 和 ssthresh 已被更新了：cwnd = cwnd/2 ，也就是设置为原来的⼀半; ssthresh = cwnd 。

我使用volumeMount来使用configmap，然后利用viper的watch机制来监听文件的变动

# 什么是TCP粘包？怎么解决这个问题
在socket网络编程中，都是端到端通信，由客户端端口+服务端端口+客户端IP+服务端IP+传输协议组成的五元组可以明确的标识一条连接。在TCP的socket编程中，发送端和接收端都有成对的socket。发送端为了将多个发往接收端的包，更加高效的的发给接收端，于是采用了优化算法（Nagle算法），将多次间隔较小、数据量较小的数据，合并成一个数据量大的数据块，然后进行封包。那么这样一来，接收端就必须使用高效科学的拆包机制来分辨这些数据。
1.Q：什么是TCP粘包问题？
TCP粘包就是指发送方发送的若干包数据到达接收方时粘成了一包，从接收缓冲区来看，后一包数据的头紧接着前一包数据的尾，出现粘包的原因是多方面的，可能是来自发送方，也可能是来自接收方。
2.Q：造成TCP粘包的原因
（1）发送方原因
TCP默认使用Nagle算法（主要作用：减少网络中报文段的数量），而Nagle算法主要做两件事：只有上一个分组得到确认，才会发送下一个分组收集多个小分组，在一个确认到来时一起发送
Nagle算法造成了发送方可能会出现粘包问题
（2）接收方原因
TCP接收到数据包时，并不会马上交到应用层进行处理，或者说应用层并不会立即处理。实际上，TCP将接收到的数据包保存在接收缓存里，然后应用程序主动从缓存读取收到的分组。这样一来，如果TCP接收数据包到缓存的速度大于应用程序从缓存中读取数据包的速度，多个包就会被缓存，应用程序就有可能读取到多个首尾相接粘到一起的包。
3.Q：什么时候需要处理粘包现象？
如果发送方发送的多组数据本来就是同一块数据的不同部分，比如说一个文件被分成多个部分发送，这时当然不需要处理粘包现象
如果多个分组毫不相干，甚至是并列关系，那么这个时候就一定要处理粘包现象了
4.Q：如何处理粘包现象？
（1）发送方
对于发送方造成的粘包问题，可以通过关闭Nagle算法来解决，使用TCP_NODELAY选项来关闭算法。
（2）接收方
接收方没有办法来处理粘包现象，只能将问题交给应用层来处理。
（2）应用层
应用层的解决办法简单可行，不仅能解决接收方的粘包问题，还可以解决发送方的粘包问题。
解决办法：循环处理，应用程序从接收缓存中读取分组时，读完一条数据，就应该循环读取下一条数据，直到所有数据都被处理完成，但是如何判断每条数据的长度呢？
格式化数据：每条数据有固定的格式（开始符，结束符），这种方法简单易行，但是选择开始符和结束符时一定要确保每条数据的内部不包含开始符和结束符。
发送长度：发送每条数据时，将数据的长度一并发送，例如规定数据的前4位是数据的长度，应用层在处理时可以根据长度来判断每个分组的开始和结束位置。
5.Q：UDP会不会产生粘包问题呢？
TCP为了保证可靠传输并减少额外的开销（每次发包都要验证），采用了基于流的传输，基于流的传输不认为消息是一条一条的，是无保护消息边界的（保护消息边界：指传输协议把数据当做一条独立的消息在网上传输，接收端一次只能接受一条独立的消息）。
UDP则是面向消息传输的，是有保护消息边界的，接收方一次只接受一条独立的信息，所以不存在粘包问题。
举个例子：有三个数据包，大小分别为2k、4k、6k，如果采用UDP发送的话，不管接受方的接收缓存有多大，我们必须要进行至少三次以上的发送才能把数据包发送完，但是使用TCP协议发送的话，我们只需要接受方的接收缓存有12k的大小，就可以一次把这3个数据包全部发送完毕。

udp头部就有消息长度了，所以可以直接取完
可以看出整个过程，IP 层从按长度切片到把切片组装成一个数据包的过程中，都只管运输，都不需要在意消息的边界和内容，都不在意消息内容了，那就不会有粘包一说了。
IP 层表示：我只管把发送端给我的数据传到接收端就完了，我也不了解里头放了啥东西。
TCP 发送端可以发 10 次字节流数据，接收端可以分 100 次去取；UDP 发送端发了 10 次数据报，那接收端就要在 10 次收完。

客户端主动关闭
如果客户端主动关闭连接，那就是正常的关闭了。首先客户端发起关闭连接的请求，服务端收到后，先发送ACK给客户端表示已收到关闭请求，然后客户端到服务端的连接就可以关闭了。此时服务端不会立刻关闭连接，服务端会将还没发完的数据发送到客户端。
此时客户端只是不能向服务端发送数据，但是可以接受数据，这就是TCP的半关闭。此时客户端处于FIN_WAIT2，服务端处于CLOSE_WAIT(如果服务端不关闭连接，将会一直处于该状态，如果连接很多，这将会浪费大量的socket资源)。
服务端在发送完数据后再向客户端发起关闭连接的请求，客户端收到后确认关闭，此时TCP全双工连接就关闭了。
服务端主动关闭
如果是服务端主动发起关闭，此时四次挥手的顺序会颠倒。那么此时客户端再向服务端发送数据时，根据TCP协议的规定，认为它是一个异常终止连接，客户端将会收到一个RST复位响应(而不是ACK响应)，如果客户端再次向服务端发送数据，系统将会发送一个SIGPIPE信号给客户端进程，告诉客户端进程该连接已关闭，不要再写了。系统给SIGPIPE信号的默认处理是直接终止收到该信号的进程，所以此时客户端进程会被极不情愿地终止。
如果不希望客户端进程被终止，可以自定义一个该信号处理的函数，通过调用函数signal(SIGPIPE, handler)实现对信号的处理，其中handler就是可以自定义的函数。
所以说当服务端主动关闭，客户端继续写两次将会导致客户端进程被终止(服务端并不能接收)，客户端不能向服务器写入数据。

拥塞控制算法
慢开始算法
在TCP刚连接好，开始发送TCP报文段时，先令拥塞窗口cwnd=1，在每收到一个对新的报文段确认后，再将cwnd加1，慢开始算法，每经过一个传输轮次（即往返RTT)，拥塞窗口cwnd就会加倍，即呈指数形式增加，当慢开始算法把拥塞窗口cwnd增大到一个阈值ssthresh，改用拥塞避免算法。
拥塞避免算法
拥塞避免算法的做法是：发送端的拥塞窗口cwnd每经过一个往返RTT，按线性规律缓慢增长。
网络拥塞的处理
当网络出现拥塞，不管是在慢开始阶段还是拥塞避免阶段，只要发送方检测到超时事件发送，就要把慢开始阈值ssthresh设置为出现拥塞时的发送方cwnd的一半（但不能小于2），然后将拥塞窗口cwnd重新设置为1，执行慢开始算法。这样做的目的是迅速减少主机发送到网络中的分组数，使得发送拥塞的路由器有足够的时间把队列中积压的分组处理完毕。
慢开始和拥塞避免算法实现过程如图所示：

注意：在慢开始阶段，若2cwnd>ssthresh,则下一个RTT的cwnd应等于ssthresh，而不是2cwnd，即cwnd不能超过ssthresh值
快重传和快恢复
快重传和快恢复算法是对慢开始和拥塞避免算法的改进
快重传
当发送方连续收到三个重复的ACK报文，直接重传对方尚未收到的报文段，而不必等待那个报文段设置的重传计时器超时。
快恢复
当发送端收到连续三个冗余ACK时，就执行“乘法减少”算法，把慢开始阈值ssthresh设置为出现拥塞时发送方cwnd的一半。与慢开始将拥塞窗口cwnd设置为1的不同之处，它把cwnd的值设置为慢开始阈值ssthresh减半后的数值，然后执行拥塞避免算法，使拥塞窗口缓慢线性增大。
实现过程如下图所示：

总结
发送方发送窗口的实际大小由流量控制和拥塞控制共同决定，流量控制窗口rwnd由接收方在TCP报文段首部窗口字段提供，而拥塞控制窗口由发送方对当前网络环境的预估，结合慢开始和拥塞避免或者更优算法快传和快恢复算法来进行拥塞控制。最终发送方实际的发送窗口大小是由rwnd和cwnd中较小的那一个确定的。
快恢复算法中应该是把当前的cwnp减半赋值给ssthresh吧，而不是把ssthresh门限减半吧。

责任链模式优缺点
优点
降低了耦合度，客户端不需要知道请求由哪个处理者处理了，而处理者也不需要知道各个处理者之间的传递关系，由系统组织和分配。
拥有良好的扩展性，增加处理者的实现很简单，只需要重写处理请求业务逻辑的方法。
缺点
请求会从链头发出，直到有处理响应，在责任链比较长的时候会比较影响性能。
请求的传递，在调式的时候比较麻烦。
责任链模式总结
设计模式中责任链模式不在使用简单的 if else 逻辑，使代码里面的一个方法太臃肿，太庞大，阅读性不好，通过责任链模式可以使各个业务编写的时候比较清晰，职责分明。

mysql多慢算慢：多慢算慢？
知道慢查询日志了，那么多慢算慢呢？
对于一般服务器，0.02s 200ms以上就算慢了
对于高并发的业务，也需要0.05s 500ms

datetime没有时区，可以表达的时间更多，用的5字节更多。timestamp有时区，时间到2038年，用的4字节。目前我们有些表用datetime，有些用timestamp。我们不关心时间截至问题，按照业务和数据库发展去情况，我们不觉得到38年这个问题还解决不了，业务可能也到不了38年。所以看情况吧，需要更多精度，用datetime

多个dtm如何协作，面对竞争问题
dtm是否一定要用屏障来解决网络乱序的问题
dtm使用主库，而且时间对齐，运维保证

ES的搜索是分2个阶段进行的，即Query阶段和Fetch阶段。  Query阶段比较轻量级，通过查询倒排索引，获取满足查询结果的文档ID列表。  而Fetch阶段比较重，需要将每个shard的结果取回，在协调结点进行全局排序。  通过From+size这种方式分批获取数据的时候，随着from加大，需要全局排序并丢弃的结果数量随之上升，性能越来越差。
 
而Scroll查询，先做轻量级的Query阶段以后，免去了繁重的全局排序过程。 它只是将查询结果集，也就是doc id列表保留在一个上下文里， 之后每次分批取回的时候，只需根据设置的size，在每个shard内部按照一定顺序（默认doc_id续)， 取回这个size数量的文档即可。 
 
由此也可以看出scroll不适合支持那种实时的和用户交互的前端分页工作，其主要用途用于从ES集群分批拉取大量结果集的情况，一般都是offline的应用场景。  比如需要将非常大的结果集拉取出来，存放到其他系统处理，或者需要做大索引的reindex等等。

**ES 在大结果集指定字段排序的场景下性能不佳，我们使用时应该尽量避免出现这种场景。如果无法避免，合适的 IndexSorting 设置能大幅提升排序性能**。IndexSorting 和查询时排序不一样，本质是在写入时对数据进行预处理。所以排序字段只能在创建时指定且不可更改。并且由于写入时要对数据进行排序，所以也会对写入性能也会有一定负面影响。


但是，在实践中使用该方式可能会存在问题，当发生了Pod扩缩容、上下线的时候；在工程中，Pod 扩容是十分常见的场景，在扩容的时候希望客户端可以及时对新启动的 Pod 快速感知到请求并接收处理。但是对于DNS服务发现，存在下面的[问题](https://github.com/grpc/grpc/issues/12295)：

- 当缩容后，由于 grpc 有连接探活机制，会自动丢弃（剔除）无效连接
- **当扩容后，由于没有感知机制，会导致后续的 Pod 无法被请求到**

上述问题出现的原因是：由于DNS有缓存，gRPC-DNSresolver 解析会缓存解析结果，解析后每 `30min` 才会刷新一次，当 Pod 被销毁时，gRPC 会剔除不健康的 Pod-ip，但是新增的 Pod-ip 必须要在重新链接之后才能解析到

RPC ，client 像调用本地函数一样调用接口，语义更加明确，更加规范，无需关心连接、序列化等细节，这些都由 RPC 框架帮你做了  
  
「通用的 HTTP 库」，如果这个库把连接池、编码等等功能都实现了，调用方每个接口的调用无须做重复性劳动，那么这个「通用的 HTTP 库」何尝不是一个 RPC
rpc 是一种编程概念，就是远程调用。  
http 只是一种实现而已。你单独 socket 实现也行。  
http 本身不是为了 rpc 出生。但是适合绝大部分情况了。

1. 基于 HTTP2 组帧压缩、TCP 连接多路复用等特性，提供了低延时高吞吐  
2. Protobuf 序列化的消息始终小于等效的 JSON 消息  
3. 出色的对双向流式传输可以实时推送接收消息，无需轮询  
4. 多语言支持和代码生成，节约大量开发时间  
5. http api 没有正式规范，站内的的争论也很多，gRpc 规范消除了争论，节省了开发时间

成熟的 rpc 库相对 http 容器，更多的是封装了“服务发现”，"负载均衡"，“熔断降级”一类面向服务的高级特性。  
rpc 框架是面向服务的更高级的封装，针对服务的可用性和效率等都做了优化。单纯使用 http 调用缺少了这些特性。  
而且使用类似 grpc gateway 类似插件也可以提供外部 http 服务，也很方便。

# 一些不保证正确的观念
能问出这个问题，说明你至少现阶段完全不需要 rpc 。你这个比较严格来说是不成立的，http 也可以是 rpc 的一种通信方式，当然我理解你问的肯定是 http + json 和狭义的 rpc ，也就是 grpc/thrift 等的区别。  
  
gRPC 无非就是 protobuf3 + http2 之上做了一些优化，而这些优化几乎只有在大厂才能体现出价值。比如说，大厂内部团队之间要撕逼，没有个 pb 定义的结构，到时候可有得扯皮。虽然 http+json 也有 swagger 和 jsonschema 等工具，但是远不如 pb 或者 thrift 类型丰富且成熟。在大厂里，序列化都可能会成为一个性能瓶颈，我记得之前公司从 thrift 转 pb ，还是 pb 转 thrift 来着，折腾了好久，性能也有很大提升。在这方面，json 序列化的性能就根本不在考虑范围内了。  
  
对于小厂来说，真没必要上什么 rpc ，http + json 撑到上市都没问题。重要的是产品，而不是炫技。rpc 解决了大厂的问题，但是也是有代价的。举例来说，rpc 所谓的跨语言几乎都只是理论上的跨语言而已，gRPC 的 python 支持就没那么好，经常会和 multiprocessing 打架，大厂自然有框架组专门的人来解决，至少规避这个问题，小公司有吗？  
  
最后，技术选型选的不只是某一个技术，更是一个公司的人员组织结构选型。小公司没那么多人，就别给自己找点没用的事儿干，除非你是面向简历编程。

总结一下上面的话，也就是pb可以避免多团队撕逼。不需要对yapi，只要拿过去pb直接gen就行。性能什么的在我们这种级别没有那么悬殊，都差不多。


Broker 端解决方案
在剖析 Broker 端丢失场景的时候， 我们得出其是通过「异步批量刷盘」的策略，先将数据存储到 「PageCache」，再进行异步刷盘， 由于没有提供 「同步刷盘」策略， 因此 Kafka 是通过「多分区多副本」的方式来最大限度保证数据不丢失。
我们可以通过以下参数配合来保证：
1）unclean.leader.election.enable：
该参数表示有哪些 Follower 可以有资格被选举为 Leader , 如果一个 Follower 的数据落后 Leader 太多，那么一旦它被选举为新的 Leader， 数据就会丢失，因
此我们要将其设置为false，防止此类情况发生。
2）replication.factor：
该参数表示分区副本的个数。建议设置 replication.factor >=3, 这样如果 Leader 副本异常 Crash 掉，Follower 副本会被选举为新的 Leader 副本继续提供服
务。
3）min.insync.replicas：
该参数表示消息至少要被写入成功到 ISR 多少个副本才算"已提交"，建议设置min.insync.replicas > 1, 这样才可以提升消息持久性，保证数据不丢失。
另外我们还需要确保一下 replication.factor > min.insync.replicas, 如果相等，只要有一个副本异常 Crash 掉，整个分区就无法正常工作了，因此推荐设置
成： replication.factor = min.insync.replicas +1, 最大限度保证系统可用性



比如取100-110的数据，每个节点上的100-110并不是真正的100-110，所以需要把每个节点的前110都拿过来放一起排序，那么：
每个集群需要查询文档数 = from + size
协调节点需要处理文档数 = 分片数 * (from + size)
如果深度分页，from是1万，那协调点处理的数据量就会非常大，ES默认限定最多为1万个文档。
【解决】
1、使用滚动搜索，记录上一次搜索id，往下翻页从id之后开始取，但是只支持往后翻，不支持指定页数。会创建一个有时间限制的快照，新增数据快照不会更新。
2、业务上处理：每次少量翻页，让用户没有翻到很大页数的兴趣

# dtm多进程轮训用owner来独占
dtm悬挂问题一般都是等到超时后，发出了回滚命令，但是action刚到，所以如果你超时时间长，一般这个问题暂时不解决也行，后续要有订单状态控制risk程序来补偿。

秒杀就是单独服务器，缓存预热，用户根据队列来立即拒绝，url动态生成，redis集群化防止宕机，接口根据用户id限流，
下单流程走异步逻辑，全局队列，消息表全局事务。
（redis预减库存这个操作，其实可以内化为每个商品都有一个最大容量为库存的redis队列，每个用户进来就插入到这个商品redis队列中，达到最大容量就拒绝掉其他用户了，只有队列中的用户才能下单这个商品，库存扣减就不需要走redis扣减了）
（防止超卖的话，可以有可用库存，警戒库存，实际库存，下单扣除可用库存，付款就扣除实际库存，所以双重校验来防止超卖。）

订单幂等性的控制
第一个方面，用户限流，一秒两秒限制用户一次
第二个方面，sql方面要乐观锁控制，状态递增，唯一键，sql加锁
第三个方面，业务分布式锁，比如preOrderId,payOrderId (一方面是分布式锁，一方面是redis 全局set)
用户绑定userid-> preOrderId preOrderId -> payOrderId (多页面打开订单，根据用户id就可以找到之前的记录)

# linux怎么管理内存？扯虚拟内存格式，malloc，缺页中断，页表，换出算法（FIFO、LRU、CLOCK）等等。  
Linux管理内存的主要目标是充分利用可用的物理内存，并为每个进程提供虚拟内存空间。这样可以让进程认为它拥有连续的地址空间，并且不会相互干扰。以下是一些与Linux内存管理相关的概念和机制：

1. 虚拟内存格式： Linux使用虚拟内存格式将进程的逻辑地址空间映射到物理内存或磁盘上的交换空间。每个进程拥有其私有的虚拟地址空间，从0x00000000到0xFFFFFFFF（32位系统）或0x0000000000000000到0x00007FFFFFFFFFFF（64位系统）。虚拟内存格式由页（page）组成，通常是4KB的大小。虚拟内存空间被分为多个页，并且每个页可以映射到物理内存或磁盘上的交换空间。
2. malloc： `malloc` 是C/C++等编程语言中用于动态分配内存的函数。通过 `malloc` 分配的内存块位于进程的堆（heap）中，堆是进程的虚拟地址空间的一部分。
    
3. 缺页中断： 当进程访问虚拟内存中的一个页，但该页当前不在物理内存中时，就会发生缺页中断。此时，操作系统负责将该页从磁盘上的交换空间加载到物理内存中，并更新页表，然后重新执行导致缺页中断的指令。
    
4. 页表： 页表是一种数据结构，用于将进程的虚拟地址映射到物理内存地址。它记录了每个虚拟页对应的物理页的位置。当进程访问虚拟地址时，操作系统使用页表来确定对应的物理地址。
    
5. 换出算法： 当物理内存不足以容纳所有活动进程使用的页时，操作系统需要将某些页置换（换出）到磁盘上的交换空间，以便为其他页腾出空间。常见的换出算法包括FIFO（先进先出）、LRU（最近最少使用）和CLOCK（时钟）等。
    
    - FIFO：选择最早进入物理内存的页进行换出。
    - LRU：选择最近最少使用的页进行换出。
    - CLOCK：使用类似时钟的算法，将物理内存中的页组织成一个环形链表，根据页面是否被访问过来确定换出的页。

总的来说，Linux管理内存通过虚拟内存格式和页表来实现进程间的隔离，使用缺页中断和换出算法来处理物理内存不足的情况。这样可以实现更高效的内存利用，并保证每个进程有独立的地址空间。

# 2、MySQL 索引在什么情况下会失效？

（1）当使用like关键字时，如果查询条件以%开头，索引无效；当like前缀没有%，后缀有%时，索引依然有效。

（2）当使用or关键字时，or语句前后没有同时使用索引或当or关键字左右查询字段只有一个是索引，该索引失效，只有当or左右查询字段均为索引时，才会生效。

（3）使用组合索引时，如果查询条件不包括该组合索引全部字段或查询条件不是该组合索引左边第一个字段时，索引失效。

（4）数据类型出现隐式转化。如某个索引字段的数据类型为varchar，查询内容为123，如不加引号的话可能会自动转换为int型，使索引无效，产生全表扫描。

（5）在索引字段上使用not及运算符

（6）对索引字段进行计算操作、字段上使用函数，索引失效。
# kafka的冷热链表的作用？
kafka的页缓存污染是否为二分查找的时候要把（page0+page最后一个)/2，这样容易缺页中断，所以要分冷热链表，然后先判断是否在热链中，热链就是最近几个页

# innodb b+树高度为2存储多少数据
InnoDB存储引擎的最小存储单元为16k（就像操作系统的最小单元为4k即1页），在这即B+树的一个节点的大小为16k
假设数据库一条数据的大小为1k，则一个节点可
以存储16条数据
而非叶子节点，key一般为主键假设8字节，指针在InnoDB中是6字节，一共为14字节，一个节点可以存储16384/14=1170个索引指针
可以算出一颗高度为2的树（即根节点为存储索
引指针节点，还有1170个叶子节点存储数据）,
每个节点可以存储16条数据，一共1170\*16条数据 =18720条
高度为3的树，可以存放1170\*1170\*16=21902400条记录
两千多万条数据，我们只需要B+树为3层的数据结构就可以完成，通过主键查询只需要3次10操作就能查到对应记录。


> translog日志提供了一个所有还未被flush到磁盘的操作的持久化记录。当ES启动的时候，它会使用最新的commit   point从磁盘恢复所有已有的segments，然后将重现所有在translog里面的操作来添加更新，这些更新发生在最新的一次commit的记录之后还未被fsync。

translog日志也可以用来提供实时的CRUD。当你试图通过文档ID来读取、更新、删除一个文档时，它会首先检查translog日志看看有没有最新的更新，然后再从响应的segment中获得文档。这意味着它每次都会对最新版本的文档做操作，并且是实时的。

## 找出两个大文件交集
使用hash函数将第一个文件的所有整数映射到1000个文件中，每个文件有1000万个整数，大约40M内存，

内存可以放下，把1000个文件记为 a1,a2,a3.....a1000,用同样的hash函数映射第二个文件到1000个文件中，这1000个文件记为b1,b2,b3......b1000，由于使用的是相同的hash函数，所以两个文件中一样的数字会被分配到文件下标一致的文件中，分别对a1和b1求交集，a2和b2求交集，ai和bi求交集，最后将结果汇总，即为两个文件的交集

## 布隆过滤器（Bloom Filter）（给两个文件，分别有100亿个字符串，我们只要1g的内存，如何找到两个文件的交集？分别给出精确算法和近似算法？）
给两个文件，分别有100亿个字符串，我们只要1g的内存，如何找到两个文件的交集？分别给出精确算法和近似算法？
精确算法：
  我们可以创建1000个文件，运用哈希函数先将文件1的字符串保存在对应的文件中，之后再文件2中取元素，通过哈希函数计算出哈希地址，去对应的文件里面找是否有与之相同的字符串。
近似算法：
  我们可以使用位图的方法，通过一个函数将一个元素映射成一个位矩阵中的一个点，这样一来，我们只要看看这个点是不是1就知道集合里有没有它了。 但是有可能两个字符串对应的整数是一样的，对于这种情况我们可以设置更多的哈希函数，对应更多的地址，这样更加精确。

内存分配128k一下用brk，会缓存起来不归还，128k以上用mmap，会立即归还
32位系统申请8g内存会失败


epoll为什么选择使用红黑树而不使用hash表呢，hash表的查询、插入复杂度也是比较低的
Hash表的查询没有红黑树快，插入动作确实比红黑树简单，但是大量数据变动频繁的情况下，hash表需要动态扩容或者缩容，而红黑树不需要。
查询速度:平衡二叉树＞红黑树＞hash
插入删除速度:hash＞红黑树＞平衡二叉树
红黑树综合属性最优，没有明显的缺点，但是优点也不突出
# 在MySQL有延迟的情况下，且不影响业务为前提，如何保障读取的binlog是实时的？
这个引擎也是连接的从库，canal都是不能连接主库的。一方面是不想主库多一个slave,一方面是不愿意主库账号泄露。 一般情况下，其实主库和部分从库配置都是差不多的。

MySQL层解决方案是搭建一台专属blockhole黑洞引擎的slave从库，blockhole黑洞引擎的特性是：只接收binlog但却并不保存数据（有效地减少从库的IO压力，从而缩短同步延迟），相当于一个binlog接收器，canal工具直接从blockhole黑洞引擎读取binlog即可。https://dev.mysql.com/doc/refman/8.0/en/blackhole-storage-engine.html

# 在从库有延迟的情况下，如何解决读取MySQL的最新数据？
1、做缓存，写完mysql就写缓存，后续请求从缓存读  
2、需要数据强一致的读取强制走主库 要做限流 防止打垮数据库  
3、既然是主库扛不住 那应该先从限流入手吧

# MySQL数据库CPU飙升的话，要怎么处理呢？
调整innodb并发线程的数量来解决CPU升高的问题。建议根据业务需求和活动规模，调整合适的值来避免CPU飙升的情况发生。

# MySQL主从复制延迟Seconds_Behind_Master是怎么计算的？
当前系统时间戳 减去 sql thread线程正在执行的relaylog事件时间戳。

# 读写分离-如何感知MySQL同步有延迟？【解答】
https://www.oschina.net/translate/making-haproxy-1-5-replication-lag-aware-in-mysql，官方解决方案。
是通过haproxy实时监听xineted服务的9200端口(自定义)，当探测到延迟时间10秒(自定义)，haproxy会自动摘除该slave的访问请求，把客户端请求路由到master节点。  
  
通常业务可以接受一定时间的延迟，但某些时候从库因为某些原因导致延迟很大，就需要强制读主库。所以Percona公司2014年给出了解决方案。

想想高并发全都打到主库，低并发搞这个查写分离有啥意义呢，我们要的是削峰
场景，比如一台从库延迟很大，那么当haproxy感知后，可以把流量切到没有延迟的从库上。作用是延迟可控。这就是Percona的解决方案。

# 如果有一条数据刚写入主库，还没来得及同步从库，此时主库挂了，自动故障转移，问如何保证数据不丢失？

这还是看怎么理解1.“还没来的及同步到slave”2.“该条数据不丢失”。先理解第二句话，个人理解一定是写入了master返回了成功，而不是异常，如果返回异常，就代表没写入，那就不存在丢失的问题。再来理解第一句话，解决方案可以是半同步或者全同步，但是两个解决方案都需要同步到了slave，一个是同步了所有，一个是同步到了部分，与第一句话矛盾了。这题出的不太好，就是纯纯八股文

# 位移（offset）提交导致的问题
2.1 消息丢失
如果提交的偏移量大于客户端处理的最后一个消息的偏移量,那么处于两个偏移量之间的消息将会丢失。（所以批量拉拉取的话，乱序提交offset的话，前面的消息处理失败了，就不会再拉了）
![[Pasted image 20240126170557.png]]
2.2 消息重复消费
如果提交的位移（offset）量小于消费者实际处理的最后一个消息的位移（offset）量，处于两个位移（offset）之间的消息会被重复处理。
![[Pasted image 20240126170606.png]]
鉴于位移提交甚至是位移管理对 Consumer 端的巨大影响， KafkaConsumer API，提供了多种提交位移的方式。


# 如果MySQL事务中发生了网络异常？
### **3.1 服务端为什么没有退出这个事务呢？**

MySQL普通的会话连接没有保活机制，即没有设置socket属性，也没有设置心跳机制。如果网络连接异常断开服务端不能及时探测到该异常。更进一步，我们通过 TCP 关闭的四次握手来看

![](https://pic2.zhimg.com/80/v2-a440e0d659c80ab10ee577d10c0d2175_720w.webp)

网络异常的时候，TCP连接的状态还是ESTABLISHED，说明 server 和 client 任何一方都**没有主动**发送**FIN包**，服务端还在等待 client端 发送数据，此时的 MySQL 事务无法直接退出。

### **3.2 事务在网络断开后如何处理**

### **事务正在执行**

一个连接进行事务后，如果事务语句正在执行，那么网络断开后会在语句执行完成后回滚掉。因为执行状态包不能送达客户端,因此会感知到这种网络断开的错误。
### **事务执行完成未提交**

如果事务中sql执行完成而没有提交，此时网络断开，那么事务还存在服务端，需要手动kill。client到server端的连接路径:

> socket->listen->poll(socketfd)->accept->newthread->poll(newfd,wait_timeout)

一旦有新的数据到来，如果需要读取或者写入由于网络问题依旧使用poll进行等待，直到超时。其中参数 `read_timeout/write_timeout` 用于读取网络数据的，如果网络不可用，会话保持的时间就是等待网络可用的时间，也就是 `wait_timeout`和 `read_timeout/write_timeout` 均使用poll的timeout实现。**见栈2**，可见 `vio_io_wait` 函数用于处理各种超时，主要用poll来处理。
### **3.3 何时退出呢？**

这里需要分情况来讨论。

### **空闲连接状态时**

此时连接的会话时间由 MySQL参数 `wait_timeout` 决定，默认是8h，也即会话时间空闲超过8h，会被MySQL自动关闭。

### **等待TCP超时**

默认情况下会话会保持2小时+11次*75秒，此时服务端为啥没有退出这个事务呢？ 。(由TCP属性决定)
### **主动kill 异常会话**

> kill $thread_id;

我们之前遇到一次机房级别的断网，应用重连之后遇到大量sql 锁等待，于是乎，我们写了一个脚本定期kill 长时间活跃的事务，仅供大家参考。


# 从库延迟,如何快速解决 循环分批次批量更改数据
思想是把大事务拆分成小事务实现。

# 浅浅提一下es的scoll  
ES的搜索是分2个阶段进行的，即Query阶段和Fetch阶段。  Query阶段比较轻量级，通过查询倒排索引，获取满足查询结果的文档ID列表。  而Fetch阶段比较重，需要将每个shard的结果取回，在协调结点进行全局排序。  通过From+size这种方式分批获取数据的时候，随着from加大，需要全局排序并丢弃的结果数量随之上升，性能越来越差。  
   
而Scroll查询，先做轻量级的Query阶段以后，免去了繁重的全局排序过程。 它只是将查询结果集，也就是doc id列表保留在一个上下文里， 之后每次分批取回的时候，只需根据设置的size，在每个shard内部按照一定顺序（默认doc_id续)， 取回这个size数量的文档即可。   
   
由此也可以看出scroll不适合支持那种实时的和用户交互的前端分页工作，其主要用途用于从ES集群分批拉取大量结果集的情况，一般都是offline的应用场景。  比如需要将非常大的结果集拉取出来，存放到其他系统处理，或者需要做大索引的reindex等等。

### 为什么B+树索引一页是16k？

从磁盘的物理结构来看存取信息的最小单位是扇区，一个扇区是512字节。  
从操作系统对硬盘的存取管理来看，存取信息的最小单位是簇，簇是一个逻辑概念，一个簇可以是2、4、8、16、32或64个连续的扇区。NTFS文件系统格式化的时候默认是8个扇区组成一个簇，即4096字节。linux的分区常用的也是4K大小。

对于系统来说，一次磁盘读取最小读取4K数据，那么如果程序只需要1.5K，5.5K数据都会是浪费。所以索引的一页一般是4K的倍数，默认是16K。

# 系统瓶颈在哪里
了解系统的瓶颈是一个伪命题，前提是要自己要追求多大的qps和tp99,

没有设定前提的qps和tp99，然后再去压测，看看指标，是不会知道瓶颈的。永远都会有瓶颈的！！！

如何做好项目的性能瓶颈判断以及优化，如何做好稳定性  
  
gin获取参数注意的事情  
gin拦截器进去和出去中不调用next会出现什么事情  
  
重构过最大qps是多少(qps是一个伪命题，不同的接口是有不同的qps的和tp99要求的)

  

kafka和nsq都是有消息重复次数上限的！！！！！！！！！！！！！！  

如果做好oncall和backup  

  

目前我的订单系统和商品系统和异步系统的瓶颈出在哪里？  

讲数据异构项目的时候，要说明为什么不能用kafak compaction？  

  

cqrs和事情驱动在business-op-mng和business-op-act体现。。。。  
  
分布式监控是否要完善jaeger  
  
我如何体现高并发系统设计的三大目标：高性能、高可用、可扩展  
  
解决问题的方法也很多，比如使用MQ消息队列做解耦，异步的处理流程。这样的回答，只是问题的“术”。我们要升华到底层的思维逻辑，这是“道”。  
  
谈复杂来源  
谈解决方案  
谈评估标准  
谈技术实现  

  

别人问nsq的话要考虑下是否问有赞nsq

  

你的扩容不能只是通过hpa,spa之类来做，也要根据系统响应时间和qps来做。(我到底要怎么保证安全扩容以及系统不被冲垮，熔断虽然觉得没什么必要，因为返回错误本身就是很难处理的，而且业务开发也很难保证说一定会开发，但是是否 限流熔断降级超时控制这些手段都可以根据某个开关来开启，目前从商品h5那边获取的启示是，防御性编程不要轻易把错误返回，一些支撑接口没有数据回来也不要轻易返回错误，尽可能返回主干。)  

  

  

异构数据同步 考虑的方案要考虑flink cdc，然后进行取舍

  

一个商品上下架或者失效之类的，如何通知到所有的商品模板都取消这个商品  
分销场景下，一个运营商根据类型挑选了商品，并且定义了有优先级的价格规则，实时去算会很忙，如何在一分钟内算好  
例如 一个运营商挑选了类目，并对类目都进行了改价，那么如何快速算好  
600多个运营商 * 6万个商品

# 那为什么是16K而不是4K 8K呢？原因是16K在大多数场景下够用了。

我们假设一条记录大小1K左右，那一页（叶子节点）能够存储16条记录。如果表的主键是bigint，也就是8B，指针大小在innodb中为6B，也就是14B。一页（非叶子节点）能够存储16K/14=1170个。那一个高度为2的B+树可以存储1170_16 = 18720条记录 ，一个高度为3的B+树可以存储1170_1170\*16=21902400条记录，大概是2千万左右。这个量一般都是够用了。
一句话就是 一定要是4的倍数，8太小，16刚刚好。
# 插入意向锁

插入意向锁是一种间隙锁形式的意向锁，在真正执行 INSERT 操作之前设置。  
当执行插入操作时，总会检查当前插入操作的下一条记录（已存在的主索引节点）上是否存在锁对象，判断是否锁住了 gap，如果锁住了，则判定和插入意向锁冲突，当前插入操作就需要等待，也就是配合上面的间隙锁或者临键锁一起防止了幻读操作。

因为插入意向锁是一种意向锁，意向锁只是表示一种意向，所以插入意向锁之间不会互相冲突，多个插入操作同时插入同一个 gap 时，无需互相等待，比如当前索引上有记录 4 和 8，两个并发 session 同时插入记录 6，7。他们会分别为(4,8)加上 GAP 锁，但相互之间并不冲突

INSERT 语句在执行插入之前，会先在 gap 中加入插入意向锁，如果是唯一索引，还会进行 Duplicate Key 判断，如果存在相同 Key 且该 Key 被加了互斥锁，则还会加共享锁，然后等待（因为这个相同的 Key 之后有可能会回滚删除，这里非常容易死锁）。等到成功插入后，会在这条记录上加排他记录锁

# gin的中间件源码，不把值放回去会怎么样来着之类的

您好，前公司管理比较扁平，虽然一直被领导重用，也负责过多个核心业务，但是职级几年来一直没什么变化。领导也争取过，但由于工作年限等原因，没能通过。我不想安于现状，想找一个更能发挥自己才能的平台。

- 公司常用的技术栈是什么？
- 你们怎么使用源码控制系统？
- 你们怎么测试代码？
- 你们怎么追踪 bug?
- 你们怎样监控项目？
- 你们怎么集成和部署代码改动？是使用持续集成和持续部署吗 (CI/CD)？
- 你们的基础设施搭建在版本管理系统里吗？或者是代码化的吗？
- 从计划到完成一项任务的工作流是什么样的？
- 你们如何准备故障恢复？
- 有标准的开发环境吗？是强制的吗？
- 你们需要花费多长时间来给产品搭建一个本地测试环境？（分钟 / 小时 / 天）
- 你们需要花费多长时间来响应代码或者依赖中的安全问题？
- 所有的开发者都可以使用他们电脑的本地管理员权限吗？
- 介绍一下你们的技术原则或者展望。
- 你们的代码有开发文档吗？有没有单独的供消费者阅读的文档？
- 你们有更高层次的文档吗？比如说 ER 图，数据库范式
- 你们使用静态代码分析吗？
- 你们如何管理内部和外部的数字资产？
- 你们如何管理依赖？
- 公司是否有技术分享交流活动？有的话，多久一次呢？(zh)
- 你们的数据库是怎么进行版本控制的？(zh)
- 业务需求有没有文档记录？是如何记录的？(zh)
- 工作是怎么组织的？
- 团队内 / 团队间的交流通常是怎样的？
- 你们使用什么工具来做项目组织？你的实际体会是什么？
- 如果遇到不同的意见怎样处理？
- 谁来设定优先级 / 计划？
- 如果团队没能赶上预期发布日期怎么办？
- 每周都会开什么类型的会议？
- 会有定期的和上级的一对一谈话吗？
- 产品 / 服务的规划是什么样的？（n 周一发布 / 持续部署 / 多个发布流 / ...)
- 生产环境发生事故了怎么办？是否有不批评人而分析问题的文化？
- 有没有一些团队正在经历还尚待解决的挑战？
- 你们如何跟踪进度？
- 预期和目标是如何设定的？谁来设定？
- Code Review 如何实施？
- 给我介绍下团队里一个典型的 sprint
- 你们如何平衡技术和商业目标？
- 你们如何共享知识？
- 团队有多大？
- 公司技术团队的架构和人员组成？(zh)
- 团队内开发、产品、运营哪一方是需求的主要提出方？哪一方更强势？(zh)


# Reference
https://zhuanlan.zhihu.com/p/262501073